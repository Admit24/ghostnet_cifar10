nohup: ignoring input
PID:139834
set gpu list :2,3

device_count: 2
repalce 13 conv layers
repalce all conv layer to ghost module
('model architecture: ', VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace)
    (3): GhostModule(
      (primary_conv): Sequential(
        (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
      (cheap_operation): Sequential(
        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
    )
    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace)
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (7): GhostModule(
      (primary_conv): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
      (cheap_operation): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
    )
    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (9): ReLU(inplace)
    (10): GhostModule(
      (primary_conv): Sequential(
        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
      (cheap_operation): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
    )
    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): ReLU(inplace)
    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (14): GhostModule(
      (primary_conv): Sequential(
        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
      (cheap_operation): Sequential(
        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
    )
    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): ReLU(inplace)
    (17): GhostModule(
      (primary_conv): Sequential(
        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
      (cheap_operation): Sequential(
        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
    )
    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (19): ReLU(inplace)
    (20): GhostModule(
      (primary_conv): Sequential(
        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
      (cheap_operation): Sequential(
        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
    )
    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): ReLU(inplace)
    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (24): GhostModule(
      (primary_conv): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
      (cheap_operation): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
    )
    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): ReLU(inplace)
    (27): GhostModule(
      (primary_conv): Sequential(
        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
      (cheap_operation): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
    )
    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (29): ReLU(inplace)
    (30): GhostModule(
      (primary_conv): Sequential(
        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
      (cheap_operation): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
    )
    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): ReLU(inplace)
    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (34): GhostModule(
      (primary_conv): Sequential(
        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
      (cheap_operation): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
    )
    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): ReLU(inplace)
    (37): GhostModule(
      (primary_conv): Sequential(
        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
      (cheap_operation): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
    )
    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (39): ReLU(inplace)
    (40): GhostModule(
      (primary_conv): Sequential(
        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
      (cheap_operation): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace)
      )
    )
    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): ReLU(inplace)
    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (44): AvgPool2d(kernel_size=1, stride=1, padding=0)
  )
  (classifier): Linear(in_features=512, out_features=10, bias=True)
))
args:
Namespace(arc='vgg16', bs=128, frozen_primary=False, gpu=[2, 3], low_lr=False, lr=0.1, max_epoch=190, point_conv=False, pretrain=False, replace_conv=True, start_epoch=0)
 cfg:
{'cls_names': ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'], 'milestones': [92, 136], 'valid_bs': 128, 'transforms_valid': Compose(
    Resize(size=(32, 32), interpolation=PIL.Image.BILINEAR)
    ToTensor()
    Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
), 'workers': 8, 'log_interval': 50, 'patience': 20, 'transforms_train': Compose(
    Resize(size=32, interpolation=PIL.Image.BILINEAR)
    RandomHorizontalFlip(p=0.5)
    RandomCrop(size=(32, 32), padding=4)
    ToTensor()
    Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
), 'factor': 0.1, 'class_num': 10, 'train_bs': 128, 'weight_decay': 0.0001, 'momentum': 0.9}
 loss_f:
CrossEntropyLoss()
 scheduler:
<torch.optim.lr_scheduler.MultiStepLR object at 0x7f091ad181d0>
 optimizer:
SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.1
    lr: 0.1
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0001
)
Training: Epoch[001/190] Iteration[050/391] Loss: 3.3699 Acc:11.83%
Training: Epoch[001/190] Iteration[100/391] Loss: 2.8182 Acc:15.05%
Training: Epoch[001/190] Iteration[150/391] Loss: 2.5636 Acc:17.59%
Training: Epoch[001/190] Iteration[200/391] Loss: 2.4104 Acc:19.61%
Training: Epoch[001/190] Iteration[250/391] Loss: 2.3088 Acc:20.84%
Training: Epoch[001/190] Iteration[300/391] Loss: 2.2365 Acc:22.15%
Training: Epoch[001/190] Iteration[350/391] Loss: 2.1778 Acc:23.16%
Epoch[001/190] Train Acc: 24.01% Valid Acc:32.35% Train loss:2.1401 Valid loss:176835.4653 LR:0.1
Training: Epoch[002/190] Iteration[050/391] Loss: 1.8129 Acc:31.62%
Training: Epoch[002/190] Iteration[100/391] Loss: 1.7905 Acc:32.12%
Training: Epoch[002/190] Iteration[150/391] Loss: 1.7718 Acc:32.69%
Training: Epoch[002/190] Iteration[200/391] Loss: 1.7526 Acc:33.59%
Training: Epoch[002/190] Iteration[250/391] Loss: 1.7370 Acc:34.03%
Training: Epoch[002/190] Iteration[300/391] Loss: 1.7225 Acc:34.54%
Training: Epoch[002/190] Iteration[350/391] Loss: 1.7050 Acc:35.15%
Epoch[002/190] Train Acc: 35.62% Valid Acc:37.07% Train loss:1.6934 Valid loss:339494280822.7518 LR:0.1
Training: Epoch[003/190] Iteration[050/391] Loss: 1.5631 Acc:41.92%
Training: Epoch[003/190] Iteration[100/391] Loss: 1.5575 Acc:41.94%
Training: Epoch[003/190] Iteration[150/391] Loss: 1.5516 Acc:42.11%
Training: Epoch[003/190] Iteration[200/391] Loss: 1.5368 Acc:42.73%
Training: Epoch[003/190] Iteration[250/391] Loss: 1.5210 Acc:43.55%
Training: Epoch[003/190] Iteration[300/391] Loss: 1.5071 Acc:44.24%
Training: Epoch[003/190] Iteration[350/391] Loss: 1.4952 Acc:44.76%
Epoch[003/190] Train Acc: 45.11% Valid Acc:48.73% Train loss:1.4857 Valid loss:90232012391.0688 LR:0.1
Training: Epoch[004/190] Iteration[050/391] Loss: 1.3676 Acc:50.52%
Training: Epoch[004/190] Iteration[100/391] Loss: 1.3589 Acc:50.67%
Training: Epoch[004/190] Iteration[150/391] Loss: 1.3429 Acc:51.34%
Training: Epoch[004/190] Iteration[200/391] Loss: 1.3304 Acc:51.87%
Training: Epoch[004/190] Iteration[250/391] Loss: 1.3198 Acc:52.59%
Training: Epoch[004/190] Iteration[300/391] Loss: 1.3103 Acc:52.92%
Training: Epoch[004/190] Iteration[350/391] Loss: 1.2976 Acc:53.35%
Epoch[004/190] Train Acc: 53.52% Valid Acc:50.19% Train loss:1.2919 Valid loss:75180395346.9622 LR:0.1
Training: Epoch[005/190] Iteration[050/391] Loss: 1.2000 Acc:57.61%
Training: Epoch[005/190] Iteration[100/391] Loss: 1.1932 Acc:57.32%
Training: Epoch[005/190] Iteration[150/391] Loss: 1.1874 Acc:57.52%
Training: Epoch[005/190] Iteration[200/391] Loss: 1.1767 Acc:57.96%
Training: Epoch[005/190] Iteration[250/391] Loss: 1.1686 Acc:58.19%
Training: Epoch[005/190] Iteration[300/391] Loss: 1.1603 Acc:58.60%
Training: Epoch[005/190] Iteration[350/391] Loss: 1.1524 Acc:58.95%
Epoch[005/190] Train Acc: 59.34% Valid Acc:60.79% Train loss:1.1433 Valid loss:71601656202.5709 LR:0.1
Training: Epoch[006/190] Iteration[050/391] Loss: 1.0662 Acc:61.14%
Training: Epoch[006/190] Iteration[100/391] Loss: 1.0612 Acc:62.00%
Training: Epoch[006/190] Iteration[150/391] Loss: 1.0646 Acc:62.06%
Training: Epoch[006/190] Iteration[200/391] Loss: 1.0517 Acc:62.68%
Training: Epoch[006/190] Iteration[250/391] Loss: 1.0415 Acc:62.95%
Training: Epoch[006/190] Iteration[300/391] Loss: 1.0342 Acc:63.27%
Training: Epoch[006/190] Iteration[350/391] Loss: 1.0311 Acc:63.45%
Epoch[006/190] Train Acc: 63.57% Valid Acc:64.73% Train loss:1.0272 Valid loss:63626384614.4117 LR:0.1
Training: Epoch[007/190] Iteration[050/391] Loss: 0.9595 Acc:66.06%
Training: Epoch[007/190] Iteration[100/391] Loss: 0.9443 Acc:66.55%
Training: Epoch[007/190] Iteration[150/391] Loss: 0.9486 Acc:66.50%
Training: Epoch[007/190] Iteration[200/391] Loss: 0.9462 Acc:66.50%
Training: Epoch[007/190] Iteration[250/391] Loss: 0.9437 Acc:66.76%
Training: Epoch[007/190] Iteration[300/391] Loss: 0.9416 Acc:66.93%
Training: Epoch[007/190] Iteration[350/391] Loss: 0.9377 Acc:67.08%
Epoch[007/190] Train Acc: 67.23% Valid Acc:68.13% Train loss:0.9325 Valid loss:90072809831.2856 LR:0.1
Training: Epoch[008/190] Iteration[050/391] Loss: 0.8740 Acc:69.20%
Training: Epoch[008/190] Iteration[100/391] Loss: 0.8707 Acc:69.34%
Training: Epoch[008/190] Iteration[150/391] Loss: 0.8748 Acc:69.30%
Training: Epoch[008/190] Iteration[200/391] Loss: 0.8729 Acc:69.42%
Training: Epoch[008/190] Iteration[250/391] Loss: 0.8679 Acc:69.61%
Training: Epoch[008/190] Iteration[300/391] Loss: 0.8655 Acc:69.77%
Training: Epoch[008/190] Iteration[350/391] Loss: 0.8612 Acc:69.94%
Epoch[008/190] Train Acc: 70.09% Valid Acc:70.43% Train loss:0.8594 Valid loss:108.2393 LR:0.1
Training: Epoch[009/190] Iteration[050/391] Loss: 0.8058 Acc:71.09%
Training: Epoch[009/190] Iteration[100/391] Loss: 0.7894 Acc:72.10%
Training: Epoch[009/190] Iteration[150/391] Loss: 0.7979 Acc:72.12%
Training: Epoch[009/190] Iteration[200/391] Loss: 0.7916 Acc:72.39%
Training: Epoch[009/190] Iteration[250/391] Loss: 0.7872 Acc:72.54%
Training: Epoch[009/190] Iteration[300/391] Loss: 0.7877 Acc:72.61%
Training: Epoch[009/190] Iteration[350/391] Loss: 0.7830 Acc:72.82%
Epoch[009/190] Train Acc: 73.03% Valid Acc:73.04% Train loss:0.7770 Valid loss:0.7898 LR:0.1
Training: Epoch[010/190] Iteration[050/391] Loss: 0.7540 Acc:74.36%
Training: Epoch[010/190] Iteration[100/391] Loss: 0.7305 Acc:75.21%
Training: Epoch[010/190] Iteration[150/391] Loss: 0.7279 Acc:75.28%
Training: Epoch[010/190] Iteration[200/391] Loss: 0.7312 Acc:75.16%
Training: Epoch[010/190] Iteration[250/391] Loss: 0.7308 Acc:75.07%
Training: Epoch[010/190] Iteration[300/391] Loss: 0.7282 Acc:75.11%
Training: Epoch[010/190] Iteration[350/391] Loss: 0.7237 Acc:75.29%
Epoch[010/190] Train Acc: 75.29% Valid Acc:74.81% Train loss:0.7247 Valid loss:9734.9921 LR:0.1
Training: Epoch[011/190] Iteration[050/391] Loss: 0.6477 Acc:77.92%
Training: Epoch[011/190] Iteration[100/391] Loss: 0.6710 Acc:76.95%
Training: Epoch[011/190] Iteration[150/391] Loss: 0.6757 Acc:76.95%
Training: Epoch[011/190] Iteration[200/391] Loss: 0.6769 Acc:76.96%
Training: Epoch[011/190] Iteration[250/391] Loss: 0.6756 Acc:76.88%
Training: Epoch[011/190] Iteration[300/391] Loss: 0.6760 Acc:76.86%
Training: Epoch[011/190] Iteration[350/391] Loss: 0.6750 Acc:76.96%
Epoch[011/190] Train Acc: 76.98% Valid Acc:77.82% Train loss:0.6739 Valid loss:0.6553 LR:0.1
Training: Epoch[012/190] Iteration[050/391] Loss: 0.6291 Acc:78.41%
Training: Epoch[012/190] Iteration[100/391] Loss: 0.6365 Acc:78.20%
Training: Epoch[012/190] Iteration[150/391] Loss: 0.6408 Acc:78.14%
Training: Epoch[012/190] Iteration[200/391] Loss: 0.6432 Acc:78.01%
Training: Epoch[012/190] Iteration[250/391] Loss: 0.6440 Acc:78.01%
Training: Epoch[012/190] Iteration[300/391] Loss: 0.6442 Acc:77.99%
Training: Epoch[012/190] Iteration[350/391] Loss: 0.6417 Acc:78.11%
Epoch[012/190] Train Acc: 78.23% Valid Acc:78.98% Train loss:0.6404 Valid loss:0.6282 LR:0.1
Training: Epoch[013/190] Iteration[050/391] Loss: 0.6265 Acc:78.19%
Training: Epoch[013/190] Iteration[100/391] Loss: 0.6072 Acc:78.95%
Training: Epoch[013/190] Iteration[150/391] Loss: 0.6119 Acc:79.01%
Training: Epoch[013/190] Iteration[200/391] Loss: 0.6087 Acc:79.15%
Training: Epoch[013/190] Iteration[250/391] Loss: 0.6103 Acc:79.14%
Training: Epoch[013/190] Iteration[300/391] Loss: 0.6100 Acc:79.21%
Training: Epoch[013/190] Iteration[350/391] Loss: 0.6103 Acc:79.21%
Epoch[013/190] Train Acc: 79.21% Valid Acc:78.16% Train loss:0.6117 Valid loss:0.6705 LR:0.1
Training: Epoch[014/190] Iteration[050/391] Loss: 0.5561 Acc:81.12%
Training: Epoch[014/190] Iteration[100/391] Loss: 0.5621 Acc:81.16%
Training: Epoch[014/190] Iteration[150/391] Loss: 0.5684 Acc:80.84%
Training: Epoch[014/190] Iteration[200/391] Loss: 0.5723 Acc:80.63%
Training: Epoch[014/190] Iteration[250/391] Loss: 0.5744 Acc:80.60%
Training: Epoch[014/190] Iteration[300/391] Loss: 0.5776 Acc:80.50%
Training: Epoch[014/190] Iteration[350/391] Loss: 0.5755 Acc:80.50%
Epoch[014/190] Train Acc: 80.43% Valid Acc:80.76% Train loss:0.5776 Valid loss:0.5702 LR:0.1
Training: Epoch[015/190] Iteration[050/391] Loss: 0.5509 Acc:81.14%
Training: Epoch[015/190] Iteration[100/391] Loss: 0.5491 Acc:81.11%
Training: Epoch[015/190] Iteration[150/391] Loss: 0.5507 Acc:81.14%
Training: Epoch[015/190] Iteration[200/391] Loss: 0.5506 Acc:81.11%
Training: Epoch[015/190] Iteration[250/391] Loss: 0.5483 Acc:81.27%
Training: Epoch[015/190] Iteration[300/391] Loss: 0.5509 Acc:81.16%
Training: Epoch[015/190] Iteration[350/391] Loss: 0.5508 Acc:81.14%
Epoch[015/190] Train Acc: 81.18% Valid Acc:80.13% Train loss:0.5498 Valid loss:0.5889 LR:0.1
Training: Epoch[016/190] Iteration[050/391] Loss: 0.5312 Acc:81.84%
Training: Epoch[016/190] Iteration[100/391] Loss: 0.5282 Acc:82.13%
Training: Epoch[016/190] Iteration[150/391] Loss: 0.5397 Acc:81.70%
Training: Epoch[016/190] Iteration[200/391] Loss: 0.5364 Acc:81.91%
Training: Epoch[016/190] Iteration[250/391] Loss: 0.5347 Acc:81.93%
Training: Epoch[016/190] Iteration[300/391] Loss: 0.5361 Acc:81.92%
Training: Epoch[016/190] Iteration[350/391] Loss: 0.5376 Acc:81.90%
Epoch[016/190] Train Acc: 82.03% Valid Acc:81.02% Train loss:0.5350 Valid loss:0.5583 LR:0.1
Training: Epoch[017/190] Iteration[050/391] Loss: 0.5049 Acc:83.00%
Training: Epoch[017/190] Iteration[100/391] Loss: 0.5001 Acc:83.08%
Training: Epoch[017/190] Iteration[150/391] Loss: 0.5029 Acc:83.00%
Training: Epoch[017/190] Iteration[200/391] Loss: 0.5066 Acc:82.89%
Training: Epoch[017/190] Iteration[250/391] Loss: 0.5118 Acc:82.75%
Training: Epoch[017/190] Iteration[300/391] Loss: 0.5157 Acc:82.58%
Training: Epoch[017/190] Iteration[350/391] Loss: 0.5196 Acc:82.42%
Epoch[017/190] Train Acc: 82.51% Valid Acc:81.34% Train loss:0.5176 Valid loss:0.5554 LR:0.1
Training: Epoch[018/190] Iteration[050/391] Loss: 0.4920 Acc:83.30%
Training: Epoch[018/190] Iteration[100/391] Loss: 0.5051 Acc:82.88%
Training: Epoch[018/190] Iteration[150/391] Loss: 0.5027 Acc:82.71%
Training: Epoch[018/190] Iteration[200/391] Loss: 0.5012 Acc:82.74%
Training: Epoch[018/190] Iteration[250/391] Loss: 0.5035 Acc:82.78%
Training: Epoch[018/190] Iteration[300/391] Loss: 0.5026 Acc:82.82%
Training: Epoch[018/190] Iteration[350/391] Loss: 0.5040 Acc:82.79%
Epoch[018/190] Train Acc: 82.83% Valid Acc:81.99% Train loss:0.5035 Valid loss:0.5522 LR:0.1
Training: Epoch[019/190] Iteration[050/391] Loss: 0.4822 Acc:83.27%
Training: Epoch[019/190] Iteration[100/391] Loss: 0.4789 Acc:83.59%
Training: Epoch[019/190] Iteration[150/391] Loss: 0.4662 Acc:84.04%
Training: Epoch[019/190] Iteration[200/391] Loss: 0.4721 Acc:83.92%
Training: Epoch[019/190] Iteration[250/391] Loss: 0.4747 Acc:83.89%
Training: Epoch[019/190] Iteration[300/391] Loss: 0.4774 Acc:83.82%
Training: Epoch[019/190] Iteration[350/391] Loss: 0.4787 Acc:83.70%
Epoch[019/190] Train Acc: 83.56% Valid Acc:81.01% Train loss:0.4823 Valid loss:0.5493 LR:0.1
Training: Epoch[020/190] Iteration[050/391] Loss: 0.4404 Acc:84.64%
Training: Epoch[020/190] Iteration[100/391] Loss: 0.4673 Acc:83.92%
Training: Epoch[020/190] Iteration[150/391] Loss: 0.4733 Acc:83.75%
Training: Epoch[020/190] Iteration[200/391] Loss: 0.4711 Acc:83.96%
Training: Epoch[020/190] Iteration[250/391] Loss: 0.4755 Acc:83.78%
Training: Epoch[020/190] Iteration[300/391] Loss: 0.4719 Acc:83.92%
Training: Epoch[020/190] Iteration[350/391] Loss: 0.4733 Acc:83.87%
Epoch[020/190] Train Acc: 83.83% Valid Acc:79.77% Train loss:0.4738 Valid loss:0.5994 LR:0.1
Training: Epoch[021/190] Iteration[050/391] Loss: 0.4451 Acc:84.73%
Training: Epoch[021/190] Iteration[100/391] Loss: 0.4401 Acc:84.86%
Training: Epoch[021/190] Iteration[150/391] Loss: 0.4425 Acc:84.99%
Training: Epoch[021/190] Iteration[200/391] Loss: 0.4458 Acc:84.86%
Training: Epoch[021/190] Iteration[250/391] Loss: 0.4507 Acc:84.71%
Training: Epoch[021/190] Iteration[300/391] Loss: 0.4534 Acc:84.57%
Training: Epoch[021/190] Iteration[350/391] Loss: 0.4565 Acc:84.44%
Epoch[021/190] Train Acc: 84.48% Valid Acc:81.64% Train loss:0.4564 Valid loss:0.5503 LR:0.1
Training: Epoch[022/190] Iteration[050/391] Loss: 0.4300 Acc:85.67%
Training: Epoch[022/190] Iteration[100/391] Loss: 0.4305 Acc:85.63%
Training: Epoch[022/190] Iteration[150/391] Loss: 0.4406 Acc:85.08%
Training: Epoch[022/190] Iteration[200/391] Loss: 0.4413 Acc:85.11%
Training: Epoch[022/190] Iteration[250/391] Loss: 0.4409 Acc:85.21%
Training: Epoch[022/190] Iteration[300/391] Loss: 0.4411 Acc:85.06%
Training: Epoch[022/190] Iteration[350/391] Loss: 0.4432 Acc:85.04%
Epoch[022/190] Train Acc: 85.05% Valid Acc:82.45% Train loss:0.4427 Valid loss:0.5411 LR:0.1
Training: Epoch[023/190] Iteration[050/391] Loss: 0.4294 Acc:85.19%
Training: Epoch[023/190] Iteration[100/391] Loss: 0.4232 Acc:85.61%
Training: Epoch[023/190] Iteration[150/391] Loss: 0.4263 Acc:85.71%
Training: Epoch[023/190] Iteration[200/391] Loss: 0.4279 Acc:85.76%
Training: Epoch[023/190] Iteration[250/391] Loss: 0.4280 Acc:85.64%
Training: Epoch[023/190] Iteration[300/391] Loss: 0.4331 Acc:85.46%
Training: Epoch[023/190] Iteration[350/391] Loss: 0.4321 Acc:85.48%
Epoch[023/190] Train Acc: 85.45% Valid Acc:83.29% Train loss:0.4340 Valid loss:0.5012 LR:0.1
Training: Epoch[024/190] Iteration[050/391] Loss: 0.3999 Acc:86.42%
Training: Epoch[024/190] Iteration[100/391] Loss: 0.4092 Acc:85.84%
Training: Epoch[024/190] Iteration[150/391] Loss: 0.4005 Acc:86.10%
Training: Epoch[024/190] Iteration[200/391] Loss: 0.4073 Acc:86.03%
Training: Epoch[024/190] Iteration[250/391] Loss: 0.4175 Acc:85.72%
Training: Epoch[024/190] Iteration[300/391] Loss: 0.4183 Acc:85.74%
Training: Epoch[024/190] Iteration[350/391] Loss: 0.4187 Acc:85.71%
Epoch[024/190] Train Acc: 85.70% Valid Acc:83.64% Train loss:0.4190 Valid loss:0.4999 LR:0.1
Training: Epoch[025/190] Iteration[050/391] Loss: 0.3896 Acc:86.83%
Training: Epoch[025/190] Iteration[100/391] Loss: 0.3876 Acc:86.80%
Training: Epoch[025/190] Iteration[150/391] Loss: 0.3968 Acc:86.50%
Training: Epoch[025/190] Iteration[200/391] Loss: 0.4028 Acc:86.25%
Training: Epoch[025/190] Iteration[250/391] Loss: 0.4046 Acc:86.31%
Training: Epoch[025/190] Iteration[300/391] Loss: 0.4042 Acc:86.34%
Training: Epoch[025/190] Iteration[350/391] Loss: 0.4081 Acc:86.21%
Epoch[025/190] Train Acc: 86.12% Valid Acc:82.89% Train loss:0.4114 Valid loss:0.5083 LR:0.1
Training: Epoch[026/190] Iteration[050/391] Loss: 0.3720 Acc:87.28%
Training: Epoch[026/190] Iteration[100/391] Loss: 0.3801 Acc:87.07%
Training: Epoch[026/190] Iteration[150/391] Loss: 0.3838 Acc:87.03%
Training: Epoch[026/190] Iteration[200/391] Loss: 0.3916 Acc:86.65%
Training: Epoch[026/190] Iteration[250/391] Loss: 0.3947 Acc:86.64%
Training: Epoch[026/190] Iteration[300/391] Loss: 0.4012 Acc:86.40%
Training: Epoch[026/190] Iteration[350/391] Loss: 0.4034 Acc:86.30%
Epoch[026/190] Train Acc: 86.17% Valid Acc:83.02% Train loss:0.4056 Valid loss:0.5169 LR:0.1
Training: Epoch[027/190] Iteration[050/391] Loss: 0.3613 Acc:87.59%
Training: Epoch[027/190] Iteration[100/391] Loss: 0.3810 Acc:86.90%
Training: Epoch[027/190] Iteration[150/391] Loss: 0.3831 Acc:86.95%
Training: Epoch[027/190] Iteration[200/391] Loss: 0.3890 Acc:86.68%
Training: Epoch[027/190] Iteration[250/391] Loss: 0.3907 Acc:86.75%
Training: Epoch[027/190] Iteration[300/391] Loss: 0.3943 Acc:86.60%
Training: Epoch[027/190] Iteration[350/391] Loss: 0.3964 Acc:86.47%
Epoch[027/190] Train Acc: 86.49% Valid Acc:83.01% Train loss:0.3954 Valid loss:0.5516 LR:0.1
Training: Epoch[028/190] Iteration[050/391] Loss: 0.3709 Acc:87.30%
Training: Epoch[028/190] Iteration[100/391] Loss: 0.3789 Acc:87.16%
Training: Epoch[028/190] Iteration[150/391] Loss: 0.3741 Acc:87.26%
Training: Epoch[028/190] Iteration[200/391] Loss: 0.3818 Acc:87.18%
Training: Epoch[028/190] Iteration[250/391] Loss: 0.3829 Acc:87.12%
Training: Epoch[028/190] Iteration[300/391] Loss: 0.3859 Acc:86.97%
Training: Epoch[028/190] Iteration[350/391] Loss: 0.3852 Acc:86.98%
Epoch[028/190] Train Acc: 86.91% Valid Acc:82.80% Train loss:0.3869 Valid loss:0.5413 LR:0.1
Training: Epoch[029/190] Iteration[050/391] Loss: 0.3897 Acc:87.56%
Training: Epoch[029/190] Iteration[100/391] Loss: 0.3739 Acc:87.70%
Training: Epoch[029/190] Iteration[150/391] Loss: 0.3720 Acc:87.72%
Training: Epoch[029/190] Iteration[200/391] Loss: 0.3760 Acc:87.59%
Training: Epoch[029/190] Iteration[250/391] Loss: 0.3777 Acc:87.52%
Training: Epoch[029/190] Iteration[300/391] Loss: 0.3788 Acc:87.45%
Training: Epoch[029/190] Iteration[350/391] Loss: 0.3786 Acc:87.36%
Epoch[029/190] Train Acc: 87.39% Valid Acc:83.08% Train loss:0.3781 Valid loss:0.5327 LR:0.1
Training: Epoch[030/190] Iteration[050/391] Loss: 0.3496 Acc:88.36%
Training: Epoch[030/190] Iteration[100/391] Loss: 0.3591 Acc:87.80%
Training: Epoch[030/190] Iteration[150/391] Loss: 0.3629 Acc:87.59%
Training: Epoch[030/190] Iteration[200/391] Loss: 0.3695 Acc:87.30%
Training: Epoch[030/190] Iteration[250/391] Loss: 0.3736 Acc:87.23%
Training: Epoch[030/190] Iteration[300/391] Loss: 0.3754 Acc:87.22%
Training: Epoch[030/190] Iteration[350/391] Loss: 0.3763 Acc:87.25%
Epoch[030/190] Train Acc: 87.24% Valid Acc:83.67% Train loss:0.3757 Valid loss:0.5075 LR:0.1
Training: Epoch[031/190] Iteration[050/391] Loss: 0.3474 Acc:88.16%
Training: Epoch[031/190] Iteration[100/391] Loss: 0.3521 Acc:88.13%
Training: Epoch[031/190] Iteration[150/391] Loss: 0.3575 Acc:87.98%
Training: Epoch[031/190] Iteration[200/391] Loss: 0.3586 Acc:87.77%
Training: Epoch[031/190] Iteration[250/391] Loss: 0.3605 Acc:87.74%
Training: Epoch[031/190] Iteration[300/391] Loss: 0.3630 Acc:87.60%
Training: Epoch[031/190] Iteration[350/391] Loss: 0.3646 Acc:87.58%
Epoch[031/190] Train Acc: 87.47% Valid Acc:82.69% Train loss:0.3687 Valid loss:0.5353 LR:0.1
Training: Epoch[032/190] Iteration[050/391] Loss: 0.3606 Acc:87.56%
Training: Epoch[032/190] Iteration[100/391] Loss: 0.3560 Acc:87.95%
Training: Epoch[032/190] Iteration[150/391] Loss: 0.3562 Acc:87.93%
Training: Epoch[032/190] Iteration[200/391] Loss: 0.3542 Acc:88.02%
Training: Epoch[032/190] Iteration[250/391] Loss: 0.3583 Acc:87.89%
Training: Epoch[032/190] Iteration[300/391] Loss: 0.3615 Acc:87.75%
Training: Epoch[032/190] Iteration[350/391] Loss: 0.3630 Acc:87.75%
Epoch[032/190] Train Acc: 87.74% Valid Acc:84.23% Train loss:0.3640 Valid loss:0.4858 LR:0.1
Training: Epoch[033/190] Iteration[050/391] Loss: 0.3335 Acc:88.92%
Training: Epoch[033/190] Iteration[100/391] Loss: 0.3341 Acc:88.77%
Training: Epoch[033/190] Iteration[150/391] Loss: 0.3467 Acc:88.33%
Training: Epoch[033/190] Iteration[200/391] Loss: 0.3491 Acc:88.19%
Training: Epoch[033/190] Iteration[250/391] Loss: 0.3538 Acc:88.07%
Training: Epoch[033/190] Iteration[300/391] Loss: 0.3553 Acc:88.07%
Training: Epoch[033/190] Iteration[350/391] Loss: 0.3576 Acc:87.96%
Epoch[033/190] Train Acc: 87.90% Valid Acc:85.12% Train loss:0.3575 Valid loss:0.4655 LR:0.1
Training: Epoch[034/190] Iteration[050/391] Loss: 0.3326 Acc:88.47%
Training: Epoch[034/190] Iteration[100/391] Loss: 0.3295 Acc:88.65%
Training: Epoch[034/190] Iteration[150/391] Loss: 0.3369 Acc:88.53%
Training: Epoch[034/190] Iteration[200/391] Loss: 0.3394 Acc:88.46%
Training: Epoch[034/190] Iteration[250/391] Loss: 0.3443 Acc:88.33%
Training: Epoch[034/190] Iteration[300/391] Loss: 0.3437 Acc:88.34%
Training: Epoch[034/190] Iteration[350/391] Loss: 0.3465 Acc:88.31%
Epoch[034/190] Train Acc: 88.23% Valid Acc:85.33% Train loss:0.3499 Valid loss:0.4523 LR:0.1
Training: Epoch[035/190] Iteration[050/391] Loss: 0.3216 Acc:89.11%
Training: Epoch[035/190] Iteration[100/391] Loss: 0.3356 Acc:88.65%
Training: Epoch[035/190] Iteration[150/391] Loss: 0.3395 Acc:88.51%
Training: Epoch[035/190] Iteration[200/391] Loss: 0.3446 Acc:88.35%
Training: Epoch[035/190] Iteration[250/391] Loss: 0.3433 Acc:88.38%
Training: Epoch[035/190] Iteration[300/391] Loss: 0.3431 Acc:88.39%
Training: Epoch[035/190] Iteration[350/391] Loss: 0.3452 Acc:88.32%
Epoch[035/190] Train Acc: 88.28% Valid Acc:83.22% Train loss:0.3468 Valid loss:0.5039 LR:0.1
Training: Epoch[036/190] Iteration[050/391] Loss: 0.3131 Acc:89.59%
Training: Epoch[036/190] Iteration[100/391] Loss: 0.3272 Acc:89.00%
Training: Epoch[036/190] Iteration[150/391] Loss: 0.3316 Acc:88.89%
Training: Epoch[036/190] Iteration[200/391] Loss: 0.3343 Acc:88.77%
Training: Epoch[036/190] Iteration[250/391] Loss: 0.3385 Acc:88.64%
Training: Epoch[036/190] Iteration[300/391] Loss: 0.3385 Acc:88.66%
Training: Epoch[036/190] Iteration[350/391] Loss: 0.3406 Acc:88.55%
Epoch[036/190] Train Acc: 88.47% Valid Acc:84.53% Train loss:0.3430 Valid loss:0.4702 LR:0.1
Training: Epoch[037/190] Iteration[050/391] Loss: 0.3235 Acc:88.69%
Training: Epoch[037/190] Iteration[100/391] Loss: 0.3311 Acc:88.62%
Training: Epoch[037/190] Iteration[150/391] Loss: 0.3237 Acc:89.01%
Training: Epoch[037/190] Iteration[200/391] Loss: 0.3269 Acc:88.98%
Training: Epoch[037/190] Iteration[250/391] Loss: 0.3268 Acc:88.91%
Training: Epoch[037/190] Iteration[300/391] Loss: 0.3288 Acc:88.84%
Training: Epoch[037/190] Iteration[350/391] Loss: 0.3322 Acc:88.67%
Epoch[037/190] Train Acc: 88.56% Valid Acc:83.68% Train loss:0.3353 Valid loss:0.4951 LR:0.1
Training: Epoch[038/190] Iteration[050/391] Loss: 0.3144 Acc:89.59%
Training: Epoch[038/190] Iteration[100/391] Loss: 0.3040 Acc:89.95%
Training: Epoch[038/190] Iteration[150/391] Loss: 0.3135 Acc:89.58%
Training: Epoch[038/190] Iteration[200/391] Loss: 0.3173 Acc:89.49%
Training: Epoch[038/190] Iteration[250/391] Loss: 0.3213 Acc:89.32%
Training: Epoch[038/190] Iteration[300/391] Loss: 0.3238 Acc:89.25%
Training: Epoch[038/190] Iteration[350/391] Loss: 0.3331 Acc:88.95%
Epoch[038/190] Train Acc: 88.91% Valid Acc:85.17% Train loss:0.3330 Valid loss:0.4636 LR:0.1
Training: Epoch[039/190] Iteration[050/391] Loss: 0.3537 Acc:88.14%
Training: Epoch[039/190] Iteration[100/391] Loss: 0.3373 Acc:88.77%
Training: Epoch[039/190] Iteration[150/391] Loss: 0.3289 Acc:88.97%
Training: Epoch[039/190] Iteration[200/391] Loss: 0.3304 Acc:88.88%
Training: Epoch[039/190] Iteration[250/391] Loss: 0.3281 Acc:88.99%
Training: Epoch[039/190] Iteration[300/391] Loss: 0.3314 Acc:88.89%
Training: Epoch[039/190] Iteration[350/391] Loss: 0.3349 Acc:88.73%
Epoch[039/190] Train Acc: 88.77% Valid Acc:84.83% Train loss:0.3348 Valid loss:0.4729 LR:0.1
Training: Epoch[040/190] Iteration[050/391] Loss: 0.2959 Acc:89.36%
Training: Epoch[040/190] Iteration[100/391] Loss: 0.3072 Acc:89.27%
Training: Epoch[040/190] Iteration[150/391] Loss: 0.3175 Acc:89.06%
Training: Epoch[040/190] Iteration[200/391] Loss: 0.3245 Acc:88.87%
Training: Epoch[040/190] Iteration[250/391] Loss: 0.3285 Acc:88.71%
Training: Epoch[040/190] Iteration[300/391] Loss: 0.3287 Acc:88.71%
Training: Epoch[040/190] Iteration[350/391] Loss: 0.3282 Acc:88.75%
Epoch[040/190] Train Acc: 88.74% Valid Acc:84.19% Train loss:0.3289 Valid loss:0.5121 LR:0.1
Training: Epoch[041/190] Iteration[050/391] Loss: 0.3215 Acc:89.31%
Training: Epoch[041/190] Iteration[100/391] Loss: 0.3195 Acc:89.19%
Training: Epoch[041/190] Iteration[150/391] Loss: 0.3194 Acc:89.15%
Training: Epoch[041/190] Iteration[200/391] Loss: 0.3200 Acc:89.18%
Training: Epoch[041/190] Iteration[250/391] Loss: 0.3207 Acc:89.16%
Training: Epoch[041/190] Iteration[300/391] Loss: 0.3189 Acc:89.26%
Training: Epoch[041/190] Iteration[350/391] Loss: 0.3222 Acc:89.19%
Epoch[041/190] Train Acc: 89.14% Valid Acc:85.38% Train loss:0.3240 Valid loss:0.4463 LR:0.1
Training: Epoch[042/190] Iteration[050/391] Loss: 0.3216 Acc:89.25%
Training: Epoch[042/190] Iteration[100/391] Loss: 0.3193 Acc:89.34%
Training: Epoch[042/190] Iteration[150/391] Loss: 0.3184 Acc:89.46%
Training: Epoch[042/190] Iteration[200/391] Loss: 0.3148 Acc:89.58%
Training: Epoch[042/190] Iteration[250/391] Loss: 0.3160 Acc:89.52%
Training: Epoch[042/190] Iteration[300/391] Loss: 0.3190 Acc:89.35%
Training: Epoch[042/190] Iteration[350/391] Loss: 0.3203 Acc:89.28%
Epoch[042/190] Train Acc: 89.23% Valid Acc:84.77% Train loss:0.3221 Valid loss:0.4751 LR:0.1
Training: Epoch[043/190] Iteration[050/391] Loss: 0.2961 Acc:90.25%
Training: Epoch[043/190] Iteration[100/391] Loss: 0.3094 Acc:89.70%
Training: Epoch[043/190] Iteration[150/391] Loss: 0.3138 Acc:89.50%
Training: Epoch[043/190] Iteration[200/391] Loss: 0.3153 Acc:89.50%
Training: Epoch[043/190] Iteration[250/391] Loss: 0.3198 Acc:89.21%
Training: Epoch[043/190] Iteration[300/391] Loss: 0.3183 Acc:89.33%
Training: Epoch[043/190] Iteration[350/391] Loss: 0.3183 Acc:89.29%
Epoch[043/190] Train Acc: 89.28% Valid Acc:85.86% Train loss:0.3185 Valid loss:0.4487 LR:0.1
Training: Epoch[044/190] Iteration[050/391] Loss: 0.2932 Acc:89.80%
Training: Epoch[044/190] Iteration[100/391] Loss: 0.2897 Acc:90.11%
Training: Epoch[044/190] Iteration[150/391] Loss: 0.2958 Acc:89.80%
Training: Epoch[044/190] Iteration[200/391] Loss: 0.3051 Acc:89.53%
Training: Epoch[044/190] Iteration[250/391] Loss: 0.3059 Acc:89.57%
Training: Epoch[044/190] Iteration[300/391] Loss: 0.3059 Acc:89.61%
Training: Epoch[044/190] Iteration[350/391] Loss: 0.3096 Acc:89.52%
Epoch[044/190] Train Acc: 89.53% Valid Acc:86.17% Train loss:0.3097 Valid loss:0.4262 LR:0.1
Training: Epoch[045/190] Iteration[050/391] Loss: 0.2727 Acc:90.45%
Training: Epoch[045/190] Iteration[100/391] Loss: 0.2974 Acc:89.69%
Training: Epoch[045/190] Iteration[150/391] Loss: 0.2968 Acc:89.96%
Training: Epoch[045/190] Iteration[200/391] Loss: 0.2998 Acc:89.85%
Training: Epoch[045/190] Iteration[250/391] Loss: 0.3037 Acc:89.73%
Training: Epoch[045/190] Iteration[300/391] Loss: 0.3093 Acc:89.55%
Training: Epoch[045/190] Iteration[350/391] Loss: 0.3104 Acc:89.52%
Epoch[045/190] Train Acc: 89.47% Valid Acc:83.86% Train loss:0.3122 Valid loss:0.5037 LR:0.1
Training: Epoch[046/190] Iteration[050/391] Loss: 0.2843 Acc:90.59%
Training: Epoch[046/190] Iteration[100/391] Loss: 0.2910 Acc:90.24%
Training: Epoch[046/190] Iteration[150/391] Loss: 0.2927 Acc:90.16%
Training: Epoch[046/190] Iteration[200/391] Loss: 0.2947 Acc:90.10%
Training: Epoch[046/190] Iteration[250/391] Loss: 0.3005 Acc:89.88%
Training: Epoch[046/190] Iteration[300/391] Loss: 0.2987 Acc:89.96%
Training: Epoch[046/190] Iteration[350/391] Loss: 0.3005 Acc:89.86%
Epoch[046/190] Train Acc: 89.79% Valid Acc:83.74% Train loss:0.3035 Valid loss:0.5104 LR:0.1
Training: Epoch[047/190] Iteration[050/391] Loss: 0.2971 Acc:89.91%
Training: Epoch[047/190] Iteration[100/391] Loss: 0.2858 Acc:90.37%
Training: Epoch[047/190] Iteration[150/391] Loss: 0.2892 Acc:90.33%
Training: Epoch[047/190] Iteration[200/391] Loss: 0.2903 Acc:90.24%
Training: Epoch[047/190] Iteration[250/391] Loss: 0.2915 Acc:90.17%
Training: Epoch[047/190] Iteration[300/391] Loss: 0.2922 Acc:90.12%
Training: Epoch[047/190] Iteration[350/391] Loss: 0.2935 Acc:90.06%
Epoch[047/190] Train Acc: 90.00% Valid Acc:85.96% Train loss:0.2968 Valid loss:0.4430 LR:0.1
Training: Epoch[048/190] Iteration[050/391] Loss: 0.2801 Acc:90.70%
Training: Epoch[048/190] Iteration[100/391] Loss: 0.2787 Acc:90.58%
Training: Epoch[048/190] Iteration[150/391] Loss: 0.2865 Acc:90.36%
Training: Epoch[048/190] Iteration[200/391] Loss: 0.2893 Acc:90.27%
Training: Epoch[048/190] Iteration[250/391] Loss: 0.2897 Acc:90.24%
Training: Epoch[048/190] Iteration[300/391] Loss: 0.2939 Acc:90.12%
Training: Epoch[048/190] Iteration[350/391] Loss: 0.2967 Acc:90.06%
Epoch[048/190] Train Acc: 90.06% Valid Acc:86.03% Train loss:0.2969 Valid loss:0.4267 LR:0.1
Training: Epoch[049/190] Iteration[050/391] Loss: 0.2739 Acc:91.08%
Training: Epoch[049/190] Iteration[100/391] Loss: 0.2875 Acc:90.44%
Training: Epoch[049/190] Iteration[150/391] Loss: 0.2920 Acc:90.33%
Training: Epoch[049/190] Iteration[200/391] Loss: 0.2949 Acc:90.21%
Training: Epoch[049/190] Iteration[250/391] Loss: 0.2985 Acc:90.05%
Training: Epoch[049/190] Iteration[300/391] Loss: 0.2972 Acc:90.06%
Training: Epoch[049/190] Iteration[350/391] Loss: 0.2998 Acc:89.92%
Epoch[049/190] Train Acc: 89.97% Valid Acc:85.21% Train loss:0.3004 Valid loss:0.4858 LR:0.1
Training: Epoch[050/190] Iteration[050/391] Loss: 0.2765 Acc:90.31%
Training: Epoch[050/190] Iteration[100/391] Loss: 0.2794 Acc:90.45%
Training: Epoch[050/190] Iteration[150/391] Loss: 0.2833 Acc:90.34%
Training: Epoch[050/190] Iteration[200/391] Loss: 0.2892 Acc:90.17%
Training: Epoch[050/190] Iteration[250/391] Loss: 0.2907 Acc:90.14%
Training: Epoch[050/190] Iteration[300/391] Loss: 0.2938 Acc:90.05%
Training: Epoch[050/190] Iteration[350/391] Loss: 0.2949 Acc:90.02%
Epoch[050/190] Train Acc: 89.92% Valid Acc:84.65% Train loss:0.2979 Valid loss:0.4933 LR:0.1
Training: Epoch[051/190] Iteration[050/391] Loss: 0.2888 Acc:90.16%
Training: Epoch[051/190] Iteration[100/391] Loss: 0.2919 Acc:89.98%
Training: Epoch[051/190] Iteration[150/391] Loss: 0.2895 Acc:90.23%
Training: Epoch[051/190] Iteration[200/391] Loss: 0.2888 Acc:90.20%
Training: Epoch[051/190] Iteration[250/391] Loss: 0.2842 Acc:90.31%
Training: Epoch[051/190] Iteration[300/391] Loss: 0.2897 Acc:90.11%
Training: Epoch[051/190] Iteration[350/391] Loss: 0.2914 Acc:90.07%
Epoch[051/190] Train Acc: 90.08% Valid Acc:85.75% Train loss:0.2907 Valid loss:0.4531 LR:0.1
Training: Epoch[052/190] Iteration[050/391] Loss: 0.2751 Acc:90.78%
Training: Epoch[052/190] Iteration[100/391] Loss: 0.2746 Acc:90.75%
Training: Epoch[052/190] Iteration[150/391] Loss: 0.2845 Acc:90.43%
Training: Epoch[052/190] Iteration[200/391] Loss: 0.2858 Acc:90.31%
Training: Epoch[052/190] Iteration[250/391] Loss: 0.2853 Acc:90.36%
Training: Epoch[052/190] Iteration[300/391] Loss: 0.2831 Acc:90.43%
Training: Epoch[052/190] Iteration[350/391] Loss: 0.2841 Acc:90.44%
Epoch[052/190] Train Acc: 90.34% Valid Acc:85.11% Train loss:0.2868 Valid loss:0.4503 LR:0.1
Training: Epoch[053/190] Iteration[050/391] Loss: 0.2672 Acc:91.28%
Training: Epoch[053/190] Iteration[100/391] Loss: 0.2659 Acc:90.98%
Training: Epoch[053/190] Iteration[150/391] Loss: 0.2711 Acc:90.86%
Training: Epoch[053/190] Iteration[200/391] Loss: 0.2760 Acc:90.71%
Training: Epoch[053/190] Iteration[250/391] Loss: 0.2777 Acc:90.65%
Training: Epoch[053/190] Iteration[300/391] Loss: 0.2823 Acc:90.49%
Training: Epoch[053/190] Iteration[350/391] Loss: 0.2831 Acc:90.51%
Epoch[053/190] Train Acc: 90.37% Valid Acc:85.96% Train loss:0.2868 Valid loss:0.4298 LR:0.1
Training: Epoch[054/190] Iteration[050/391] Loss: 0.2648 Acc:90.94%
Training: Epoch[054/190] Iteration[100/391] Loss: 0.2648 Acc:90.83%
Training: Epoch[054/190] Iteration[150/391] Loss: 0.2726 Acc:90.67%
Training: Epoch[054/190] Iteration[200/391] Loss: 0.2728 Acc:90.73%
Training: Epoch[054/190] Iteration[250/391] Loss: 0.2758 Acc:90.78%
Training: Epoch[054/190] Iteration[300/391] Loss: 0.2769 Acc:90.70%
Training: Epoch[054/190] Iteration[350/391] Loss: 0.2769 Acc:90.72%
Epoch[054/190] Train Acc: 90.58% Valid Acc:85.75% Train loss:0.2797 Valid loss:0.4621 LR:0.1
Training: Epoch[055/190] Iteration[050/391] Loss: 0.2591 Acc:91.34%
Training: Epoch[055/190] Iteration[100/391] Loss: 0.2681 Acc:91.09%
Training: Epoch[055/190] Iteration[150/391] Loss: 0.2653 Acc:91.18%
Training: Epoch[055/190] Iteration[200/391] Loss: 0.2704 Acc:91.04%
Training: Epoch[055/190] Iteration[250/391] Loss: 0.2738 Acc:90.92%
Training: Epoch[055/190] Iteration[300/391] Loss: 0.2752 Acc:90.76%
Training: Epoch[055/190] Iteration[350/391] Loss: 0.2796 Acc:90.64%
Epoch[055/190] Train Acc: 90.58% Valid Acc:86.94% Train loss:0.2805 Valid loss:0.4226 LR:0.1
Training: Epoch[056/190] Iteration[050/391] Loss: 0.2701 Acc:90.91%
Training: Epoch[056/190] Iteration[100/391] Loss: 0.2659 Acc:90.77%
Training: Epoch[056/190] Iteration[150/391] Loss: 0.2717 Acc:90.73%
Training: Epoch[056/190] Iteration[200/391] Loss: 0.2737 Acc:90.70%
Training: Epoch[056/190] Iteration[250/391] Loss: 0.2784 Acc:90.61%
Training: Epoch[056/190] Iteration[300/391] Loss: 0.2771 Acc:90.62%
Training: Epoch[056/190] Iteration[350/391] Loss: 0.2813 Acc:90.46%
Epoch[056/190] Train Acc: 90.42% Valid Acc:85.78% Train loss:0.2831 Valid loss:0.4460 LR:0.1
Training: Epoch[057/190] Iteration[050/391] Loss: 0.2526 Acc:91.31%
Training: Epoch[057/190] Iteration[100/391] Loss: 0.2542 Acc:91.31%
Training: Epoch[057/190] Iteration[150/391] Loss: 0.2584 Acc:91.15%
Training: Epoch[057/190] Iteration[200/391] Loss: 0.2683 Acc:90.88%
Training: Epoch[057/190] Iteration[250/391] Loss: 0.2689 Acc:90.85%
Training: Epoch[057/190] Iteration[300/391] Loss: 0.2712 Acc:90.80%
Training: Epoch[057/190] Iteration[350/391] Loss: 0.2744 Acc:90.68%
Epoch[057/190] Train Acc: 90.59% Valid Acc:85.94% Train loss:0.2769 Valid loss:0.4575 LR:0.1
Training: Epoch[058/190] Iteration[050/391] Loss: 0.2612 Acc:91.30%
Training: Epoch[058/190] Iteration[100/391] Loss: 0.2599 Acc:91.34%
Training: Epoch[058/190] Iteration[150/391] Loss: 0.2635 Acc:91.20%
Training: Epoch[058/190] Iteration[200/391] Loss: 0.2632 Acc:91.17%
Training: Epoch[058/190] Iteration[250/391] Loss: 0.2666 Acc:91.03%
Training: Epoch[058/190] Iteration[300/391] Loss: 0.2677 Acc:91.00%
Training: Epoch[058/190] Iteration[350/391] Loss: 0.2692 Acc:90.98%
Epoch[058/190] Train Acc: 90.87% Valid Acc:86.33% Train loss:0.2704 Valid loss:0.4099 LR:0.1
Training: Epoch[059/190] Iteration[050/391] Loss: 0.2545 Acc:91.39%
Training: Epoch[059/190] Iteration[100/391] Loss: 0.2641 Acc:91.18%
Training: Epoch[059/190] Iteration[150/391] Loss: 0.2676 Acc:91.01%
Training: Epoch[059/190] Iteration[200/391] Loss: 0.2640 Acc:91.13%
Training: Epoch[059/190] Iteration[250/391] Loss: 0.2704 Acc:90.93%
Training: Epoch[059/190] Iteration[300/391] Loss: 0.2731 Acc:90.76%
Training: Epoch[059/190] Iteration[350/391] Loss: 0.2757 Acc:90.66%
Epoch[059/190] Train Acc: 90.60% Valid Acc:85.98% Train loss:0.2778 Valid loss:0.4375 LR:0.1
Training: Epoch[060/190] Iteration[050/391] Loss: 0.2476 Acc:91.64%
Training: Epoch[060/190] Iteration[100/391] Loss: 0.2587 Acc:91.38%
Training: Epoch[060/190] Iteration[150/391] Loss: 0.2651 Acc:91.19%
Training: Epoch[060/190] Iteration[200/391] Loss: 0.2673 Acc:91.14%
Training: Epoch[060/190] Iteration[250/391] Loss: 0.2683 Acc:91.10%
Training: Epoch[060/190] Iteration[300/391] Loss: 0.2698 Acc:90.98%
Training: Epoch[060/190] Iteration[350/391] Loss: 0.2716 Acc:90.85%
Epoch[060/190] Train Acc: 90.84% Valid Acc:86.74% Train loss:0.2718 Valid loss:0.4192 LR:0.1
Training: Epoch[061/190] Iteration[050/391] Loss: 0.2476 Acc:91.55%
Training: Epoch[061/190] Iteration[100/391] Loss: 0.2622 Acc:91.18%
Training: Epoch[061/190] Iteration[150/391] Loss: 0.2629 Acc:91.33%
Training: Epoch[061/190] Iteration[200/391] Loss: 0.2660 Acc:91.23%
Training: Epoch[061/190] Iteration[250/391] Loss: 0.2678 Acc:91.08%
Training: Epoch[061/190] Iteration[300/391] Loss: 0.2695 Acc:91.06%
Training: Epoch[061/190] Iteration[350/391] Loss: 0.2681 Acc:91.06%
Epoch[061/190] Train Acc: 90.98% Valid Acc:86.29% Train loss:0.2704 Valid loss:0.4372 LR:0.1
Training: Epoch[062/190] Iteration[050/391] Loss: 0.2545 Acc:91.25%
Training: Epoch[062/190] Iteration[100/391] Loss: 0.2558 Acc:91.39%
Training: Epoch[062/190] Iteration[150/391] Loss: 0.2544 Acc:91.40%
Training: Epoch[062/190] Iteration[200/391] Loss: 0.2599 Acc:91.32%
Training: Epoch[062/190] Iteration[250/391] Loss: 0.2619 Acc:91.27%
Training: Epoch[062/190] Iteration[300/391] Loss: 0.2634 Acc:91.18%
Training: Epoch[062/190] Iteration[350/391] Loss: 0.2641 Acc:91.10%
Epoch[062/190] Train Acc: 91.07% Valid Acc:85.43% Train loss:0.2651 Valid loss:0.4688 LR:0.1
Training: Epoch[063/190] Iteration[050/391] Loss: 0.2663 Acc:90.77%
Training: Epoch[063/190] Iteration[100/391] Loss: 0.2633 Acc:91.19%
Training: Epoch[063/190] Iteration[150/391] Loss: 0.2629 Acc:91.12%
Training: Epoch[063/190] Iteration[200/391] Loss: 0.2660 Acc:91.11%
Training: Epoch[063/190] Iteration[250/391] Loss: 0.2671 Acc:91.10%
Training: Epoch[063/190] Iteration[300/391] Loss: 0.2682 Acc:90.98%
Training: Epoch[063/190] Iteration[350/391] Loss: 0.2682 Acc:90.96%
Epoch[063/190] Train Acc: 90.90% Valid Acc:85.26% Train loss:0.2697 Valid loss:0.4681 LR:0.1
Training: Epoch[064/190] Iteration[050/391] Loss: 0.2182 Acc:92.78%
Training: Epoch[064/190] Iteration[100/391] Loss: 0.2395 Acc:91.97%
Training: Epoch[064/190] Iteration[150/391] Loss: 0.2520 Acc:91.56%
Training: Epoch[064/190] Iteration[200/391] Loss: 0.2528 Acc:91.43%
Training: Epoch[064/190] Iteration[250/391] Loss: 0.2552 Acc:91.42%
Training: Epoch[064/190] Iteration[300/391] Loss: 0.2571 Acc:91.42%
Training: Epoch[064/190] Iteration[350/391] Loss: 0.2601 Acc:91.32%
Epoch[064/190] Train Acc: 91.29% Valid Acc:86.63% Train loss:0.2611 Valid loss:0.4349 LR:0.1
Training: Epoch[065/190] Iteration[050/391] Loss: 0.2549 Acc:91.28%
Training: Epoch[065/190] Iteration[100/391] Loss: 0.2446 Acc:91.66%
Training: Epoch[065/190] Iteration[150/391] Loss: 0.2618 Acc:91.11%
Training: Epoch[065/190] Iteration[200/391] Loss: 0.2624 Acc:91.24%
Training: Epoch[065/190] Iteration[250/391] Loss: 0.2622 Acc:91.29%
Training: Epoch[065/190] Iteration[300/391] Loss: 0.2620 Acc:91.28%
Training: Epoch[065/190] Iteration[350/391] Loss: 0.2621 Acc:91.29%
Epoch[065/190] Train Acc: 91.24% Valid Acc:86.89% Train loss:0.2653 Valid loss:0.3955 LR:0.1
Training: Epoch[066/190] Iteration[050/391] Loss: 0.2385 Acc:92.08%
Training: Epoch[066/190] Iteration[100/391] Loss: 0.2550 Acc:91.53%
Training: Epoch[066/190] Iteration[150/391] Loss: 0.2570 Acc:91.45%
Training: Epoch[066/190] Iteration[200/391] Loss: 0.2544 Acc:91.53%
Training: Epoch[066/190] Iteration[250/391] Loss: 0.2583 Acc:91.42%
Training: Epoch[066/190] Iteration[300/391] Loss: 0.2590 Acc:91.41%
Training: Epoch[066/190] Iteration[350/391] Loss: 0.2593 Acc:91.41%
Epoch[066/190] Train Acc: 91.32% Valid Acc:84.08% Train loss:0.2619 Valid loss:0.5321 LR:0.1
Training: Epoch[067/190] Iteration[050/391] Loss: 0.2588 Acc:91.34%
Training: Epoch[067/190] Iteration[100/391] Loss: 0.2548 Acc:91.48%
Training: Epoch[067/190] Iteration[150/391] Loss: 0.2527 Acc:91.48%
Training: Epoch[067/190] Iteration[200/391] Loss: 0.2533 Acc:91.45%
Training: Epoch[067/190] Iteration[250/391] Loss: 0.2562 Acc:91.33%
Training: Epoch[067/190] Iteration[300/391] Loss: 0.2565 Acc:91.39%
Training: Epoch[067/190] Iteration[350/391] Loss: 0.2568 Acc:91.36%
Epoch[067/190] Train Acc: 91.34% Valid Acc:87.12% Train loss:0.2574 Valid loss:0.4145 LR:0.1
Training: Epoch[068/190] Iteration[050/391] Loss: 0.2330 Acc:92.17%
Training: Epoch[068/190] Iteration[100/391] Loss: 0.2362 Acc:92.10%
Training: Epoch[068/190] Iteration[150/391] Loss: 0.2343 Acc:92.02%
Training: Epoch[068/190] Iteration[200/391] Loss: 0.2439 Acc:91.71%
Training: Epoch[068/190] Iteration[250/391] Loss: 0.2457 Acc:91.62%
Training: Epoch[068/190] Iteration[300/391] Loss: 0.2483 Acc:91.58%
Training: Epoch[068/190] Iteration[350/391] Loss: 0.2519 Acc:91.44%
Epoch[068/190] Train Acc: 91.37% Valid Acc:85.90% Train loss:0.2547 Valid loss:0.4456 LR:0.1
Training: Epoch[069/190] Iteration[050/391] Loss: 0.2490 Acc:91.23%
Training: Epoch[069/190] Iteration[100/391] Loss: 0.2547 Acc:91.12%
Training: Epoch[069/190] Iteration[150/391] Loss: 0.2603 Acc:91.01%
Training: Epoch[069/190] Iteration[200/391] Loss: 0.2572 Acc:91.08%
Training: Epoch[069/190] Iteration[250/391] Loss: 0.2597 Acc:91.01%
Training: Epoch[069/190] Iteration[300/391] Loss: 0.2615 Acc:90.98%
Training: Epoch[069/190] Iteration[350/391] Loss: 0.2596 Acc:91.15%
Epoch[069/190] Train Acc: 91.16% Valid Acc:85.88% Train loss:0.2589 Valid loss:0.4566 LR:0.1
Training: Epoch[070/190] Iteration[050/391] Loss: 0.2337 Acc:92.38%
Training: Epoch[070/190] Iteration[100/391] Loss: 0.2396 Acc:91.96%
Training: Epoch[070/190] Iteration[150/391] Loss: 0.2443 Acc:91.77%
Training: Epoch[070/190] Iteration[200/391] Loss: 0.2456 Acc:91.72%
Training: Epoch[070/190] Iteration[250/391] Loss: 0.2456 Acc:91.68%
Training: Epoch[070/190] Iteration[300/391] Loss: 0.2519 Acc:91.44%
Training: Epoch[070/190] Iteration[350/391] Loss: 0.2532 Acc:91.46%
Epoch[070/190] Train Acc: 91.42% Valid Acc:86.75% Train loss:0.2546 Valid loss:0.4240 LR:0.1
Training: Epoch[071/190] Iteration[050/391] Loss: 0.2257 Acc:92.67%
Training: Epoch[071/190] Iteration[100/391] Loss: 0.2311 Acc:92.30%
Training: Epoch[071/190] Iteration[150/391] Loss: 0.2364 Acc:92.19%
Training: Epoch[071/190] Iteration[200/391] Loss: 0.2405 Acc:92.06%
Training: Epoch[071/190] Iteration[250/391] Loss: 0.2410 Acc:91.96%
Training: Epoch[071/190] Iteration[300/391] Loss: 0.2423 Acc:91.87%
Training: Epoch[071/190] Iteration[350/391] Loss: 0.2433 Acc:91.83%
Epoch[071/190] Train Acc: 91.77% Valid Acc:85.77% Train loss:0.2453 Valid loss:0.4413 LR:0.1
Training: Epoch[072/190] Iteration[050/391] Loss: 0.2410 Acc:92.08%
Training: Epoch[072/190] Iteration[100/391] Loss: 0.2490 Acc:91.56%
Training: Epoch[072/190] Iteration[150/391] Loss: 0.2443 Acc:91.78%
Training: Epoch[072/190] Iteration[200/391] Loss: 0.2484 Acc:91.70%
Training: Epoch[072/190] Iteration[250/391] Loss: 0.2529 Acc:91.55%
Training: Epoch[072/190] Iteration[300/391] Loss: 0.2512 Acc:91.55%
Training: Epoch[072/190] Iteration[350/391] Loss: 0.2507 Acc:91.59%
Epoch[072/190] Train Acc: 91.46% Valid Acc:86.80% Train loss:0.2530 Valid loss:0.4219 LR:0.1
Training: Epoch[073/190] Iteration[050/391] Loss: 0.2439 Acc:91.48%
Training: Epoch[073/190] Iteration[100/391] Loss: 0.2504 Acc:91.35%
Training: Epoch[073/190] Iteration[150/391] Loss: 0.2487 Acc:91.45%
Training: Epoch[073/190] Iteration[200/391] Loss: 0.2490 Acc:91.46%
Training: Epoch[073/190] Iteration[250/391] Loss: 0.2449 Acc:91.70%
Training: Epoch[073/190] Iteration[300/391] Loss: 0.2484 Acc:91.67%
Training: Epoch[073/190] Iteration[350/391] Loss: 0.2489 Acc:91.66%
Epoch[073/190] Train Acc: 91.60% Valid Acc:85.67% Train loss:0.2508 Valid loss:0.4553 LR:0.1
Training: Epoch[074/190] Iteration[050/391] Loss: 0.2288 Acc:92.44%
Training: Epoch[074/190] Iteration[100/391] Loss: 0.2435 Acc:91.90%
Training: Epoch[074/190] Iteration[150/391] Loss: 0.2462 Acc:91.81%
Training: Epoch[074/190] Iteration[200/391] Loss: 0.2505 Acc:91.61%
Training: Epoch[074/190] Iteration[250/391] Loss: 0.2461 Acc:91.74%
Training: Epoch[074/190] Iteration[300/391] Loss: 0.2460 Acc:91.70%
Training: Epoch[074/190] Iteration[350/391] Loss: 0.2454 Acc:91.77%
Epoch[074/190] Train Acc: 91.69% Valid Acc:86.43% Train loss:0.2477 Valid loss:0.4204 LR:0.1
Training: Epoch[075/190] Iteration[050/391] Loss: 0.2449 Acc:91.69%
Training: Epoch[075/190] Iteration[100/391] Loss: 0.2393 Acc:91.84%
Training: Epoch[075/190] Iteration[150/391] Loss: 0.2394 Acc:91.84%
Training: Epoch[075/190] Iteration[200/391] Loss: 0.2405 Acc:91.88%
Training: Epoch[075/190] Iteration[250/391] Loss: 0.2408 Acc:91.87%
Training: Epoch[075/190] Iteration[300/391] Loss: 0.2418 Acc:91.83%
Training: Epoch[075/190] Iteration[350/391] Loss: 0.2442 Acc:91.75%
Epoch[075/190] Train Acc: 91.70% Valid Acc:83.30% Train loss:0.2457 Valid loss:0.5493 LR:0.1
Training: Epoch[076/190] Iteration[050/391] Loss: 0.2347 Acc:91.88%
Training: Epoch[076/190] Iteration[100/391] Loss: 0.2315 Acc:92.16%
Training: Epoch[076/190] Iteration[150/391] Loss: 0.2300 Acc:92.27%
Training: Epoch[076/190] Iteration[200/391] Loss: 0.2349 Acc:92.09%
Training: Epoch[076/190] Iteration[250/391] Loss: 0.2385 Acc:91.92%
Training: Epoch[076/190] Iteration[300/391] Loss: 0.2388 Acc:91.82%
Training: Epoch[076/190] Iteration[350/391] Loss: 0.2402 Acc:91.74%
Epoch[076/190] Train Acc: 91.59% Valid Acc:85.23% Train loss:0.2455 Valid loss:0.4984 LR:0.1
Training: Epoch[077/190] Iteration[050/391] Loss: 0.2083 Acc:93.00%
Training: Epoch[077/190] Iteration[100/391] Loss: 0.2236 Acc:92.52%
Training: Epoch[077/190] Iteration[150/391] Loss: 0.2250 Acc:92.52%
Training: Epoch[077/190] Iteration[200/391] Loss: 0.2300 Acc:92.29%
Training: Epoch[077/190] Iteration[250/391] Loss: 0.2350 Acc:92.13%
Training: Epoch[077/190] Iteration[300/391] Loss: 0.2396 Acc:92.00%
Training: Epoch[077/190] Iteration[350/391] Loss: 0.2426 Acc:91.90%
Epoch[077/190] Train Acc: 91.81% Valid Acc:87.20% Train loss:0.2441 Valid loss:0.3974 LR:0.1
Training: Epoch[078/190] Iteration[050/391] Loss: 0.2365 Acc:92.08%
Training: Epoch[078/190] Iteration[100/391] Loss: 0.2353 Acc:92.08%
Training: Epoch[078/190] Iteration[150/391] Loss: 0.2422 Acc:91.90%
Training: Epoch[078/190] Iteration[200/391] Loss: 0.2433 Acc:91.89%
Training: Epoch[078/190] Iteration[250/391] Loss: 0.2452 Acc:91.76%
Training: Epoch[078/190] Iteration[300/391] Loss: 0.2427 Acc:91.86%
Training: Epoch[078/190] Iteration[350/391] Loss: 0.2447 Acc:91.78%
Epoch[078/190] Train Acc: 91.70% Valid Acc:87.19% Train loss:0.2466 Valid loss:0.3976 LR:0.1
Training: Epoch[079/190] Iteration[050/391] Loss: 0.2312 Acc:92.19%
Training: Epoch[079/190] Iteration[100/391] Loss: 0.2385 Acc:91.99%
Training: Epoch[079/190] Iteration[150/391] Loss: 0.2391 Acc:92.16%
Training: Epoch[079/190] Iteration[200/391] Loss: 0.2385 Acc:92.10%
Training: Epoch[079/190] Iteration[250/391] Loss: 0.2403 Acc:92.03%
Training: Epoch[079/190] Iteration[300/391] Loss: 0.2430 Acc:91.95%
Training: Epoch[079/190] Iteration[350/391] Loss: 0.2456 Acc:91.89%
Epoch[079/190] Train Acc: 91.79% Valid Acc:84.47% Train loss:0.2473 Valid loss:0.4963 LR:0.1
Training: Epoch[080/190] Iteration[050/391] Loss: 0.2322 Acc:92.17%
Training: Epoch[080/190] Iteration[100/391] Loss: 0.2306 Acc:92.24%
Training: Epoch[080/190] Iteration[150/391] Loss: 0.2293 Acc:92.36%
Training: Epoch[080/190] Iteration[200/391] Loss: 0.2349 Acc:92.12%
Training: Epoch[080/190] Iteration[250/391] Loss: 0.2362 Acc:92.06%
Training: Epoch[080/190] Iteration[300/391] Loss: 0.2384 Acc:91.97%
Training: Epoch[080/190] Iteration[350/391] Loss: 0.2399 Acc:91.95%
Epoch[080/190] Train Acc: 91.93% Valid Acc:86.43% Train loss:0.2403 Valid loss:0.4371 LR:0.1
Training: Epoch[081/190] Iteration[050/391] Loss: 0.2469 Acc:91.83%
Training: Epoch[081/190] Iteration[100/391] Loss: 0.2395 Acc:91.90%
Training: Epoch[081/190] Iteration[150/391] Loss: 0.2399 Acc:91.86%
Training: Epoch[081/190] Iteration[200/391] Loss: 0.2417 Acc:91.90%
Training: Epoch[081/190] Iteration[250/391] Loss: 0.2384 Acc:91.93%
Training: Epoch[081/190] Iteration[300/391] Loss: 0.2390 Acc:91.97%
Training: Epoch[081/190] Iteration[350/391] Loss: 0.2396 Acc:91.89%
Epoch[081/190] Train Acc: 91.85% Valid Acc:87.59% Train loss:0.2408 Valid loss:0.4056 LR:0.1
Training: Epoch[082/190] Iteration[050/391] Loss: 0.2287 Acc:92.00%
Training: Epoch[082/190] Iteration[100/391] Loss: 0.2244 Acc:92.20%
Training: Epoch[082/190] Iteration[150/391] Loss: 0.2371 Acc:91.92%
Training: Epoch[082/190] Iteration[200/391] Loss: 0.2439 Acc:91.77%
Training: Epoch[082/190] Iteration[250/391] Loss: 0.2439 Acc:91.76%
Training: Epoch[082/190] Iteration[300/391] Loss: 0.2456 Acc:91.69%
Training: Epoch[082/190] Iteration[350/391] Loss: 0.2462 Acc:91.70%
Epoch[082/190] Train Acc: 91.63% Valid Acc:86.78% Train loss:0.2483 Valid loss:0.4271 LR:0.1
Training: Epoch[083/190] Iteration[050/391] Loss: 0.2387 Acc:91.80%
Training: Epoch[083/190] Iteration[100/391] Loss: 0.2310 Acc:92.14%
Training: Epoch[083/190] Iteration[150/391] Loss: 0.2340 Acc:92.11%
Training: Epoch[083/190] Iteration[200/391] Loss: 0.2375 Acc:91.97%
Training: Epoch[083/190] Iteration[250/391] Loss: 0.2439 Acc:91.81%
Training: Epoch[083/190] Iteration[300/391] Loss: 0.2467 Acc:91.65%
Training: Epoch[083/190] Iteration[350/391] Loss: 0.2459 Acc:91.69%
Epoch[083/190] Train Acc: 91.64% Valid Acc:86.95% Train loss:0.2481 Valid loss:0.4175 LR:0.1
Training: Epoch[084/190] Iteration[050/391] Loss: 0.2340 Acc:92.41%
Training: Epoch[084/190] Iteration[100/391] Loss: 0.2356 Acc:92.24%
Training: Epoch[084/190] Iteration[150/391] Loss: 0.2398 Acc:92.00%
Training: Epoch[084/190] Iteration[200/391] Loss: 0.2428 Acc:91.88%
Training: Epoch[084/190] Iteration[250/391] Loss: 0.2424 Acc:91.90%
Training: Epoch[084/190] Iteration[300/391] Loss: 0.2446 Acc:91.84%
Training: Epoch[084/190] Iteration[350/391] Loss: 0.2453 Acc:91.86%
Epoch[084/190] Train Acc: 91.85% Valid Acc:87.69% Train loss:0.2454 Valid loss:0.3900 LR:0.1
Training: Epoch[085/190] Iteration[050/391] Loss: 0.2055 Acc:93.05%
Training: Epoch[085/190] Iteration[100/391] Loss: 0.2185 Acc:92.48%
Training: Epoch[085/190] Iteration[150/391] Loss: 0.2170 Acc:92.51%
Training: Epoch[085/190] Iteration[200/391] Loss: 0.2254 Acc:92.17%
Training: Epoch[085/190] Iteration[250/391] Loss: 0.2299 Acc:92.02%
Training: Epoch[085/190] Iteration[300/391] Loss: 0.2322 Acc:92.00%
Training: Epoch[085/190] Iteration[350/391] Loss: 0.2329 Acc:91.98%
Epoch[085/190] Train Acc: 91.90% Valid Acc:85.53% Train loss:0.2358 Valid loss:0.4541 LR:0.1
Training: Epoch[086/190] Iteration[050/391] Loss: 0.2138 Acc:93.00%
Training: Epoch[086/190] Iteration[100/391] Loss: 0.2125 Acc:92.90%
Training: Epoch[086/190] Iteration[150/391] Loss: 0.2191 Acc:92.76%
Training: Epoch[086/190] Iteration[200/391] Loss: 0.2270 Acc:92.41%
Training: Epoch[086/190] Iteration[250/391] Loss: 0.2270 Acc:92.36%
Training: Epoch[086/190] Iteration[300/391] Loss: 0.2291 Acc:92.29%
Training: Epoch[086/190] Iteration[350/391] Loss: 0.2333 Acc:92.09%
Epoch[086/190] Train Acc: 92.08% Valid Acc:86.47% Train loss:0.2342 Valid loss:0.4331 LR:0.1
Training: Epoch[087/190] Iteration[050/391] Loss: 0.2213 Acc:92.53%
Training: Epoch[087/190] Iteration[100/391] Loss: 0.2200 Acc:92.75%
Training: Epoch[087/190] Iteration[150/391] Loss: 0.2255 Acc:92.64%
Training: Epoch[087/190] Iteration[200/391] Loss: 0.2232 Acc:92.61%
Training: Epoch[087/190] Iteration[250/391] Loss: 0.2304 Acc:92.29%
Training: Epoch[087/190] Iteration[300/391] Loss: 0.2316 Acc:92.22%
Training: Epoch[087/190] Iteration[350/391] Loss: 0.2324 Acc:92.16%
Epoch[087/190] Train Acc: 92.13% Valid Acc:86.78% Train loss:0.2335 Valid loss:0.4208 LR:0.1
Training: Epoch[088/190] Iteration[050/391] Loss: 0.2345 Acc:92.34%
Training: Epoch[088/190] Iteration[100/391] Loss: 0.2352 Acc:92.12%
Training: Epoch[088/190] Iteration[150/391] Loss: 0.2365 Acc:91.94%
Training: Epoch[088/190] Iteration[200/391] Loss: 0.2381 Acc:91.95%
Training: Epoch[088/190] Iteration[250/391] Loss: 0.2332 Acc:92.10%
Training: Epoch[088/190] Iteration[300/391] Loss: 0.2337 Acc:92.07%
Training: Epoch[088/190] Iteration[350/391] Loss: 0.2331 Acc:92.15%
Epoch[088/190] Train Acc: 92.11% Valid Acc:86.28% Train loss:0.2329 Valid loss:0.4571 LR:0.1
Training: Epoch[089/190] Iteration[050/391] Loss: 0.2346 Acc:92.00%
Training: Epoch[089/190] Iteration[100/391] Loss: 0.2358 Acc:91.94%
Training: Epoch[089/190] Iteration[150/391] Loss: 0.2328 Acc:92.12%
Training: Epoch[089/190] Iteration[200/391] Loss: 0.2323 Acc:92.14%
Training: Epoch[089/190] Iteration[250/391] Loss: 0.2356 Acc:91.98%
Training: Epoch[089/190] Iteration[300/391] Loss: 0.2359 Acc:92.05%
Training: Epoch[089/190] Iteration[350/391] Loss: 0.2366 Acc:92.05%
Epoch[089/190] Train Acc: 92.00% Valid Acc:87.95% Train loss:0.2394 Valid loss:0.3983 LR:0.1
Training: Epoch[090/190] Iteration[050/391] Loss: 0.2248 Acc:92.66%
Training: Epoch[090/190] Iteration[100/391] Loss: 0.2202 Acc:92.65%
Training: Epoch[090/190] Iteration[150/391] Loss: 0.2246 Acc:92.51%
Training: Epoch[090/190] Iteration[200/391] Loss: 0.2231 Acc:92.57%
Training: Epoch[090/190] Iteration[250/391] Loss: 0.2258 Acc:92.52%
Training: Epoch[090/190] Iteration[300/391] Loss: 0.2287 Acc:92.34%
Training: Epoch[090/190] Iteration[350/391] Loss: 0.2291 Acc:92.32%
Epoch[090/190] Train Acc: 92.29% Valid Acc:87.12% Train loss:0.2294 Valid loss:0.4242 LR:0.1
Training: Epoch[091/190] Iteration[050/391] Loss: 0.2151 Acc:92.41%
Training: Epoch[091/190] Iteration[100/391] Loss: 0.2238 Acc:92.34%
Training: Epoch[091/190] Iteration[150/391] Loss: 0.2238 Acc:92.37%
Training: Epoch[091/190] Iteration[200/391] Loss: 0.2236 Acc:92.41%
Training: Epoch[091/190] Iteration[250/391] Loss: 0.2227 Acc:92.44%
Training: Epoch[091/190] Iteration[300/391] Loss: 0.2253 Acc:92.40%
Training: Epoch[091/190] Iteration[350/391] Loss: 0.2270 Acc:92.34%
Epoch[091/190] Train Acc: 92.16% Valid Acc:86.58% Train loss:0.2323 Valid loss:0.4158 LR:0.1
Training: Epoch[092/190] Iteration[050/391] Loss: 0.2045 Acc:93.23%
Training: Epoch[092/190] Iteration[100/391] Loss: 0.2124 Acc:92.83%
Training: Epoch[092/190] Iteration[150/391] Loss: 0.2211 Acc:92.61%
Training: Epoch[092/190] Iteration[200/391] Loss: 0.2257 Acc:92.49%
Training: Epoch[092/190] Iteration[250/391] Loss: 0.2253 Acc:92.42%
Training: Epoch[092/190] Iteration[300/391] Loss: 0.2275 Acc:92.32%
Training: Epoch[092/190] Iteration[350/391] Loss: 0.2283 Acc:92.30%
Epoch[092/190] Train Acc: 92.29% Valid Acc:87.63% Train loss:0.2278 Valid loss:0.3961 LR:0.1
Training: Epoch[093/190] Iteration[050/391] Loss: 0.2196 Acc:92.47%
Training: Epoch[093/190] Iteration[100/391] Loss: 0.2212 Acc:92.45%
Training: Epoch[093/190] Iteration[150/391] Loss: 0.2233 Acc:92.36%
Training: Epoch[093/190] Iteration[200/391] Loss: 0.2174 Acc:92.57%
Training: Epoch[093/190] Iteration[250/391] Loss: 0.2217 Acc:92.37%
Training: Epoch[093/190] Iteration[300/391] Loss: 0.2221 Acc:92.42%
Training: Epoch[093/190] Iteration[350/391] Loss: 0.2233 Acc:92.38%
Epoch[093/190] Train Acc: 92.31% Valid Acc:86.34% Train loss:0.2248 Valid loss:0.4457 LR:0.1
Training: Epoch[094/190] Iteration[050/391] Loss: 0.1966 Acc:93.09%
Training: Epoch[094/190] Iteration[100/391] Loss: 0.1782 Acc:93.87%
Training: Epoch[094/190] Iteration[150/391] Loss: 0.1692 Acc:94.22%
Training: Epoch[094/190] Iteration[200/391] Loss: 0.1580 Acc:94.62%
Training: Epoch[094/190] Iteration[250/391] Loss: 0.1532 Acc:94.76%
Training: Epoch[094/190] Iteration[300/391] Loss: 0.1492 Acc:94.87%
Training: Epoch[094/190] Iteration[350/391] Loss: 0.1450 Acc:95.02%
Epoch[094/190] Train Acc: 95.11% Valid Acc:90.66% Train loss:0.1429 Valid loss:0.3102 LR:0.01
Training: Epoch[095/190] Iteration[050/391] Loss: 0.1170 Acc:96.12%
Training: Epoch[095/190] Iteration[100/391] Loss: 0.1125 Acc:96.20%
Training: Epoch[095/190] Iteration[150/391] Loss: 0.1110 Acc:96.21%
Training: Epoch[095/190] Iteration[200/391] Loss: 0.1102 Acc:96.28%
Training: Epoch[095/190] Iteration[250/391] Loss: 0.1108 Acc:96.22%
Training: Epoch[095/190] Iteration[300/391] Loss: 0.1104 Acc:96.25%
Training: Epoch[095/190] Iteration[350/391] Loss: 0.1087 Acc:96.27%
Epoch[095/190] Train Acc: 96.31% Valid Acc:90.81% Train loss:0.1076 Valid loss:0.3140 LR:0.01
Training: Epoch[096/190] Iteration[050/391] Loss: 0.0959 Acc:96.70%
Training: Epoch[096/190] Iteration[100/391] Loss: 0.0907 Acc:96.98%
Training: Epoch[096/190] Iteration[150/391] Loss: 0.0926 Acc:96.90%
Training: Epoch[096/190] Iteration[200/391] Loss: 0.0932 Acc:96.84%
Training: Epoch[096/190] Iteration[250/391] Loss: 0.0942 Acc:96.83%
Training: Epoch[096/190] Iteration[300/391] Loss: 0.0953 Acc:96.76%
Training: Epoch[096/190] Iteration[350/391] Loss: 0.0947 Acc:96.76%
Epoch[096/190] Train Acc: 96.78% Valid Acc:90.79% Train loss:0.0941 Valid loss:0.3177 LR:0.01
Training: Epoch[097/190] Iteration[050/391] Loss: 0.0823 Acc:97.03%
Training: Epoch[097/190] Iteration[100/391] Loss: 0.0835 Acc:97.16%
Training: Epoch[097/190] Iteration[150/391] Loss: 0.0829 Acc:97.23%
Training: Epoch[097/190] Iteration[200/391] Loss: 0.0841 Acc:97.11%
Training: Epoch[097/190] Iteration[250/391] Loss: 0.0860 Acc:97.05%
Training: Epoch[097/190] Iteration[300/391] Loss: 0.0864 Acc:97.06%
Training: Epoch[097/190] Iteration[350/391] Loss: 0.0881 Acc:97.01%
Epoch[097/190] Train Acc: 97.00% Valid Acc:90.84% Train loss:0.0883 Valid loss:0.3208 LR:0.01
Training: Epoch[098/190] Iteration[050/391] Loss: 0.0790 Acc:97.12%
Training: Epoch[098/190] Iteration[100/391] Loss: 0.0808 Acc:97.23%
Training: Epoch[098/190] Iteration[150/391] Loss: 0.0785 Acc:97.32%
Training: Epoch[098/190] Iteration[200/391] Loss: 0.0797 Acc:97.28%
Training: Epoch[098/190] Iteration[250/391] Loss: 0.0792 Acc:97.32%
Training: Epoch[098/190] Iteration[300/391] Loss: 0.0789 Acc:97.34%
Training: Epoch[098/190] Iteration[350/391] Loss: 0.0789 Acc:97.34%
Epoch[098/190] Train Acc: 97.32% Valid Acc:91.03% Train loss:0.0792 Valid loss:0.3179 LR:0.01
Training: Epoch[099/190] Iteration[050/391] Loss: 0.0700 Acc:97.61%
Training: Epoch[099/190] Iteration[100/391] Loss: 0.0715 Acc:97.57%
Training: Epoch[099/190] Iteration[150/391] Loss: 0.0734 Acc:97.48%
Training: Epoch[099/190] Iteration[200/391] Loss: 0.0748 Acc:97.45%
Training: Epoch[099/190] Iteration[250/391] Loss: 0.0732 Acc:97.54%
Training: Epoch[099/190] Iteration[300/391] Loss: 0.0738 Acc:97.53%
Training: Epoch[099/190] Iteration[350/391] Loss: 0.0742 Acc:97.50%
Epoch[099/190] Train Acc: 97.55% Valid Acc:91.14% Train loss:0.0727 Valid loss:0.3201 LR:0.01
Training: Epoch[100/190] Iteration[050/391] Loss: 0.0628 Acc:97.97%
Training: Epoch[100/190] Iteration[100/391] Loss: 0.0625 Acc:97.92%
Training: Epoch[100/190] Iteration[150/391] Loss: 0.0646 Acc:97.74%
Training: Epoch[100/190] Iteration[200/391] Loss: 0.0642 Acc:97.81%
Training: Epoch[100/190] Iteration[250/391] Loss: 0.0659 Acc:97.75%
Training: Epoch[100/190] Iteration[300/391] Loss: 0.0665 Acc:97.68%
Training: Epoch[100/190] Iteration[350/391] Loss: 0.0672 Acc:97.65%
Epoch[100/190] Train Acc: 97.62% Valid Acc:91.09% Train loss:0.0680 Valid loss:0.3266 LR:0.01
Training: Epoch[101/190] Iteration[050/391] Loss: 0.0613 Acc:97.89%
Training: Epoch[101/190] Iteration[100/391] Loss: 0.0642 Acc:97.80%
Training: Epoch[101/190] Iteration[150/391] Loss: 0.0642 Acc:97.79%
Training: Epoch[101/190] Iteration[200/391] Loss: 0.0660 Acc:97.73%
Training: Epoch[101/190] Iteration[250/391] Loss: 0.0661 Acc:97.70%
Training: Epoch[101/190] Iteration[300/391] Loss: 0.0656 Acc:97.71%
Training: Epoch[101/190] Iteration[350/391] Loss: 0.0649 Acc:97.73%
Epoch[101/190] Train Acc: 97.72% Valid Acc:91.14% Train loss:0.0652 Valid loss:0.3342 LR:0.01
Training: Epoch[102/190] Iteration[050/391] Loss: 0.0563 Acc:98.17%
Training: Epoch[102/190] Iteration[100/391] Loss: 0.0572 Acc:98.09%
Training: Epoch[102/190] Iteration[150/391] Loss: 0.0603 Acc:98.00%
Training: Epoch[102/190] Iteration[200/391] Loss: 0.0615 Acc:97.98%
Training: Epoch[102/190] Iteration[250/391] Loss: 0.0624 Acc:97.94%
Training: Epoch[102/190] Iteration[300/391] Loss: 0.0627 Acc:97.92%
Training: Epoch[102/190] Iteration[350/391] Loss: 0.0633 Acc:97.89%
Epoch[102/190] Train Acc: 97.83% Valid Acc:91.48% Train loss:0.0644 Valid loss:0.3281 LR:0.01
Training: Epoch[103/190] Iteration[050/391] Loss: 0.0568 Acc:98.20%
Training: Epoch[103/190] Iteration[100/391] Loss: 0.0542 Acc:98.13%
Training: Epoch[103/190] Iteration[150/391] Loss: 0.0550 Acc:98.17%
Training: Epoch[103/190] Iteration[200/391] Loss: 0.0567 Acc:98.09%
Training: Epoch[103/190] Iteration[250/391] Loss: 0.0571 Acc:98.09%
Training: Epoch[103/190] Iteration[300/391] Loss: 0.0573 Acc:98.09%
Training: Epoch[103/190] Iteration[350/391] Loss: 0.0582 Acc:98.04%
Epoch[103/190] Train Acc: 97.99% Valid Acc:91.19% Train loss:0.0592 Valid loss:0.3264 LR:0.01
Training: Epoch[104/190] Iteration[050/391] Loss: 0.0527 Acc:98.36%
Training: Epoch[104/190] Iteration[100/391] Loss: 0.0534 Acc:98.28%
Training: Epoch[104/190] Iteration[150/391] Loss: 0.0562 Acc:98.12%
Training: Epoch[104/190] Iteration[200/391] Loss: 0.0572 Acc:98.04%
Training: Epoch[104/190] Iteration[250/391] Loss: 0.0568 Acc:98.04%
Training: Epoch[104/190] Iteration[300/391] Loss: 0.0584 Acc:97.97%
Training: Epoch[104/190] Iteration[350/391] Loss: 0.0579 Acc:98.01%
Epoch[104/190] Train Acc: 98.01% Valid Acc:91.25% Train loss:0.0580 Valid loss:0.3362 LR:0.01
Training: Epoch[105/190] Iteration[050/391] Loss: 0.0461 Acc:98.39%
Training: Epoch[105/190] Iteration[100/391] Loss: 0.0511 Acc:98.35%
Training: Epoch[105/190] Iteration[150/391] Loss: 0.0542 Acc:98.22%
Training: Epoch[105/190] Iteration[200/391] Loss: 0.0527 Acc:98.26%
Training: Epoch[105/190] Iteration[250/391] Loss: 0.0509 Acc:98.32%
Training: Epoch[105/190] Iteration[300/391] Loss: 0.0516 Acc:98.29%
Training: Epoch[105/190] Iteration[350/391] Loss: 0.0519 Acc:98.27%
Epoch[105/190] Train Acc: 98.25% Valid Acc:91.24% Train loss:0.0522 Valid loss:0.3428 LR:0.01
Training: Epoch[106/190] Iteration[050/391] Loss: 0.0468 Acc:98.34%
Training: Epoch[106/190] Iteration[100/391] Loss: 0.0549 Acc:98.14%
Training: Epoch[106/190] Iteration[150/391] Loss: 0.0529 Acc:98.20%
Training: Epoch[106/190] Iteration[200/391] Loss: 0.0512 Acc:98.19%
Training: Epoch[106/190] Iteration[250/391] Loss: 0.0510 Acc:98.20%
Training: Epoch[106/190] Iteration[300/391] Loss: 0.0512 Acc:98.20%
Training: Epoch[106/190] Iteration[350/391] Loss: 0.0512 Acc:98.19%
Epoch[106/190] Train Acc: 98.17% Valid Acc:91.31% Train loss:0.0520 Valid loss:0.3362 LR:0.01
Training: Epoch[107/190] Iteration[050/391] Loss: 0.0513 Acc:98.53%
Training: Epoch[107/190] Iteration[100/391] Loss: 0.0503 Acc:98.44%
Training: Epoch[107/190] Iteration[150/391] Loss: 0.0487 Acc:98.41%
Training: Epoch[107/190] Iteration[200/391] Loss: 0.0492 Acc:98.34%
Training: Epoch[107/190] Iteration[250/391] Loss: 0.0482 Acc:98.39%
Training: Epoch[107/190] Iteration[300/391] Loss: 0.0498 Acc:98.32%
Training: Epoch[107/190] Iteration[350/391] Loss: 0.0501 Acc:98.31%
Epoch[107/190] Train Acc: 98.32% Valid Acc:91.34% Train loss:0.0497 Valid loss:0.3454 LR:0.01
Training: Epoch[108/190] Iteration[050/391] Loss: 0.0489 Acc:98.27%
Training: Epoch[108/190] Iteration[100/391] Loss: 0.0505 Acc:98.20%
Training: Epoch[108/190] Iteration[150/391] Loss: 0.0515 Acc:98.19%
Training: Epoch[108/190] Iteration[200/391] Loss: 0.0514 Acc:98.19%
Training: Epoch[108/190] Iteration[250/391] Loss: 0.0523 Acc:98.17%
Training: Epoch[108/190] Iteration[300/391] Loss: 0.0514 Acc:98.19%
Training: Epoch[108/190] Iteration[350/391] Loss: 0.0511 Acc:98.20%
Epoch[108/190] Train Acc: 98.23% Valid Acc:91.44% Train loss:0.0502 Valid loss:0.3407 LR:0.01
Training: Epoch[109/190] Iteration[050/391] Loss: 0.0403 Acc:98.70%
Training: Epoch[109/190] Iteration[100/391] Loss: 0.0422 Acc:98.54%
Training: Epoch[109/190] Iteration[150/391] Loss: 0.0442 Acc:98.51%
Training: Epoch[109/190] Iteration[200/391] Loss: 0.0460 Acc:98.45%
Training: Epoch[109/190] Iteration[250/391] Loss: 0.0462 Acc:98.45%
Training: Epoch[109/190] Iteration[300/391] Loss: 0.0467 Acc:98.45%
Training: Epoch[109/190] Iteration[350/391] Loss: 0.0472 Acc:98.42%
Epoch[109/190] Train Acc: 98.43% Valid Acc:91.19% Train loss:0.0471 Valid loss:0.3445 LR:0.01
Training: Epoch[110/190] Iteration[050/391] Loss: 0.0394 Acc:98.61%
Training: Epoch[110/190] Iteration[100/391] Loss: 0.0423 Acc:98.48%
Training: Epoch[110/190] Iteration[150/391] Loss: 0.0427 Acc:98.49%
Training: Epoch[110/190] Iteration[200/391] Loss: 0.0419 Acc:98.52%
Training: Epoch[110/190] Iteration[250/391] Loss: 0.0421 Acc:98.52%
Training: Epoch[110/190] Iteration[300/391] Loss: 0.0433 Acc:98.47%
Training: Epoch[110/190] Iteration[350/391] Loss: 0.0426 Acc:98.48%
Epoch[110/190] Train Acc: 98.48% Valid Acc:91.51% Train loss:0.0426 Valid loss:0.3525 LR:0.01
Training: Epoch[111/190] Iteration[050/391] Loss: 0.0383 Acc:98.56%
Training: Epoch[111/190] Iteration[100/391] Loss: 0.0408 Acc:98.52%
Training: Epoch[111/190] Iteration[150/391] Loss: 0.0404 Acc:98.56%
Training: Epoch[111/190] Iteration[200/391] Loss: 0.0412 Acc:98.52%
Training: Epoch[111/190] Iteration[250/391] Loss: 0.0419 Acc:98.50%
Training: Epoch[111/190] Iteration[300/391] Loss: 0.0415 Acc:98.52%
Training: Epoch[111/190] Iteration[350/391] Loss: 0.0423 Acc:98.50%
Epoch[111/190] Train Acc: 98.52% Valid Acc:91.34% Train loss:0.0420 Valid loss:0.3594 LR:0.01
Training: Epoch[112/190] Iteration[050/391] Loss: 0.0412 Acc:98.41%
Training: Epoch[112/190] Iteration[100/391] Loss: 0.0382 Acc:98.58%
Training: Epoch[112/190] Iteration[150/391] Loss: 0.0399 Acc:98.63%
Training: Epoch[112/190] Iteration[200/391] Loss: 0.0395 Acc:98.64%
Training: Epoch[112/190] Iteration[250/391] Loss: 0.0405 Acc:98.63%
Training: Epoch[112/190] Iteration[300/391] Loss: 0.0407 Acc:98.63%
Training: Epoch[112/190] Iteration[350/391] Loss: 0.0413 Acc:98.61%
Epoch[112/190] Train Acc: 98.59% Valid Acc:91.42% Train loss:0.0416 Valid loss:0.3579 LR:0.01
Training: Epoch[113/190] Iteration[050/391] Loss: 0.0409 Acc:98.36%
Training: Epoch[113/190] Iteration[100/391] Loss: 0.0385 Acc:98.50%
Training: Epoch[113/190] Iteration[150/391] Loss: 0.0387 Acc:98.55%
Training: Epoch[113/190] Iteration[200/391] Loss: 0.0397 Acc:98.51%
Training: Epoch[113/190] Iteration[250/391] Loss: 0.0395 Acc:98.55%
Training: Epoch[113/190] Iteration[300/391] Loss: 0.0394 Acc:98.57%
Training: Epoch[113/190] Iteration[350/391] Loss: 0.0400 Acc:98.57%
Epoch[113/190] Train Acc: 98.54% Valid Acc:91.03% Train loss:0.0408 Valid loss:0.3678 LR:0.01
Training: Epoch[114/190] Iteration[050/391] Loss: 0.0361 Acc:98.80%
Training: Epoch[114/190] Iteration[100/391] Loss: 0.0362 Acc:98.78%
Training: Epoch[114/190] Iteration[150/391] Loss: 0.0373 Acc:98.74%
Training: Epoch[114/190] Iteration[200/391] Loss: 0.0384 Acc:98.71%
Training: Epoch[114/190] Iteration[250/391] Loss: 0.0394 Acc:98.67%
Training: Epoch[114/190] Iteration[300/391] Loss: 0.0392 Acc:98.65%
Training: Epoch[114/190] Iteration[350/391] Loss: 0.0389 Acc:98.67%
Epoch[114/190] Train Acc: 98.67% Valid Acc:91.26% Train loss:0.0387 Valid loss:0.3689 LR:0.01
Training: Epoch[115/190] Iteration[050/391] Loss: 0.0385 Acc:98.56%
Training: Epoch[115/190] Iteration[100/391] Loss: 0.0362 Acc:98.65%
Training: Epoch[115/190] Iteration[150/391] Loss: 0.0363 Acc:98.70%
Training: Epoch[115/190] Iteration[200/391] Loss: 0.0369 Acc:98.67%
Training: Epoch[115/190] Iteration[250/391] Loss: 0.0366 Acc:98.69%
Training: Epoch[115/190] Iteration[300/391] Loss: 0.0374 Acc:98.68%
Training: Epoch[115/190] Iteration[350/391] Loss: 0.0377 Acc:98.70%
Epoch[115/190] Train Acc: 98.69% Valid Acc:91.20% Train loss:0.0380 Valid loss:0.3677 LR:0.01
Training: Epoch[116/190] Iteration[050/391] Loss: 0.0347 Acc:98.77%
Training: Epoch[116/190] Iteration[100/391] Loss: 0.0337 Acc:98.77%
Training: Epoch[116/190] Iteration[150/391] Loss: 0.0338 Acc:98.80%
Training: Epoch[116/190] Iteration[200/391] Loss: 0.0339 Acc:98.80%
Training: Epoch[116/190] Iteration[250/391] Loss: 0.0342 Acc:98.82%
Training: Epoch[116/190] Iteration[300/391] Loss: 0.0354 Acc:98.77%
Training: Epoch[116/190] Iteration[350/391] Loss: 0.0354 Acc:98.77%
Epoch[116/190] Train Acc: 98.74% Valid Acc:91.51% Train loss:0.0363 Valid loss:0.3629 LR:0.01
Training: Epoch[117/190] Iteration[050/391] Loss: 0.0381 Acc:98.58%
Training: Epoch[117/190] Iteration[100/391] Loss: 0.0342 Acc:98.76%
Training: Epoch[117/190] Iteration[150/391] Loss: 0.0363 Acc:98.71%
Training: Epoch[117/190] Iteration[200/391] Loss: 0.0354 Acc:98.77%
Training: Epoch[117/190] Iteration[250/391] Loss: 0.0353 Acc:98.77%
Training: Epoch[117/190] Iteration[300/391] Loss: 0.0343 Acc:98.81%
Training: Epoch[117/190] Iteration[350/391] Loss: 0.0342 Acc:98.81%
Epoch[117/190] Train Acc: 98.82% Valid Acc:91.38% Train loss:0.0339 Valid loss:0.3728 LR:0.01
Training: Epoch[118/190] Iteration[050/391] Loss: 0.0370 Acc:98.75%
Training: Epoch[118/190] Iteration[100/391] Loss: 0.0373 Acc:98.78%
Training: Epoch[118/190] Iteration[150/391] Loss: 0.0357 Acc:98.81%
Training: Epoch[118/190] Iteration[200/391] Loss: 0.0361 Acc:98.80%
Training: Epoch[118/190] Iteration[250/391] Loss: 0.0355 Acc:98.81%
Training: Epoch[118/190] Iteration[300/391] Loss: 0.0345 Acc:98.83%
Training: Epoch[118/190] Iteration[350/391] Loss: 0.0354 Acc:98.80%
Epoch[118/190] Train Acc: 98.78% Valid Acc:91.41% Train loss:0.0361 Valid loss:0.3686 LR:0.01
Training: Epoch[119/190] Iteration[050/391] Loss: 0.0369 Acc:98.88%
Training: Epoch[119/190] Iteration[100/391] Loss: 0.0343 Acc:98.91%
Training: Epoch[119/190] Iteration[150/391] Loss: 0.0359 Acc:98.82%
Training: Epoch[119/190] Iteration[200/391] Loss: 0.0352 Acc:98.79%
Training: Epoch[119/190] Iteration[250/391] Loss: 0.0353 Acc:98.79%
Training: Epoch[119/190] Iteration[300/391] Loss: 0.0354 Acc:98.76%
Training: Epoch[119/190] Iteration[350/391] Loss: 0.0353 Acc:98.76%
Epoch[119/190] Train Acc: 98.77% Valid Acc:91.37% Train loss:0.0349 Valid loss:0.3766 LR:0.01
Training: Epoch[120/190] Iteration[050/391] Loss: 0.0325 Acc:98.80%
Training: Epoch[120/190] Iteration[100/391] Loss: 0.0307 Acc:98.91%
Training: Epoch[120/190] Iteration[150/391] Loss: 0.0335 Acc:98.82%
Training: Epoch[120/190] Iteration[200/391] Loss: 0.0335 Acc:98.84%
Training: Epoch[120/190] Iteration[250/391] Loss: 0.0342 Acc:98.80%
Training: Epoch[120/190] Iteration[300/391] Loss: 0.0345 Acc:98.78%
Training: Epoch[120/190] Iteration[350/391] Loss: 0.0353 Acc:98.73%
Epoch[120/190] Train Acc: 98.73% Valid Acc:91.50% Train loss:0.0354 Valid loss:0.3705 LR:0.01
Training: Epoch[121/190] Iteration[050/391] Loss: 0.0274 Acc:98.94%
Training: Epoch[121/190] Iteration[100/391] Loss: 0.0276 Acc:99.02%
Training: Epoch[121/190] Iteration[150/391] Loss: 0.0296 Acc:98.93%
Training: Epoch[121/190] Iteration[200/391] Loss: 0.0306 Acc:98.88%
Training: Epoch[121/190] Iteration[250/391] Loss: 0.0302 Acc:98.91%
Training: Epoch[121/190] Iteration[300/391] Loss: 0.0309 Acc:98.91%
Training: Epoch[121/190] Iteration[350/391] Loss: 0.0307 Acc:98.93%
Epoch[121/190] Train Acc: 98.93% Valid Acc:91.34% Train loss:0.0310 Valid loss:0.3895 LR:0.01
Training: Epoch[122/190] Iteration[050/391] Loss: 0.0324 Acc:98.92%
Training: Epoch[122/190] Iteration[100/391] Loss: 0.0315 Acc:98.88%
Training: Epoch[122/190] Iteration[150/391] Loss: 0.0311 Acc:98.94%
Training: Epoch[122/190] Iteration[200/391] Loss: 0.0320 Acc:98.91%
Training: Epoch[122/190] Iteration[250/391] Loss: 0.0329 Acc:98.86%
Training: Epoch[122/190] Iteration[300/391] Loss: 0.0336 Acc:98.84%
Training: Epoch[122/190] Iteration[350/391] Loss: 0.0332 Acc:98.85%
Epoch[122/190] Train Acc: 98.84% Valid Acc:91.41% Train loss:0.0335 Valid loss:0.3812 LR:0.01
Training: Epoch[123/190] Iteration[050/391] Loss: 0.0298 Acc:99.00%
Training: Epoch[123/190] Iteration[100/391] Loss: 0.0284 Acc:99.05%
Training: Epoch[123/190] Iteration[150/391] Loss: 0.0285 Acc:99.01%
Training: Epoch[123/190] Iteration[200/391] Loss: 0.0294 Acc:98.97%
Training: Epoch[123/190] Iteration[250/391] Loss: 0.0291 Acc:98.98%
Training: Epoch[123/190] Iteration[300/391] Loss: 0.0297 Acc:98.95%
Training: Epoch[123/190] Iteration[350/391] Loss: 0.0297 Acc:98.95%
Epoch[123/190] Train Acc: 98.92% Valid Acc:91.49% Train loss:0.0304 Valid loss:0.3914 LR:0.01
Training: Epoch[124/190] Iteration[050/391] Loss: 0.0281 Acc:98.95%
Training: Epoch[124/190] Iteration[100/391] Loss: 0.0281 Acc:99.01%
Training: Epoch[124/190] Iteration[150/391] Loss: 0.0282 Acc:99.00%
Training: Epoch[124/190] Iteration[200/391] Loss: 0.0299 Acc:98.91%
Training: Epoch[124/190] Iteration[250/391] Loss: 0.0303 Acc:98.91%
Training: Epoch[124/190] Iteration[300/391] Loss: 0.0297 Acc:98.95%
Training: Epoch[124/190] Iteration[350/391] Loss: 0.0298 Acc:98.94%
Epoch[124/190] Train Acc: 98.91% Valid Acc:91.37% Train loss:0.0307 Valid loss:0.3847 LR:0.01
Training: Epoch[125/190] Iteration[050/391] Loss: 0.0277 Acc:99.05%
Training: Epoch[125/190] Iteration[100/391] Loss: 0.0286 Acc:99.00%
Training: Epoch[125/190] Iteration[150/391] Loss: 0.0278 Acc:99.06%
Training: Epoch[125/190] Iteration[200/391] Loss: 0.0271 Acc:99.10%
Training: Epoch[125/190] Iteration[250/391] Loss: 0.0273 Acc:99.08%
Training: Epoch[125/190] Iteration[300/391] Loss: 0.0271 Acc:99.08%
Training: Epoch[125/190] Iteration[350/391] Loss: 0.0277 Acc:99.08%
Epoch[125/190] Train Acc: 99.05% Valid Acc:91.51% Train loss:0.0280 Valid loss:0.3851 LR:0.01
Training: Epoch[126/190] Iteration[050/391] Loss: 0.0289 Acc:99.05%
Training: Epoch[126/190] Iteration[100/391] Loss: 0.0271 Acc:99.06%
Training: Epoch[126/190] Iteration[150/391] Loss: 0.0276 Acc:99.08%
Training: Epoch[126/190] Iteration[200/391] Loss: 0.0280 Acc:99.05%
Training: Epoch[126/190] Iteration[250/391] Loss: 0.0282 Acc:99.03%
Training: Epoch[126/190] Iteration[300/391] Loss: 0.0281 Acc:99.05%
Training: Epoch[126/190] Iteration[350/391] Loss: 0.0279 Acc:99.07%
Epoch[126/190] Train Acc: 99.06% Valid Acc:91.32% Train loss:0.0283 Valid loss:0.4035 LR:0.01
Training: Epoch[127/190] Iteration[050/391] Loss: 0.0258 Acc:99.25%
Training: Epoch[127/190] Iteration[100/391] Loss: 0.0282 Acc:99.11%
Training: Epoch[127/190] Iteration[150/391] Loss: 0.0278 Acc:99.09%
Training: Epoch[127/190] Iteration[200/391] Loss: 0.0281 Acc:99.06%
Training: Epoch[127/190] Iteration[250/391] Loss: 0.0286 Acc:99.04%
Training: Epoch[127/190] Iteration[300/391] Loss: 0.0280 Acc:99.04%
Training: Epoch[127/190] Iteration[350/391] Loss: 0.0283 Acc:99.04%
Epoch[127/190] Train Acc: 98.99% Valid Acc:91.48% Train loss:0.0300 Valid loss:0.3869 LR:0.01
Training: Epoch[128/190] Iteration[050/391] Loss: 0.0275 Acc:99.09%
Training: Epoch[128/190] Iteration[100/391] Loss: 0.0268 Acc:99.07%
Training: Epoch[128/190] Iteration[150/391] Loss: 0.0267 Acc:99.11%
Training: Epoch[128/190] Iteration[200/391] Loss: 0.0273 Acc:99.08%
Training: Epoch[128/190] Iteration[250/391] Loss: 0.0279 Acc:99.05%
Training: Epoch[128/190] Iteration[300/391] Loss: 0.0287 Acc:99.04%
Training: Epoch[128/190] Iteration[350/391] Loss: 0.0294 Acc:99.02%
Epoch[128/190] Train Acc: 99.01% Valid Acc:91.17% Train loss:0.0295 Valid loss:0.3786 LR:0.01
Training: Epoch[129/190] Iteration[050/391] Loss: 0.0230 Acc:99.20%
Training: Epoch[129/190] Iteration[100/391] Loss: 0.0264 Acc:99.05%
Training: Epoch[129/190] Iteration[150/391] Loss: 0.0267 Acc:99.05%
Training: Epoch[129/190] Iteration[200/391] Loss: 0.0272 Acc:99.05%
Training: Epoch[129/190] Iteration[250/391] Loss: 0.0269 Acc:99.07%
Training: Epoch[129/190] Iteration[300/391] Loss: 0.0268 Acc:99.07%
Training: Epoch[129/190] Iteration[350/391] Loss: 0.0271 Acc:99.06%
Epoch[129/190] Train Acc: 99.04% Valid Acc:91.29% Train loss:0.0274 Valid loss:0.3902 LR:0.01
Training: Epoch[130/190] Iteration[050/391] Loss: 0.0284 Acc:99.02%
Training: Epoch[130/190] Iteration[100/391] Loss: 0.0253 Acc:99.12%
Training: Epoch[130/190] Iteration[150/391] Loss: 0.0254 Acc:99.14%
Training: Epoch[130/190] Iteration[200/391] Loss: 0.0252 Acc:99.12%
Training: Epoch[130/190] Iteration[250/391] Loss: 0.0254 Acc:99.15%
Training: Epoch[130/190] Iteration[300/391] Loss: 0.0257 Acc:99.12%
Training: Epoch[130/190] Iteration[350/391] Loss: 0.0264 Acc:99.10%
Epoch[130/190] Train Acc: 99.09% Valid Acc:91.27% Train loss:0.0267 Valid loss:0.3965 LR:0.01
Training: Epoch[131/190] Iteration[050/391] Loss: 0.0231 Acc:99.33%
Training: Epoch[131/190] Iteration[100/391] Loss: 0.0246 Acc:99.29%
Training: Epoch[131/190] Iteration[150/391] Loss: 0.0236 Acc:99.29%
Training: Epoch[131/190] Iteration[200/391] Loss: 0.0241 Acc:99.26%
Training: Epoch[131/190] Iteration[250/391] Loss: 0.0250 Acc:99.22%
Training: Epoch[131/190] Iteration[300/391] Loss: 0.0252 Acc:99.21%
Training: Epoch[131/190] Iteration[350/391] Loss: 0.0258 Acc:99.18%
Epoch[131/190] Train Acc: 99.17% Valid Acc:91.47% Train loss:0.0257 Valid loss:0.3853 LR:0.01
Training: Epoch[132/190] Iteration[050/391] Loss: 0.0269 Acc:99.09%
Training: Epoch[132/190] Iteration[100/391] Loss: 0.0253 Acc:99.11%
Training: Epoch[132/190] Iteration[150/391] Loss: 0.0247 Acc:99.12%
Training: Epoch[132/190] Iteration[200/391] Loss: 0.0247 Acc:99.12%
Training: Epoch[132/190] Iteration[250/391] Loss: 0.0245 Acc:99.13%
Training: Epoch[132/190] Iteration[300/391] Loss: 0.0259 Acc:99.09%
Training: Epoch[132/190] Iteration[350/391] Loss: 0.0261 Acc:99.07%
Epoch[132/190] Train Acc: 99.04% Valid Acc:91.17% Train loss:0.0269 Valid loss:0.3982 LR:0.01
Training: Epoch[133/190] Iteration[050/391] Loss: 0.0313 Acc:99.00%
Training: Epoch[133/190] Iteration[100/391] Loss: 0.0305 Acc:99.00%
Training: Epoch[133/190] Iteration[150/391] Loss: 0.0282 Acc:99.06%
Training: Epoch[133/190] Iteration[200/391] Loss: 0.0272 Acc:99.07%
Training: Epoch[133/190] Iteration[250/391] Loss: 0.0267 Acc:99.07%
Training: Epoch[133/190] Iteration[300/391] Loss: 0.0267 Acc:99.08%
Training: Epoch[133/190] Iteration[350/391] Loss: 0.0275 Acc:99.06%
Epoch[133/190] Train Acc: 99.07% Valid Acc:91.13% Train loss:0.0275 Valid loss:0.3992 LR:0.01
Training: Epoch[134/190] Iteration[050/391] Loss: 0.0278 Acc:99.08%
Training: Epoch[134/190] Iteration[100/391] Loss: 0.0276 Acc:99.07%
Training: Epoch[134/190] Iteration[150/391] Loss: 0.0268 Acc:99.08%
Training: Epoch[134/190] Iteration[200/391] Loss: 0.0265 Acc:99.09%
Training: Epoch[134/190] Iteration[250/391] Loss: 0.0275 Acc:99.05%
Training: Epoch[134/190] Iteration[300/391] Loss: 0.0275 Acc:99.06%
Training: Epoch[134/190] Iteration[350/391] Loss: 0.0269 Acc:99.08%
Epoch[134/190] Train Acc: 99.10% Valid Acc:91.47% Train loss:0.0268 Valid loss:0.4019 LR:0.01
Training: Epoch[135/190] Iteration[050/391] Loss: 0.0299 Acc:98.94%
Training: Epoch[135/190] Iteration[100/391] Loss: 0.0280 Acc:99.03%
Training: Epoch[135/190] Iteration[150/391] Loss: 0.0257 Acc:99.12%
Training: Epoch[135/190] Iteration[200/391] Loss: 0.0247 Acc:99.17%
Training: Epoch[135/190] Iteration[250/391] Loss: 0.0251 Acc:99.17%
Training: Epoch[135/190] Iteration[300/391] Loss: 0.0254 Acc:99.17%
Training: Epoch[135/190] Iteration[350/391] Loss: 0.0257 Acc:99.16%
Epoch[135/190] Train Acc: 99.17% Valid Acc:91.36% Train loss:0.0256 Valid loss:0.4029 LR:0.01
Training: Epoch[136/190] Iteration[050/391] Loss: 0.0243 Acc:99.08%
Training: Epoch[136/190] Iteration[100/391] Loss: 0.0272 Acc:98.99%
Training: Epoch[136/190] Iteration[150/391] Loss: 0.0261 Acc:99.03%
Training: Epoch[136/190] Iteration[200/391] Loss: 0.0270 Acc:99.02%
Training: Epoch[136/190] Iteration[250/391] Loss: 0.0261 Acc:99.06%
Training: Epoch[136/190] Iteration[300/391] Loss: 0.0267 Acc:99.04%
Training: Epoch[136/190] Iteration[350/391] Loss: 0.0270 Acc:99.04%
Epoch[136/190] Train Acc: 99.04% Valid Acc:91.52% Train loss:0.0268 Valid loss:0.3990 LR:0.01
Training: Epoch[137/190] Iteration[050/391] Loss: 0.0252 Acc:99.25%
Training: Epoch[137/190] Iteration[100/391] Loss: 0.0233 Acc:99.23%
Training: Epoch[137/190] Iteration[150/391] Loss: 0.0229 Acc:99.21%
Training: Epoch[137/190] Iteration[200/391] Loss: 0.0224 Acc:99.22%
Training: Epoch[137/190] Iteration[250/391] Loss: 0.0230 Acc:99.22%
Training: Epoch[137/190] Iteration[300/391] Loss: 0.0237 Acc:99.19%
Training: Epoch[137/190] Iteration[350/391] Loss: 0.0245 Acc:99.16%
Epoch[137/190] Train Acc: 99.15% Valid Acc:91.43% Train loss:0.0245 Valid loss:0.4010 LR:0.01
Training: Epoch[138/190] Iteration[050/391] Loss: 0.0224 Acc:99.14%
Training: Epoch[138/190] Iteration[100/391] Loss: 0.0218 Acc:99.20%
Training: Epoch[138/190] Iteration[150/391] Loss: 0.0215 Acc:99.20%
Training: Epoch[138/190] Iteration[200/391] Loss: 0.0226 Acc:99.20%
Training: Epoch[138/190] Iteration[250/391] Loss: 0.0216 Acc:99.23%
Training: Epoch[138/190] Iteration[300/391] Loss: 0.0214 Acc:99.27%
Training: Epoch[138/190] Iteration[350/391] Loss: 0.0210 Acc:99.29%
Epoch[138/190] Train Acc: 99.31% Valid Acc:91.44% Train loss:0.0205 Valid loss:0.3952 LR:0.001
Training: Epoch[139/190] Iteration[050/391] Loss: 0.0215 Acc:99.31%
Training: Epoch[139/190] Iteration[100/391] Loss: 0.0223 Acc:99.30%
Training: Epoch[139/190] Iteration[150/391] Loss: 0.0207 Acc:99.33%
Training: Epoch[139/190] Iteration[200/391] Loss: 0.0205 Acc:99.33%
Training: Epoch[139/190] Iteration[250/391] Loss: 0.0201 Acc:99.34%
Training: Epoch[139/190] Iteration[300/391] Loss: 0.0207 Acc:99.32%
Training: Epoch[139/190] Iteration[350/391] Loss: 0.0206 Acc:99.31%
Epoch[139/190] Train Acc: 99.32% Valid Acc:91.50% Train loss:0.0204 Valid loss:0.3929 LR:0.001
Training: Epoch[140/190] Iteration[050/391] Loss: 0.0178 Acc:99.42%
Training: Epoch[140/190] Iteration[100/391] Loss: 0.0180 Acc:99.40%
Training: Epoch[140/190] Iteration[150/391] Loss: 0.0165 Acc:99.44%
Training: Epoch[140/190] Iteration[200/391] Loss: 0.0169 Acc:99.43%
Training: Epoch[140/190] Iteration[250/391] Loss: 0.0174 Acc:99.42%
Training: Epoch[140/190] Iteration[300/391] Loss: 0.0177 Acc:99.41%
Training: Epoch[140/190] Iteration[350/391] Loss: 0.0178 Acc:99.41%
Epoch[140/190] Train Acc: 99.41% Valid Acc:91.46% Train loss:0.0179 Valid loss:0.3931 LR:0.001
Training: Epoch[141/190] Iteration[050/391] Loss: 0.0166 Acc:99.53%
Training: Epoch[141/190] Iteration[100/391] Loss: 0.0162 Acc:99.51%
Training: Epoch[141/190] Iteration[150/391] Loss: 0.0166 Acc:99.47%
Training: Epoch[141/190] Iteration[200/391] Loss: 0.0173 Acc:99.45%
Training: Epoch[141/190] Iteration[250/391] Loss: 0.0176 Acc:99.43%
Training: Epoch[141/190] Iteration[300/391] Loss: 0.0173 Acc:99.44%
Training: Epoch[141/190] Iteration[350/391] Loss: 0.0171 Acc:99.44%
Epoch[141/190] Train Acc: 99.44% Valid Acc:91.71% Train loss:0.0170 Valid loss:0.3919 LR:0.001
Training: Epoch[142/190] Iteration[050/391] Loss: 0.0177 Acc:99.42%
Training: Epoch[142/190] Iteration[100/391] Loss: 0.0181 Acc:99.41%
Training: Epoch[142/190] Iteration[150/391] Loss: 0.0174 Acc:99.43%
Training: Epoch[142/190] Iteration[200/391] Loss: 0.0177 Acc:99.38%
Training: Epoch[142/190] Iteration[250/391] Loss: 0.0169 Acc:99.40%
Training: Epoch[142/190] Iteration[300/391] Loss: 0.0171 Acc:99.41%
Training: Epoch[142/190] Iteration[350/391] Loss: 0.0169 Acc:99.41%
Epoch[142/190] Train Acc: 99.41% Valid Acc:91.54% Train loss:0.0169 Valid loss:0.3977 LR:0.001
Training: Epoch[143/190] Iteration[050/391] Loss: 0.0161 Acc:99.47%
Training: Epoch[143/190] Iteration[100/391] Loss: 0.0166 Acc:99.45%
Training: Epoch[143/190] Iteration[150/391] Loss: 0.0174 Acc:99.41%
Training: Epoch[143/190] Iteration[200/391] Loss: 0.0165 Acc:99.43%
Training: Epoch[143/190] Iteration[250/391] Loss: 0.0159 Acc:99.45%
Training: Epoch[143/190] Iteration[300/391] Loss: 0.0160 Acc:99.46%
Training: Epoch[143/190] Iteration[350/391] Loss: 0.0158 Acc:99.46%
Epoch[143/190] Train Acc: 99.47% Valid Acc:91.56% Train loss:0.0157 Valid loss:0.4018 LR:0.001
Training: Epoch[144/190] Iteration[050/391] Loss: 0.0156 Acc:99.48%
Training: Epoch[144/190] Iteration[100/391] Loss: 0.0142 Acc:99.53%
Training: Epoch[144/190] Iteration[150/391] Loss: 0.0149 Acc:99.53%
Training: Epoch[144/190] Iteration[200/391] Loss: 0.0145 Acc:99.52%
Training: Epoch[144/190] Iteration[250/391] Loss: 0.0149 Acc:99.52%
Training: Epoch[144/190] Iteration[300/391] Loss: 0.0152 Acc:99.50%
Training: Epoch[144/190] Iteration[350/391] Loss: 0.0150 Acc:99.51%
Epoch[144/190] Train Acc: 99.51% Valid Acc:91.47% Train loss:0.0150 Valid loss:0.4047 LR:0.001
Training: Epoch[145/190] Iteration[050/391] Loss: 0.0135 Acc:99.56%
Training: Epoch[145/190] Iteration[100/391] Loss: 0.0144 Acc:99.52%
Training: Epoch[145/190] Iteration[150/391] Loss: 0.0137 Acc:99.58%
Training: Epoch[145/190] Iteration[200/391] Loss: 0.0143 Acc:99.54%
Training: Epoch[145/190] Iteration[250/391] Loss: 0.0144 Acc:99.54%
Training: Epoch[145/190] Iteration[300/391] Loss: 0.0147 Acc:99.53%
Training: Epoch[145/190] Iteration[350/391] Loss: 0.0147 Acc:99.53%
Epoch[145/190] Train Acc: 99.52% Valid Acc:91.64% Train loss:0.0151 Valid loss:0.4048 LR:0.001
Training: Epoch[146/190] Iteration[050/391] Loss: 0.0129 Acc:99.53%
Training: Epoch[146/190] Iteration[100/391] Loss: 0.0175 Acc:99.45%
Training: Epoch[146/190] Iteration[150/391] Loss: 0.0158 Acc:99.50%
Training: Epoch[146/190] Iteration[200/391] Loss: 0.0155 Acc:99.48%
Training: Epoch[146/190] Iteration[250/391] Loss: 0.0153 Acc:99.50%
Training: Epoch[146/190] Iteration[300/391] Loss: 0.0148 Acc:99.52%
Training: Epoch[146/190] Iteration[350/391] Loss: 0.0151 Acc:99.50%
Epoch[146/190] Train Acc: 99.52% Valid Acc:91.57% Train loss:0.0148 Valid loss:0.4066 LR:0.001
Training: Epoch[147/190] Iteration[050/391] Loss: 0.0137 Acc:99.61%
Training: Epoch[147/190] Iteration[100/391] Loss: 0.0149 Acc:99.55%
Training: Epoch[147/190] Iteration[150/391] Loss: 0.0143 Acc:99.57%
Training: Epoch[147/190] Iteration[200/391] Loss: 0.0136 Acc:99.57%
Training: Epoch[147/190] Iteration[250/391] Loss: 0.0133 Acc:99.58%
Training: Epoch[147/190] Iteration[300/391] Loss: 0.0136 Acc:99.56%
Training: Epoch[147/190] Iteration[350/391] Loss: 0.0144 Acc:99.53%
Epoch[147/190] Train Acc: 99.52% Valid Acc:91.64% Train loss:0.0147 Valid loss:0.4030 LR:0.001
Training: Epoch[148/190] Iteration[050/391] Loss: 0.0144 Acc:99.53%
Training: Epoch[148/190] Iteration[100/391] Loss: 0.0151 Acc:99.52%
Training: Epoch[148/190] Iteration[150/391] Loss: 0.0147 Acc:99.53%
Training: Epoch[148/190] Iteration[200/391] Loss: 0.0145 Acc:99.53%
Training: Epoch[148/190] Iteration[250/391] Loss: 0.0150 Acc:99.52%
Training: Epoch[148/190] Iteration[300/391] Loss: 0.0150 Acc:99.51%
Training: Epoch[148/190] Iteration[350/391] Loss: 0.0151 Acc:99.52%
Epoch[148/190] Train Acc: 99.52% Valid Acc:91.50% Train loss:0.0150 Valid loss:0.4067 LR:0.001
Training: Epoch[149/190] Iteration[050/391] Loss: 0.0173 Acc:99.44%
Training: Epoch[149/190] Iteration[100/391] Loss: 0.0159 Acc:99.47%
Training: Epoch[149/190] Iteration[150/391] Loss: 0.0153 Acc:99.55%
Training: Epoch[149/190] Iteration[200/391] Loss: 0.0140 Acc:99.58%
Training: Epoch[149/190] Iteration[250/391] Loss: 0.0142 Acc:99.58%
Training: Epoch[149/190] Iteration[300/391] Loss: 0.0145 Acc:99.56%
Training: Epoch[149/190] Iteration[350/391] Loss: 0.0146 Acc:99.56%
Epoch[149/190] Train Acc: 99.55% Valid Acc:91.69% Train loss:0.0146 Valid loss:0.4044 LR:0.001
Training: Epoch[150/190] Iteration[050/391] Loss: 0.0136 Acc:99.62%
Training: Epoch[150/190] Iteration[100/391] Loss: 0.0136 Acc:99.57%
Training: Epoch[150/190] Iteration[150/391] Loss: 0.0138 Acc:99.56%
Training: Epoch[150/190] Iteration[200/391] Loss: 0.0150 Acc:99.51%
Training: Epoch[150/190] Iteration[250/391] Loss: 0.0148 Acc:99.51%
Training: Epoch[150/190] Iteration[300/391] Loss: 0.0154 Acc:99.50%
Training: Epoch[150/190] Iteration[350/391] Loss: 0.0150 Acc:99.51%
Epoch[150/190] Train Acc: 99.51% Valid Acc:91.56% Train loss:0.0152 Valid loss:0.4105 LR:0.001
Training: Epoch[151/190] Iteration[050/391] Loss: 0.0165 Acc:99.38%
Training: Epoch[151/190] Iteration[100/391] Loss: 0.0134 Acc:99.54%
Training: Epoch[151/190] Iteration[150/391] Loss: 0.0134 Acc:99.55%
Training: Epoch[151/190] Iteration[200/391] Loss: 0.0141 Acc:99.51%
Training: Epoch[151/190] Iteration[250/391] Loss: 0.0136 Acc:99.52%
Training: Epoch[151/190] Iteration[300/391] Loss: 0.0134 Acc:99.53%
Training: Epoch[151/190] Iteration[350/391] Loss: 0.0137 Acc:99.52%
Epoch[151/190] Train Acc: 99.53% Valid Acc:91.71% Train loss:0.0136 Valid loss:0.4064 LR:0.001
Training: Epoch[152/190] Iteration[050/391] Loss: 0.0143 Acc:99.59%
Training: Epoch[152/190] Iteration[100/391] Loss: 0.0164 Acc:99.48%
Training: Epoch[152/190] Iteration[150/391] Loss: 0.0160 Acc:99.47%
Training: Epoch[152/190] Iteration[200/391] Loss: 0.0153 Acc:99.49%
Training: Epoch[152/190] Iteration[250/391] Loss: 0.0150 Acc:99.49%
Training: Epoch[152/190] Iteration[300/391] Loss: 0.0143 Acc:99.52%
Training: Epoch[152/190] Iteration[350/391] Loss: 0.0147 Acc:99.52%
Epoch[152/190] Train Acc: 99.52% Valid Acc:91.52% Train loss:0.0144 Valid loss:0.4100 LR:0.001
Training: Epoch[153/190] Iteration[050/391] Loss: 0.0153 Acc:99.48%
Training: Epoch[153/190] Iteration[100/391] Loss: 0.0137 Acc:99.59%
Training: Epoch[153/190] Iteration[150/391] Loss: 0.0143 Acc:99.54%
Training: Epoch[153/190] Iteration[200/391] Loss: 0.0134 Acc:99.58%
Training: Epoch[153/190] Iteration[250/391] Loss: 0.0128 Acc:99.60%
Training: Epoch[153/190] Iteration[300/391] Loss: 0.0129 Acc:99.60%
Training: Epoch[153/190] Iteration[350/391] Loss: 0.0125 Acc:99.61%
Epoch[153/190] Train Acc: 99.59% Valid Acc:91.74% Train loss:0.0131 Valid loss:0.4080 LR:0.001
Training: Epoch[154/190] Iteration[050/391] Loss: 0.0160 Acc:99.38%
Training: Epoch[154/190] Iteration[100/391] Loss: 0.0153 Acc:99.42%
Training: Epoch[154/190] Iteration[150/391] Loss: 0.0155 Acc:99.44%
Training: Epoch[154/190] Iteration[200/391] Loss: 0.0167 Acc:99.43%
Training: Epoch[154/190] Iteration[250/391] Loss: 0.0158 Acc:99.46%
Training: Epoch[154/190] Iteration[300/391] Loss: 0.0155 Acc:99.47%
Training: Epoch[154/190] Iteration[350/391] Loss: 0.0152 Acc:99.50%
Epoch[154/190] Train Acc: 99.51% Valid Acc:91.57% Train loss:0.0150 Valid loss:0.4079 LR:0.001
Training: Epoch[155/190] Iteration[050/391] Loss: 0.0133 Acc:99.53%
Training: Epoch[155/190] Iteration[100/391] Loss: 0.0131 Acc:99.48%
Training: Epoch[155/190] Iteration[150/391] Loss: 0.0130 Acc:99.49%
Training: Epoch[155/190] Iteration[200/391] Loss: 0.0134 Acc:99.50%
Training: Epoch[155/190] Iteration[250/391] Loss: 0.0136 Acc:99.51%
Training: Epoch[155/190] Iteration[300/391] Loss: 0.0145 Acc:99.49%
Training: Epoch[155/190] Iteration[350/391] Loss: 0.0140 Acc:99.51%
Epoch[155/190] Train Acc: 99.53% Valid Acc:91.53% Train loss:0.0138 Valid loss:0.4086 LR:0.001
Training: Epoch[156/190] Iteration[050/391] Loss: 0.0107 Acc:99.67%
Training: Epoch[156/190] Iteration[100/391] Loss: 0.0112 Acc:99.64%
Training: Epoch[156/190] Iteration[150/391] Loss: 0.0120 Acc:99.58%
Training: Epoch[156/190] Iteration[200/391] Loss: 0.0129 Acc:99.54%
Training: Epoch[156/190] Iteration[250/391] Loss: 0.0128 Acc:99.56%
Training: Epoch[156/190] Iteration[300/391] Loss: 0.0123 Acc:99.58%
Training: Epoch[156/190] Iteration[350/391] Loss: 0.0121 Acc:99.58%
Epoch[156/190] Train Acc: 99.57% Valid Acc:91.64% Train loss:0.0120 Valid loss:0.4084 LR:0.001
Training: Epoch[157/190] Iteration[050/391] Loss: 0.0112 Acc:99.61%
Training: Epoch[157/190] Iteration[100/391] Loss: 0.0121 Acc:99.58%
Training: Epoch[157/190] Iteration[150/391] Loss: 0.0122 Acc:99.58%
Training: Epoch[157/190] Iteration[200/391] Loss: 0.0126 Acc:99.55%
Training: Epoch[157/190] Iteration[250/391] Loss: 0.0131 Acc:99.56%
Training: Epoch[157/190] Iteration[300/391] Loss: 0.0132 Acc:99.54%
Training: Epoch[157/190] Iteration[350/391] Loss: 0.0130 Acc:99.56%
Epoch[157/190] Train Acc: 99.55% Valid Acc:91.58% Train loss:0.0131 Valid loss:0.4112 LR:0.001
Training: Epoch[158/190] Iteration[050/391] Loss: 0.0136 Acc:99.58%
Training: Epoch[158/190] Iteration[100/391] Loss: 0.0149 Acc:99.51%
Training: Epoch[158/190] Iteration[150/391] Loss: 0.0141 Acc:99.52%
Training: Epoch[158/190] Iteration[200/391] Loss: 0.0136 Acc:99.54%
Training: Epoch[158/190] Iteration[250/391] Loss: 0.0140 Acc:99.52%
Training: Epoch[158/190] Iteration[300/391] Loss: 0.0134 Acc:99.54%
Training: Epoch[158/190] Iteration[350/391] Loss: 0.0131 Acc:99.55%
Epoch[158/190] Train Acc: 99.56% Valid Acc:91.72% Train loss:0.0127 Valid loss:0.4095 LR:0.001
Training: Epoch[159/190] Iteration[050/391] Loss: 0.0110 Acc:99.59%
Training: Epoch[159/190] Iteration[100/391] Loss: 0.0127 Acc:99.57%
Training: Epoch[159/190] Iteration[150/391] Loss: 0.0129 Acc:99.58%
Training: Epoch[159/190] Iteration[200/391] Loss: 0.0129 Acc:99.57%
Training: Epoch[159/190] Iteration[250/391] Loss: 0.0128 Acc:99.56%
Training: Epoch[159/190] Iteration[300/391] Loss: 0.0127 Acc:99.55%
Training: Epoch[159/190] Iteration[350/391] Loss: 0.0126 Acc:99.56%
Epoch[159/190] Train Acc: 99.56% Valid Acc:91.63% Train loss:0.0124 Valid loss:0.4094 LR:0.001
Training: Epoch[160/190] Iteration[050/391] Loss: 0.0103 Acc:99.73%
Training: Epoch[160/190] Iteration[100/391] Loss: 0.0103 Acc:99.70%
Training: Epoch[160/190] Iteration[150/391] Loss: 0.0108 Acc:99.65%
Training: Epoch[160/190] Iteration[200/391] Loss: 0.0112 Acc:99.63%
Training: Epoch[160/190] Iteration[250/391] Loss: 0.0117 Acc:99.61%
Training: Epoch[160/190] Iteration[300/391] Loss: 0.0120 Acc:99.60%
Training: Epoch[160/190] Iteration[350/391] Loss: 0.0119 Acc:99.60%
Epoch[160/190] Train Acc: 99.60% Valid Acc:91.63% Train loss:0.0121 Valid loss:0.4134 LR:0.001
Training: Epoch[161/190] Iteration[050/391] Loss: 0.0156 Acc:99.48%
Training: Epoch[161/190] Iteration[100/391] Loss: 0.0132 Acc:99.52%
Training: Epoch[161/190] Iteration[150/391] Loss: 0.0133 Acc:99.51%
Training: Epoch[161/190] Iteration[200/391] Loss: 0.0137 Acc:99.51%
Training: Epoch[161/190] Iteration[250/391] Loss: 0.0144 Acc:99.49%
Training: Epoch[161/190] Iteration[300/391] Loss: 0.0143 Acc:99.50%
Training: Epoch[161/190] Iteration[350/391] Loss: 0.0136 Acc:99.53%
Epoch[161/190] Train Acc: 99.54% Valid Acc:91.54% Train loss:0.0133 Valid loss:0.4147 LR:0.001
Training: Epoch[162/190] Iteration[050/391] Loss: 0.0125 Acc:99.53%
Training: Epoch[162/190] Iteration[100/391] Loss: 0.0126 Acc:99.58%
Training: Epoch[162/190] Iteration[150/391] Loss: 0.0127 Acc:99.55%
Training: Epoch[162/190] Iteration[200/391] Loss: 0.0136 Acc:99.52%
Training: Epoch[162/190] Iteration[250/391] Loss: 0.0133 Acc:99.53%
Training: Epoch[162/190] Iteration[300/391] Loss: 0.0131 Acc:99.54%
Training: Epoch[162/190] Iteration[350/391] Loss: 0.0134 Acc:99.54%
Epoch[162/190] Train Acc: 99.53% Valid Acc:91.60% Train loss:0.0137 Valid loss:0.4137 LR:0.001
Training: Epoch[163/190] Iteration[050/391] Loss: 0.0154 Acc:99.50%
Training: Epoch[163/190] Iteration[100/391] Loss: 0.0124 Acc:99.60%
Training: Epoch[163/190] Iteration[150/391] Loss: 0.0127 Acc:99.59%
Training: Epoch[163/190] Iteration[200/391] Loss: 0.0125 Acc:99.57%
Training: Epoch[163/190] Iteration[250/391] Loss: 0.0124 Acc:99.58%
Training: Epoch[163/190] Iteration[300/391] Loss: 0.0128 Acc:99.57%
Training: Epoch[163/190] Iteration[350/391] Loss: 0.0128 Acc:99.57%
Epoch[163/190] Train Acc: 99.59% Valid Acc:91.59% Train loss:0.0126 Valid loss:0.4130 LR:0.001
Training: Epoch[164/190] Iteration[050/391] Loss: 0.0120 Acc:99.58%
Training: Epoch[164/190] Iteration[100/391] Loss: 0.0111 Acc:99.62%
Training: Epoch[164/190] Iteration[150/391] Loss: 0.0113 Acc:99.60%
Training: Epoch[164/190] Iteration[200/391] Loss: 0.0111 Acc:99.61%
Training: Epoch[164/190] Iteration[250/391] Loss: 0.0115 Acc:99.61%
Training: Epoch[164/190] Iteration[300/391] Loss: 0.0114 Acc:99.61%
Training: Epoch[164/190] Iteration[350/391] Loss: 0.0112 Acc:99.62%
Epoch[164/190] Train Acc: 99.62% Valid Acc:91.63% Train loss:0.0112 Valid loss:0.4124 LR:0.001
Training: Epoch[165/190] Iteration[050/391] Loss: 0.0127 Acc:99.56%
Training: Epoch[165/190] Iteration[100/391] Loss: 0.0127 Acc:99.53%
Training: Epoch[165/190] Iteration[150/391] Loss: 0.0129 Acc:99.57%
Training: Epoch[165/190] Iteration[200/391] Loss: 0.0126 Acc:99.58%
Training: Epoch[165/190] Iteration[250/391] Loss: 0.0128 Acc:99.58%
Training: Epoch[165/190] Iteration[300/391] Loss: 0.0130 Acc:99.56%
Training: Epoch[165/190] Iteration[350/391] Loss: 0.0130 Acc:99.57%
Epoch[165/190] Train Acc: 99.56% Valid Acc:91.54% Train loss:0.0130 Valid loss:0.4173 LR:0.001
Training: Epoch[166/190] Iteration[050/391] Loss: 0.0118 Acc:99.62%
Training: Epoch[166/190] Iteration[100/391] Loss: 0.0111 Acc:99.65%
Training: Epoch[166/190] Iteration[150/391] Loss: 0.0112 Acc:99.64%
Training: Epoch[166/190] Iteration[200/391] Loss: 0.0111 Acc:99.65%
Training: Epoch[166/190] Iteration[250/391] Loss: 0.0118 Acc:99.62%
Training: Epoch[166/190] Iteration[300/391] Loss: 0.0119 Acc:99.62%
Training: Epoch[166/190] Iteration[350/391] Loss: 0.0118 Acc:99.62%
Epoch[166/190] Train Acc: 99.61% Valid Acc:91.49% Train loss:0.0121 Valid loss:0.4168 LR:0.001
Training: Epoch[167/190] Iteration[050/391] Loss: 0.0109 Acc:99.66%
Training: Epoch[167/190] Iteration[100/391] Loss: 0.0112 Acc:99.64%
Training: Epoch[167/190] Iteration[150/391] Loss: 0.0109 Acc:99.65%
Training: Epoch[167/190] Iteration[200/391] Loss: 0.0114 Acc:99.63%
Training: Epoch[167/190] Iteration[250/391] Loss: 0.0118 Acc:99.62%
Training: Epoch[167/190] Iteration[300/391] Loss: 0.0115 Acc:99.61%
Training: Epoch[167/190] Iteration[350/391] Loss: 0.0113 Acc:99.62%
Epoch[167/190] Train Acc: 99.60% Valid Acc:91.44% Train loss:0.0116 Valid loss:0.4204 LR:0.001
Training: Epoch[168/190] Iteration[050/391] Loss: 0.0144 Acc:99.38%
Training: Epoch[168/190] Iteration[100/391] Loss: 0.0127 Acc:99.48%
Training: Epoch[168/190] Iteration[150/391] Loss: 0.0134 Acc:99.49%
Training: Epoch[168/190] Iteration[200/391] Loss: 0.0130 Acc:99.52%
Training: Epoch[168/190] Iteration[250/391] Loss: 0.0131 Acc:99.54%
Training: Epoch[168/190] Iteration[300/391] Loss: 0.0124 Acc:99.57%
Training: Epoch[168/190] Iteration[350/391] Loss: 0.0120 Acc:99.58%
Epoch[168/190] Train Acc: 99.57% Valid Acc:91.42% Train loss:0.0122 Valid loss:0.4185 LR:0.001
Training: Epoch[169/190] Iteration[050/391] Loss: 0.0108 Acc:99.72%
Training: Epoch[169/190] Iteration[100/391] Loss: 0.0113 Acc:99.62%
Training: Epoch[169/190] Iteration[150/391] Loss: 0.0110 Acc:99.64%
Training: Epoch[169/190] Iteration[200/391] Loss: 0.0122 Acc:99.60%
Training: Epoch[169/190] Iteration[250/391] Loss: 0.0121 Acc:99.62%
Training: Epoch[169/190] Iteration[300/391] Loss: 0.0119 Acc:99.63%
Training: Epoch[169/190] Iteration[350/391] Loss: 0.0120 Acc:99.64%
Epoch[169/190] Train Acc: 99.63% Valid Acc:91.61% Train loss:0.0122 Valid loss:0.4200 LR:0.001
Training: Epoch[170/190] Iteration[050/391] Loss: 0.0109 Acc:99.64%
Training: Epoch[170/190] Iteration[100/391] Loss: 0.0103 Acc:99.66%
Training: Epoch[170/190] Iteration[150/391] Loss: 0.0110 Acc:99.62%
Training: Epoch[170/190] Iteration[200/391] Loss: 0.0108 Acc:99.64%
Training: Epoch[170/190] Iteration[250/391] Loss: 0.0112 Acc:99.63%
Training: Epoch[170/190] Iteration[300/391] Loss: 0.0108 Acc:99.65%
Training: Epoch[170/190] Iteration[350/391] Loss: 0.0104 Acc:99.66%
Epoch[170/190] Train Acc: 99.65% Valid Acc:91.49% Train loss:0.0107 Valid loss:0.4231 LR:0.001
Training: Epoch[171/190] Iteration[050/391] Loss: 0.0127 Acc:99.52%
Training: Epoch[171/190] Iteration[100/391] Loss: 0.0122 Acc:99.59%
Training: Epoch[171/190] Iteration[150/391] Loss: 0.0112 Acc:99.62%
Training: Epoch[171/190] Iteration[200/391] Loss: 0.0110 Acc:99.64%
Training: Epoch[171/190] Iteration[250/391] Loss: 0.0113 Acc:99.63%
Training: Epoch[171/190] Iteration[300/391] Loss: 0.0115 Acc:99.61%
Training: Epoch[171/190] Iteration[350/391] Loss: 0.0114 Acc:99.60%
Epoch[171/190] Train Acc: 99.61% Valid Acc:91.64% Train loss:0.0115 Valid loss:0.4190 LR:0.001
Training: Epoch[172/190] Iteration[050/391] Loss: 0.0098 Acc:99.72%
Training: Epoch[172/190] Iteration[100/391] Loss: 0.0116 Acc:99.62%
Training: Epoch[172/190] Iteration[150/391] Loss: 0.0112 Acc:99.62%
Training: Epoch[172/190] Iteration[200/391] Loss: 0.0113 Acc:99.64%
Training: Epoch[172/190] Iteration[250/391] Loss: 0.0116 Acc:99.62%
Training: Epoch[172/190] Iteration[300/391] Loss: 0.0116 Acc:99.62%
Training: Epoch[172/190] Iteration[350/391] Loss: 0.0115 Acc:99.62%
Epoch[172/190] Train Acc: 99.63% Valid Acc:91.60% Train loss:0.0114 Valid loss:0.4214 LR:0.001
Training: Epoch[173/190] Iteration[050/391] Loss: 0.0138 Acc:99.58%
Training: Epoch[173/190] Iteration[100/391] Loss: 0.0133 Acc:99.60%
Training: Epoch[173/190] Iteration[150/391] Loss: 0.0121 Acc:99.64%
Training: Epoch[173/190] Iteration[200/391] Loss: 0.0113 Acc:99.67%
Training: Epoch[173/190] Iteration[250/391] Loss: 0.0122 Acc:99.61%
Training: Epoch[173/190] Iteration[300/391] Loss: 0.0119 Acc:99.61%
Training: Epoch[173/190] Iteration[350/391] Loss: 0.0119 Acc:99.61%
Epoch[173/190] Train Acc: 99.61% Valid Acc:91.58% Train loss:0.0120 Valid loss:0.4230 LR:0.001
Training: Epoch[174/190] Iteration[050/391] Loss: 0.0095 Acc:99.77%
Training: Epoch[174/190] Iteration[100/391] Loss: 0.0084 Acc:99.78%
Training: Epoch[174/190] Iteration[150/391] Loss: 0.0097 Acc:99.71%
Training: Epoch[174/190] Iteration[200/391] Loss: 0.0107 Acc:99.66%
Training: Epoch[174/190] Iteration[250/391] Loss: 0.0103 Acc:99.67%
Training: Epoch[174/190] Iteration[300/391] Loss: 0.0103 Acc:99.68%
Training: Epoch[174/190] Iteration[350/391] Loss: 0.0105 Acc:99.67%
Epoch[174/190] Train Acc: 99.67% Valid Acc:91.45% Train loss:0.0105 Valid loss:0.4229 LR:0.001
Training: Epoch[175/190] Iteration[050/391] Loss: 0.0097 Acc:99.70%
Training: Epoch[175/190] Iteration[100/391] Loss: 0.0112 Acc:99.61%
Training: Epoch[175/190] Iteration[150/391] Loss: 0.0108 Acc:99.66%
Training: Epoch[175/190] Iteration[200/391] Loss: 0.0107 Acc:99.65%
Training: Epoch[175/190] Iteration[250/391] Loss: 0.0111 Acc:99.62%
Training: Epoch[175/190] Iteration[300/391] Loss: 0.0109 Acc:99.63%
Training: Epoch[175/190] Iteration[350/391] Loss: 0.0108 Acc:99.63%
Epoch[175/190] Train Acc: 99.62% Valid Acc:91.46% Train loss:0.0108 Valid loss:0.4224 LR:0.001
Training: Epoch[176/190] Iteration[050/391] Loss: 0.0114 Acc:99.59%
Training: Epoch[176/190] Iteration[100/391] Loss: 0.0116 Acc:99.57%
Training: Epoch[176/190] Iteration[150/391] Loss: 0.0132 Acc:99.57%
Training: Epoch[176/190] Iteration[200/391] Loss: 0.0127 Acc:99.57%
Training: Epoch[176/190] Iteration[250/391] Loss: 0.0123 Acc:99.60%
Training: Epoch[176/190] Iteration[300/391] Loss: 0.0123 Acc:99.59%
Training: Epoch[176/190] Iteration[350/391] Loss: 0.0122 Acc:99.60%
Epoch[176/190] Train Acc: 99.60% Valid Acc:91.68% Train loss:0.0119 Valid loss:0.4195 LR:0.001
Training: Epoch[177/190] Iteration[050/391] Loss: 0.0114 Acc:99.69%
Training: Epoch[177/190] Iteration[100/391] Loss: 0.0108 Acc:99.65%
Training: Epoch[177/190] Iteration[150/391] Loss: 0.0104 Acc:99.66%
Training: Epoch[177/190] Iteration[200/391] Loss: 0.0101 Acc:99.67%
Training: Epoch[177/190] Iteration[250/391] Loss: 0.0099 Acc:99.67%
Training: Epoch[177/190] Iteration[300/391] Loss: 0.0101 Acc:99.66%
Training: Epoch[177/190] Iteration[350/391] Loss: 0.0100 Acc:99.67%
Epoch[177/190] Train Acc: 99.68% Valid Acc:91.48% Train loss:0.0099 Valid loss:0.4248 LR:0.001
Training: Epoch[178/190] Iteration[050/391] Loss: 0.0098 Acc:99.73%
Training: Epoch[178/190] Iteration[100/391] Loss: 0.0111 Acc:99.66%
Training: Epoch[178/190] Iteration[150/391] Loss: 0.0112 Acc:99.65%
Training: Epoch[178/190] Iteration[200/391] Loss: 0.0116 Acc:99.63%
Training: Epoch[178/190] Iteration[250/391] Loss: 0.0114 Acc:99.65%
Training: Epoch[178/190] Iteration[300/391] Loss: 0.0113 Acc:99.65%
Training: Epoch[178/190] Iteration[350/391] Loss: 0.0110 Acc:99.66%
Epoch[178/190] Train Acc: 99.66% Valid Acc:91.51% Train loss:0.0109 Valid loss:0.4242 LR:0.001
Training: Epoch[179/190] Iteration[050/391] Loss: 0.0098 Acc:99.67%
Training: Epoch[179/190] Iteration[100/391] Loss: 0.0104 Acc:99.66%
Training: Epoch[179/190] Iteration[150/391] Loss: 0.0106 Acc:99.64%
Training: Epoch[179/190] Iteration[200/391] Loss: 0.0108 Acc:99.65%
Training: Epoch[179/190] Iteration[250/391] Loss: 0.0113 Acc:99.63%
Training: Epoch[179/190] Iteration[300/391] Loss: 0.0115 Acc:99.64%
Training: Epoch[179/190] Iteration[350/391] Loss: 0.0114 Acc:99.63%
Epoch[179/190] Train Acc: 99.62% Valid Acc:91.64% Train loss:0.0113 Valid loss:0.4233 LR:0.001
Training: Epoch[180/190] Iteration[050/391] Loss: 0.0077 Acc:99.78%
Training: Epoch[180/190] Iteration[100/391] Loss: 0.0086 Acc:99.74%
Training: Epoch[180/190] Iteration[150/391] Loss: 0.0095 Acc:99.69%
Training: Epoch[180/190] Iteration[200/391] Loss: 0.0095 Acc:99.69%
Training: Epoch[180/190] Iteration[250/391] Loss: 0.0100 Acc:99.67%
Training: Epoch[180/190] Iteration[300/391] Loss: 0.0105 Acc:99.67%
Training: Epoch[180/190] Iteration[350/391] Loss: 0.0108 Acc:99.67%
Epoch[180/190] Train Acc: 99.67% Valid Acc:91.61% Train loss:0.0108 Valid loss:0.4227 LR:0.001
Training: Epoch[181/190] Iteration[050/391] Loss: 0.0129 Acc:99.55%
Training: Epoch[181/190] Iteration[100/391] Loss: 0.0114 Acc:99.61%
Training: Epoch[181/190] Iteration[150/391] Loss: 0.0118 Acc:99.59%
Training: Epoch[181/190] Iteration[200/391] Loss: 0.0118 Acc:99.60%
Training: Epoch[181/190] Iteration[250/391] Loss: 0.0116 Acc:99.61%
Training: Epoch[181/190] Iteration[300/391] Loss: 0.0115 Acc:99.62%
Training: Epoch[181/190] Iteration[350/391] Loss: 0.0113 Acc:99.62%
Epoch[181/190] Train Acc: 99.63% Valid Acc:91.56% Train loss:0.0113 Valid loss:0.4257 LR:0.001
Training: Epoch[182/190] Iteration[050/391] Loss: 0.0095 Acc:99.67%
Training: Epoch[182/190] Iteration[100/391] Loss: 0.0091 Acc:99.70%
Training: Epoch[182/190] Iteration[150/391] Loss: 0.0090 Acc:99.69%
Training: Epoch[182/190] Iteration[200/391] Loss: 0.0109 Acc:99.64%
Training: Epoch[182/190] Iteration[250/391] Loss: 0.0105 Acc:99.66%
Training: Epoch[182/190] Iteration[300/391] Loss: 0.0104 Acc:99.66%
Training: Epoch[182/190] Iteration[350/391] Loss: 0.0104 Acc:99.66%
Epoch[182/190] Train Acc: 99.66% Valid Acc:91.47% Train loss:0.0105 Valid loss:0.4281 LR:0.001
Training: Epoch[183/190] Iteration[050/391] Loss: 0.0092 Acc:99.69%
Training: Epoch[183/190] Iteration[100/391] Loss: 0.0114 Acc:99.60%
Training: Epoch[183/190] Iteration[150/391] Loss: 0.0105 Acc:99.64%
Training: Epoch[183/190] Iteration[200/391] Loss: 0.0102 Acc:99.66%
Training: Epoch[183/190] Iteration[250/391] Loss: 0.0104 Acc:99.66%
Training: Epoch[183/190] Iteration[300/391] Loss: 0.0103 Acc:99.67%
Training: Epoch[183/190] Iteration[350/391] Loss: 0.0104 Acc:99.67%
Epoch[183/190] Train Acc: 99.66% Valid Acc:91.57% Train loss:0.0105 Valid loss:0.4273 LR:0.001
Training: Epoch[184/190] Iteration[050/391] Loss: 0.0101 Acc:99.61%
Training: Epoch[184/190] Iteration[100/391] Loss: 0.0096 Acc:99.64%
Training: Epoch[184/190] Iteration[150/391] Loss: 0.0091 Acc:99.67%
Training: Epoch[184/190] Iteration[200/391] Loss: 0.0094 Acc:99.68%
Training: Epoch[184/190] Iteration[250/391] Loss: 0.0099 Acc:99.67%
Training: Epoch[184/190] Iteration[300/391] Loss: 0.0093 Acc:99.70%
Training: Epoch[184/190] Iteration[350/391] Loss: 0.0092 Acc:99.71%
Epoch[184/190] Train Acc: 99.71% Valid Acc:91.56% Train loss:0.0093 Valid loss:0.4266 LR:0.001
Training: Epoch[185/190] Iteration[050/391] Loss: 0.0104 Acc:99.66%
Training: Epoch[185/190] Iteration[100/391] Loss: 0.0103 Acc:99.63%
Training: Epoch[185/190] Iteration[150/391] Loss: 0.0096 Acc:99.67%
Training: Epoch[185/190] Iteration[200/391] Loss: 0.0099 Acc:99.66%
Training: Epoch[185/190] Iteration[250/391] Loss: 0.0096 Acc:99.68%
Training: Epoch[185/190] Iteration[300/391] Loss: 0.0095 Acc:99.69%
Training: Epoch[185/190] Iteration[350/391] Loss: 0.0094 Acc:99.68%
Epoch[185/190] Train Acc: 99.68% Valid Acc:91.58% Train loss:0.0095 Valid loss:0.4281 LR:0.001
Training: Epoch[186/190] Iteration[050/391] Loss: 0.0102 Acc:99.69%
Training: Epoch[186/190] Iteration[100/391] Loss: 0.0106 Acc:99.65%
Training: Epoch[186/190] Iteration[150/391] Loss: 0.0116 Acc:99.58%
Training: Epoch[186/190] Iteration[200/391] Loss: 0.0105 Acc:99.64%
Training: Epoch[186/190] Iteration[250/391] Loss: 0.0095 Acc:99.68%
Training: Epoch[186/190] Iteration[300/391] Loss: 0.0097 Acc:99.68%
Training: Epoch[186/190] Iteration[350/391] Loss: 0.0099 Acc:99.66%
Epoch[186/190] Train Acc: 99.66% Valid Acc:91.62% Train loss:0.0100 Valid loss:0.4280 LR:0.001
Training: Epoch[187/190] Iteration[050/391] Loss: 0.0125 Acc:99.50%
Training: Epoch[187/190] Iteration[100/391] Loss: 0.0127 Acc:99.55%
Training: Epoch[187/190] Iteration[150/391] Loss: 0.0114 Acc:99.61%
Training: Epoch[187/190] Iteration[200/391] Loss: 0.0116 Acc:99.59%
Training: Epoch[187/190] Iteration[250/391] Loss: 0.0113 Acc:99.61%
Training: Epoch[187/190] Iteration[300/391] Loss: 0.0112 Acc:99.62%
Training: Epoch[187/190] Iteration[350/391] Loss: 0.0112 Acc:99.62%
Epoch[187/190] Train Acc: 99.63% Valid Acc:91.60% Train loss:0.0110 Valid loss:0.4296 LR:0.001
Training: Epoch[188/190] Iteration[050/391] Loss: 0.0125 Acc:99.62%
Training: Epoch[188/190] Iteration[100/391] Loss: 0.0103 Acc:99.70%
Training: Epoch[188/190] Iteration[150/391] Loss: 0.0107 Acc:99.66%
Training: Epoch[188/190] Iteration[200/391] Loss: 0.0104 Acc:99.68%
Training: Epoch[188/190] Iteration[250/391] Loss: 0.0105 Acc:99.67%
Training: Epoch[188/190] Iteration[300/391] Loss: 0.0098 Acc:99.70%
Training: Epoch[188/190] Iteration[350/391] Loss: 0.0097 Acc:99.69%
Epoch[188/190] Train Acc: 99.70% Valid Acc:91.59% Train loss:0.0096 Valid loss:0.4305 LR:0.001
Training: Epoch[189/190] Iteration[050/391] Loss: 0.0103 Acc:99.58%
Training: Epoch[189/190] Iteration[100/391] Loss: 0.0105 Acc:99.60%
Training: Epoch[189/190] Iteration[150/391] Loss: 0.0103 Acc:99.64%
Training: Epoch[189/190] Iteration[200/391] Loss: 0.0099 Acc:99.65%
Training: Epoch[189/190] Iteration[250/391] Loss: 0.0096 Acc:99.68%
Training: Epoch[189/190] Iteration[300/391] Loss: 0.0100 Acc:99.65%
Training: Epoch[189/190] Iteration[350/391] Loss: 0.0100 Acc:99.66%
Epoch[189/190] Train Acc: 99.67% Valid Acc:91.63% Train loss:0.0100 Valid loss:0.4266 LR:0.001
Training: Epoch[190/190] Iteration[050/391] Loss: 0.0077 Acc:99.75%
Training: Epoch[190/190] Iteration[100/391] Loss: 0.0083 Acc:99.70%
Training: Epoch[190/190] Iteration[150/391] Loss: 0.0093 Acc:99.68%
Training: Epoch[190/190] Iteration[200/391] Loss: 0.0099 Acc:99.65%
Training: Epoch[190/190] Iteration[250/391] Loss: 0.0098 Acc:99.65%
Training: Epoch[190/190] Iteration[300/391] Loss: 0.0097 Acc:99.66%
Training: Epoch[190/190] Iteration[350/391] Loss: 0.0098 Acc:99.67%
Epoch[190/190] Train Acc: 99.67% Valid Acc:91.72% Train loss:0.0100 Valid loss:0.4296 LR:0.001
class:plane     , total num:5000.0, correct num:4990.0  Recall: 99.80% Precision: 99.58%
class:car       , total num:5000.0, correct num:4993.0  Recall: 99.86% Precision: 99.90%
class:bird      , total num:5000.0, correct num:4982.0  Recall: 99.64% Precision: 99.64%
class:cat       , total num:5000.0, correct num:4964.0  Recall: 99.28% Precision: 99.18%
class:deer      , total num:5000.0, correct num:4986.0  Recall: 99.72% Precision: 99.78%
class:dog       , total num:5000.0, correct num:4960.0  Recall: 99.20% Precision: 99.50%
class:frog      , total num:5000.0, correct num:4991.0  Recall: 99.82% Precision: 99.72%
class:horse     , total num:5000.0, correct num:4986.0  Recall: 99.72% Precision: 99.76%
class:ship      , total num:5000.0, correct num:4991.0  Recall: 99.82% Precision: 99.82%
class:truck     , total num:5000.0, correct num:4990.0  Recall: 99.80% Precision: 99.78%
class:plane     , total num:1000.0, correct num:929.0  Recall: 92.89% Precision: 92.34%
class:car       , total num:1000.0, correct num:964.0  Recall: 96.39% Precision: 96.39%
class:bird      , total num:1000.0, correct num:883.0  Recall: 88.29% Precision: 91.30%
class:cat       , total num:1000.0, correct num:810.0  Recall: 80.99% Precision: 83.41%
class:deer      , total num:1000.0, correct num:923.0  Recall: 92.29% Precision: 91.02%
class:dog       , total num:1000.0, correct num:877.0  Recall: 87.69% Precision: 84.97%
class:frog      , total num:1000.0, correct num:953.0  Recall: 95.29% Precision: 93.70%
class:horse     , total num:1000.0, correct num:937.0  Recall: 93.69% Precision: 94.64%
class:ship      , total num:1000.0, correct num:948.0  Recall: 94.79% Precision: 94.13%
class:truck     , total num:1000.0, correct num:948.0  Recall: 94.79% Precision: 95.17%
 done ~~~~ 04-02_06-40, best acc: 0.9174 in :152
