nohup: ignoring input
PID:161263
set gpu list :1,0

device_count: 2
args:
Namespace(arc='resnet56', bs=128, frozen_primary=False, gpu=[1, 0], low_lr=False, lr=0.1, max_epoch=190, point_conv=False, pretrain=False, replace_conv=False, start_epoch=0)
 cfg:
{'cls_names': ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'], 'milestones': [92, 136], 'valid_bs': 128, 'transforms_valid': Compose(
    Resize(size=(32, 32), interpolation=PIL.Image.BILINEAR)
    ToTensor()
    Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
), 'workers': 8, 'log_interval': 50, 'patience': 20, 'transforms_train': Compose(
    Resize(size=32, interpolation=PIL.Image.BILINEAR)
    RandomHorizontalFlip(p=0.5)
    RandomCrop(size=(32, 32), padding=4)
    ToTensor()
    Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
), 'factor': 0.1, 'class_num': 10, 'train_bs': 128, 'weight_decay': 0.0001, 'momentum': 0.9}
 loss_f:
CrossEntropyLoss()
 scheduler:
<torch.optim.lr_scheduler.MultiStepLR object at 0x7fb5a6a7a890>
 optimizer:
SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.1
    lr: 0.1
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0001
)
Training: Epoch[001/190] Iteration[050/391] Loss: 2.2252 Acc:22.70%
Training: Epoch[001/190] Iteration[100/391] Loss: 2.0099 Acc:27.20%
Training: Epoch[001/190] Iteration[150/391] Loss: 1.8932 Acc:30.71%
Training: Epoch[001/190] Iteration[200/391] Loss: 1.8185 Acc:33.38%
Training: Epoch[001/190] Iteration[250/391] Loss: 1.7565 Acc:35.46%
Training: Epoch[001/190] Iteration[300/391] Loss: 1.7054 Acc:37.31%
Training: Epoch[001/190] Iteration[350/391] Loss: 1.6571 Acc:39.15%
Epoch[001/190] Train Acc: 40.67% Valid Acc:52.47% Train loss:1.6172 Valid loss:1.3111 LR:0.1
Training: Epoch[002/190] Iteration[050/391] Loss: 1.2751 Acc:54.28%
Training: Epoch[002/190] Iteration[100/391] Loss: 1.2190 Acc:56.27%
Training: Epoch[002/190] Iteration[150/391] Loss: 1.1905 Acc:57.13%
Training: Epoch[002/190] Iteration[200/391] Loss: 1.1680 Acc:58.09%
Training: Epoch[002/190] Iteration[250/391] Loss: 1.1433 Acc:59.05%
Training: Epoch[002/190] Iteration[300/391] Loss: 1.1268 Acc:59.74%
Training: Epoch[002/190] Iteration[350/391] Loss: 1.1046 Acc:60.65%
Epoch[002/190] Train Acc: 61.22% Valid Acc:66.09% Train loss:1.0883 Valid loss:0.9635 LR:0.1
Training: Epoch[003/190] Iteration[050/391] Loss: 0.9069 Acc:67.77%
Training: Epoch[003/190] Iteration[100/391] Loss: 0.9056 Acc:68.22%
Training: Epoch[003/190] Iteration[150/391] Loss: 0.8966 Acc:68.49%
Training: Epoch[003/190] Iteration[200/391] Loss: 0.8875 Acc:68.77%
Training: Epoch[003/190] Iteration[250/391] Loss: 0.8814 Acc:69.05%
Training: Epoch[003/190] Iteration[300/391] Loss: 0.8701 Acc:69.45%
Training: Epoch[003/190] Iteration[350/391] Loss: 0.8576 Acc:69.85%
Epoch[003/190] Train Acc: 70.10% Valid Acc:67.06% Train loss:0.8508 Valid loss:0.9316 LR:0.1
Training: Epoch[004/190] Iteration[050/391] Loss: 0.7500 Acc:73.89%
Training: Epoch[004/190] Iteration[100/391] Loss: 0.7520 Acc:73.61%
Training: Epoch[004/190] Iteration[150/391] Loss: 0.7461 Acc:73.65%
Training: Epoch[004/190] Iteration[200/391] Loss: 0.7385 Acc:74.13%
Training: Epoch[004/190] Iteration[250/391] Loss: 0.7330 Acc:74.38%
Training: Epoch[004/190] Iteration[300/391] Loss: 0.7310 Acc:74.43%
Training: Epoch[004/190] Iteration[350/391] Loss: 0.7246 Acc:74.80%
Epoch[004/190] Train Acc: 74.90% Valid Acc:75.25% Train loss:0.7215 Valid loss:0.7229 LR:0.1
Training: Epoch[005/190] Iteration[050/391] Loss: 0.6614 Acc:76.88%
Training: Epoch[005/190] Iteration[100/391] Loss: 0.6562 Acc:77.13%
Training: Epoch[005/190] Iteration[150/391] Loss: 0.6590 Acc:77.11%
Training: Epoch[005/190] Iteration[200/391] Loss: 0.6609 Acc:77.10%
Training: Epoch[005/190] Iteration[250/391] Loss: 0.6531 Acc:77.43%
Training: Epoch[005/190] Iteration[300/391] Loss: 0.6525 Acc:77.38%
Training: Epoch[005/190] Iteration[350/391] Loss: 0.6466 Acc:77.58%
Epoch[005/190] Train Acc: 77.59% Valid Acc:71.40% Train loss:0.6457 Valid loss:0.8744 LR:0.1
Training: Epoch[006/190] Iteration[050/391] Loss: 0.6002 Acc:79.25%
Training: Epoch[006/190] Iteration[100/391] Loss: 0.6049 Acc:79.11%
Training: Epoch[006/190] Iteration[150/391] Loss: 0.5935 Acc:79.54%
Training: Epoch[006/190] Iteration[200/391] Loss: 0.5931 Acc:79.44%
Training: Epoch[006/190] Iteration[250/391] Loss: 0.5950 Acc:79.38%
Training: Epoch[006/190] Iteration[300/391] Loss: 0.5932 Acc:79.46%
Training: Epoch[006/190] Iteration[350/391] Loss: 0.5890 Acc:79.56%
Epoch[006/190] Train Acc: 79.62% Valid Acc:75.93% Train loss:0.5867 Valid loss:0.7468 LR:0.1
Training: Epoch[007/190] Iteration[050/391] Loss: 0.5638 Acc:80.84%
Training: Epoch[007/190] Iteration[100/391] Loss: 0.5686 Acc:80.62%
Training: Epoch[007/190] Iteration[150/391] Loss: 0.5533 Acc:81.05%
Training: Epoch[007/190] Iteration[200/391] Loss: 0.5510 Acc:81.19%
Training: Epoch[007/190] Iteration[250/391] Loss: 0.5421 Acc:81.45%
Training: Epoch[007/190] Iteration[300/391] Loss: 0.5448 Acc:81.25%
Training: Epoch[007/190] Iteration[350/391] Loss: 0.5461 Acc:81.16%
Epoch[007/190] Train Acc: 81.20% Valid Acc:80.10% Train loss:0.5443 Valid loss:0.5845 LR:0.1
Training: Epoch[008/190] Iteration[050/391] Loss: 0.5183 Acc:82.22%
Training: Epoch[008/190] Iteration[100/391] Loss: 0.5157 Acc:82.14%
Training: Epoch[008/190] Iteration[150/391] Loss: 0.5171 Acc:82.23%
Training: Epoch[008/190] Iteration[200/391] Loss: 0.5117 Acc:82.36%
Training: Epoch[008/190] Iteration[250/391] Loss: 0.5033 Acc:82.64%
Training: Epoch[008/190] Iteration[300/391] Loss: 0.5004 Acc:82.76%
Training: Epoch[008/190] Iteration[350/391] Loss: 0.4988 Acc:82.80%
Epoch[008/190] Train Acc: 82.82% Valid Acc:75.68% Train loss:0.4997 Valid loss:0.7756 LR:0.1
Training: Epoch[009/190] Iteration[050/391] Loss: 0.4643 Acc:83.50%
Training: Epoch[009/190] Iteration[100/391] Loss: 0.4715 Acc:83.38%
Training: Epoch[009/190] Iteration[150/391] Loss: 0.4741 Acc:83.56%
Training: Epoch[009/190] Iteration[200/391] Loss: 0.4788 Acc:83.40%
Training: Epoch[009/190] Iteration[250/391] Loss: 0.4801 Acc:83.31%
Training: Epoch[009/190] Iteration[300/391] Loss: 0.4781 Acc:83.43%
Training: Epoch[009/190] Iteration[350/391] Loss: 0.4776 Acc:83.43%
Epoch[009/190] Train Acc: 83.40% Valid Acc:81.20% Train loss:0.4784 Valid loss:0.5643 LR:0.1
Training: Epoch[010/190] Iteration[050/391] Loss: 0.4545 Acc:84.44%
Training: Epoch[010/190] Iteration[100/391] Loss: 0.4555 Acc:84.26%
Training: Epoch[010/190] Iteration[150/391] Loss: 0.4579 Acc:84.08%
Training: Epoch[010/190] Iteration[200/391] Loss: 0.4547 Acc:84.24%
Training: Epoch[010/190] Iteration[250/391] Loss: 0.4571 Acc:84.17%
Training: Epoch[010/190] Iteration[300/391] Loss: 0.4581 Acc:84.20%
Training: Epoch[010/190] Iteration[350/391] Loss: 0.4581 Acc:84.18%
Epoch[010/190] Train Acc: 84.25% Valid Acc:82.41% Train loss:0.4565 Valid loss:0.5445 LR:0.1
Training: Epoch[011/190] Iteration[050/391] Loss: 0.4314 Acc:84.39%
Training: Epoch[011/190] Iteration[100/391] Loss: 0.4225 Acc:84.73%
Training: Epoch[011/190] Iteration[150/391] Loss: 0.4196 Acc:85.09%
Training: Epoch[011/190] Iteration[200/391] Loss: 0.4254 Acc:84.92%
Training: Epoch[011/190] Iteration[250/391] Loss: 0.4272 Acc:84.99%
Training: Epoch[011/190] Iteration[300/391] Loss: 0.4255 Acc:85.05%
Training: Epoch[011/190] Iteration[350/391] Loss: 0.4304 Acc:84.97%
Epoch[011/190] Train Acc: 84.88% Valid Acc:80.44% Train loss:0.4326 Valid loss:0.5724 LR:0.1
Training: Epoch[012/190] Iteration[050/391] Loss: 0.3912 Acc:86.61%
Training: Epoch[012/190] Iteration[100/391] Loss: 0.3952 Acc:86.41%
Training: Epoch[012/190] Iteration[150/391] Loss: 0.4057 Acc:86.01%
Training: Epoch[012/190] Iteration[200/391] Loss: 0.4106 Acc:85.83%
Training: Epoch[012/190] Iteration[250/391] Loss: 0.4087 Acc:85.89%
Training: Epoch[012/190] Iteration[300/391] Loss: 0.4095 Acc:85.87%
Training: Epoch[012/190] Iteration[350/391] Loss: 0.4096 Acc:85.87%
Epoch[012/190] Train Acc: 85.77% Valid Acc:80.31% Train loss:0.4114 Valid loss:0.6091 LR:0.1
Training: Epoch[013/190] Iteration[050/391] Loss: 0.3906 Acc:86.05%
Training: Epoch[013/190] Iteration[100/391] Loss: 0.3887 Acc:86.41%
Training: Epoch[013/190] Iteration[150/391] Loss: 0.3919 Acc:86.21%
Training: Epoch[013/190] Iteration[200/391] Loss: 0.3941 Acc:86.21%
Training: Epoch[013/190] Iteration[250/391] Loss: 0.3943 Acc:86.19%
Training: Epoch[013/190] Iteration[300/391] Loss: 0.3908 Acc:86.32%
Training: Epoch[013/190] Iteration[350/391] Loss: 0.3953 Acc:86.20%
Epoch[013/190] Train Acc: 86.08% Valid Acc:79.91% Train loss:0.3980 Valid loss:0.6111 LR:0.1
Training: Epoch[014/190] Iteration[050/391] Loss: 0.3659 Acc:87.08%
Training: Epoch[014/190] Iteration[100/391] Loss: 0.3696 Acc:87.18%
Training: Epoch[014/190] Iteration[150/391] Loss: 0.3724 Acc:87.16%
Training: Epoch[014/190] Iteration[200/391] Loss: 0.3764 Acc:86.97%
Training: Epoch[014/190] Iteration[250/391] Loss: 0.3821 Acc:86.88%
Training: Epoch[014/190] Iteration[300/391] Loss: 0.3811 Acc:86.86%
Training: Epoch[014/190] Iteration[350/391] Loss: 0.3809 Acc:86.82%
Epoch[014/190] Train Acc: 86.78% Valid Acc:81.08% Train loss:0.3817 Valid loss:0.5759 LR:0.1
Training: Epoch[015/190] Iteration[050/391] Loss: 0.3478 Acc:87.62%
Training: Epoch[015/190] Iteration[100/391] Loss: 0.3575 Acc:87.42%
Training: Epoch[015/190] Iteration[150/391] Loss: 0.3614 Acc:87.41%
Training: Epoch[015/190] Iteration[200/391] Loss: 0.3608 Acc:87.41%
Training: Epoch[015/190] Iteration[250/391] Loss: 0.3663 Acc:87.17%
Training: Epoch[015/190] Iteration[300/391] Loss: 0.3660 Acc:87.17%
Training: Epoch[015/190] Iteration[350/391] Loss: 0.3669 Acc:87.20%
Epoch[015/190] Train Acc: 87.26% Valid Acc:84.09% Train loss:0.3652 Valid loss:0.5013 LR:0.1
Training: Epoch[016/190] Iteration[050/391] Loss: 0.3652 Acc:87.44%
Training: Epoch[016/190] Iteration[100/391] Loss: 0.3532 Acc:87.75%
Training: Epoch[016/190] Iteration[150/391] Loss: 0.3505 Acc:87.74%
Training: Epoch[016/190] Iteration[200/391] Loss: 0.3556 Acc:87.50%
Training: Epoch[016/190] Iteration[250/391] Loss: 0.3530 Acc:87.56%
Training: Epoch[016/190] Iteration[300/391] Loss: 0.3562 Acc:87.53%
Training: Epoch[016/190] Iteration[350/391] Loss: 0.3574 Acc:87.48%
Epoch[016/190] Train Acc: 87.58% Valid Acc:83.30% Train loss:0.3559 Valid loss:0.5342 LR:0.1
Training: Epoch[017/190] Iteration[050/391] Loss: 0.3509 Acc:88.31%
Training: Epoch[017/190] Iteration[100/391] Loss: 0.3511 Acc:88.11%
Training: Epoch[017/190] Iteration[150/391] Loss: 0.3491 Acc:87.95%
Training: Epoch[017/190] Iteration[200/391] Loss: 0.3481 Acc:87.99%
Training: Epoch[017/190] Iteration[250/391] Loss: 0.3496 Acc:87.92%
Training: Epoch[017/190] Iteration[300/391] Loss: 0.3512 Acc:87.79%
Training: Epoch[017/190] Iteration[350/391] Loss: 0.3523 Acc:87.71%
Epoch[017/190] Train Acc: 87.76% Valid Acc:83.25% Train loss:0.3512 Valid loss:0.5254 LR:0.1
Training: Epoch[018/190] Iteration[050/391] Loss: 0.3275 Acc:88.33%
Training: Epoch[018/190] Iteration[100/391] Loss: 0.3246 Acc:88.56%
Training: Epoch[018/190] Iteration[150/391] Loss: 0.3207 Acc:88.70%
Training: Epoch[018/190] Iteration[200/391] Loss: 0.3302 Acc:88.44%
Training: Epoch[018/190] Iteration[250/391] Loss: 0.3296 Acc:88.49%
Training: Epoch[018/190] Iteration[300/391] Loss: 0.3313 Acc:88.42%
Training: Epoch[018/190] Iteration[350/391] Loss: 0.3322 Acc:88.34%
Epoch[018/190] Train Acc: 88.30% Valid Acc:86.17% Train loss:0.3350 Valid loss:0.4122 LR:0.1
Training: Epoch[019/190] Iteration[050/391] Loss: 0.3150 Acc:89.05%
Training: Epoch[019/190] Iteration[100/391] Loss: 0.3259 Acc:88.81%
Training: Epoch[019/190] Iteration[150/391] Loss: 0.3254 Acc:88.87%
Training: Epoch[019/190] Iteration[200/391] Loss: 0.3283 Acc:88.73%
Training: Epoch[019/190] Iteration[250/391] Loss: 0.3280 Acc:88.67%
Training: Epoch[019/190] Iteration[300/391] Loss: 0.3336 Acc:88.45%
Training: Epoch[019/190] Iteration[350/391] Loss: 0.3319 Acc:88.51%
Epoch[019/190] Train Acc: 88.62% Valid Acc:82.63% Train loss:0.3293 Valid loss:0.5421 LR:0.1
Training: Epoch[020/190] Iteration[050/391] Loss: 0.2993 Acc:90.14%
Training: Epoch[020/190] Iteration[100/391] Loss: 0.3131 Acc:89.41%
Training: Epoch[020/190] Iteration[150/391] Loss: 0.3104 Acc:89.44%
Training: Epoch[020/190] Iteration[200/391] Loss: 0.3139 Acc:89.25%
Training: Epoch[020/190] Iteration[250/391] Loss: 0.3173 Acc:89.12%
Training: Epoch[020/190] Iteration[300/391] Loss: 0.3212 Acc:88.98%
Training: Epoch[020/190] Iteration[350/391] Loss: 0.3185 Acc:89.06%
Epoch[020/190] Train Acc: 88.97% Valid Acc:81.09% Train loss:0.3203 Valid loss:0.5713 LR:0.1
Training: Epoch[021/190] Iteration[050/391] Loss: 0.2944 Acc:89.97%
Training: Epoch[021/190] Iteration[100/391] Loss: 0.2979 Acc:89.73%
Training: Epoch[021/190] Iteration[150/391] Loss: 0.3073 Acc:89.33%
Training: Epoch[021/190] Iteration[200/391] Loss: 0.3101 Acc:89.16%
Training: Epoch[021/190] Iteration[250/391] Loss: 0.3090 Acc:89.24%
Training: Epoch[021/190] Iteration[300/391] Loss: 0.3114 Acc:89.17%
Training: Epoch[021/190] Iteration[350/391] Loss: 0.3095 Acc:89.21%
Epoch[021/190] Train Acc: 89.20% Valid Acc:81.45% Train loss:0.3113 Valid loss:0.5926 LR:0.1
Training: Epoch[022/190] Iteration[050/391] Loss: 0.2874 Acc:89.84%
Training: Epoch[022/190] Iteration[100/391] Loss: 0.2870 Acc:89.77%
Training: Epoch[022/190] Iteration[150/391] Loss: 0.2974 Acc:89.47%
Training: Epoch[022/190] Iteration[200/391] Loss: 0.2989 Acc:89.60%
Training: Epoch[022/190] Iteration[250/391] Loss: 0.3005 Acc:89.57%
Training: Epoch[022/190] Iteration[300/391] Loss: 0.3038 Acc:89.47%
Training: Epoch[022/190] Iteration[350/391] Loss: 0.3062 Acc:89.42%
Epoch[022/190] Train Acc: 89.34% Valid Acc:84.76% Train loss:0.3074 Valid loss:0.4501 LR:0.1
Training: Epoch[023/190] Iteration[050/391] Loss: 0.2785 Acc:90.23%
Training: Epoch[023/190] Iteration[100/391] Loss: 0.2772 Acc:90.17%
Training: Epoch[023/190] Iteration[150/391] Loss: 0.2834 Acc:89.93%
Training: Epoch[023/190] Iteration[200/391] Loss: 0.2951 Acc:89.51%
Training: Epoch[023/190] Iteration[250/391] Loss: 0.2983 Acc:89.52%
Training: Epoch[023/190] Iteration[300/391] Loss: 0.3012 Acc:89.43%
Training: Epoch[023/190] Iteration[350/391] Loss: 0.3010 Acc:89.47%
Epoch[023/190] Train Acc: 89.41% Valid Acc:84.23% Train loss:0.3028 Valid loss:0.4919 LR:0.1
Training: Epoch[024/190] Iteration[050/391] Loss: 0.2594 Acc:90.77%
Training: Epoch[024/190] Iteration[100/391] Loss: 0.2658 Acc:90.66%
Training: Epoch[024/190] Iteration[150/391] Loss: 0.2790 Acc:90.27%
Training: Epoch[024/190] Iteration[200/391] Loss: 0.2802 Acc:90.25%
Training: Epoch[024/190] Iteration[250/391] Loss: 0.2805 Acc:90.23%
Training: Epoch[024/190] Iteration[300/391] Loss: 0.2882 Acc:89.95%
Training: Epoch[024/190] Iteration[350/391] Loss: 0.2897 Acc:89.94%
Epoch[024/190] Train Acc: 89.95% Valid Acc:86.83% Train loss:0.2891 Valid loss:0.4138 LR:0.1
Training: Epoch[025/190] Iteration[050/391] Loss: 0.2777 Acc:90.17%
Training: Epoch[025/190] Iteration[100/391] Loss: 0.2799 Acc:90.28%
Training: Epoch[025/190] Iteration[150/391] Loss: 0.2828 Acc:90.29%
Training: Epoch[025/190] Iteration[200/391] Loss: 0.2844 Acc:90.32%
Training: Epoch[025/190] Iteration[250/391] Loss: 0.2845 Acc:90.31%
Training: Epoch[025/190] Iteration[300/391] Loss: 0.2892 Acc:90.16%
Training: Epoch[025/190] Iteration[350/391] Loss: 0.2843 Acc:90.27%
Epoch[025/190] Train Acc: 90.10% Valid Acc:80.77% Train loss:0.2881 Valid loss:0.6723 LR:0.1
Training: Epoch[026/190] Iteration[050/391] Loss: 0.2790 Acc:89.72%
Training: Epoch[026/190] Iteration[100/391] Loss: 0.2758 Acc:90.30%
Training: Epoch[026/190] Iteration[150/391] Loss: 0.2738 Acc:90.45%
Training: Epoch[026/190] Iteration[200/391] Loss: 0.2754 Acc:90.46%
Training: Epoch[026/190] Iteration[250/391] Loss: 0.2774 Acc:90.37%
Training: Epoch[026/190] Iteration[300/391] Loss: 0.2753 Acc:90.50%
Training: Epoch[026/190] Iteration[350/391] Loss: 0.2738 Acc:90.47%
Epoch[026/190] Train Acc: 90.46% Valid Acc:84.60% Train loss:0.2750 Valid loss:0.5004 LR:0.1
Training: Epoch[027/190] Iteration[050/391] Loss: 0.2508 Acc:91.08%
Training: Epoch[027/190] Iteration[100/391] Loss: 0.2629 Acc:90.73%
Training: Epoch[027/190] Iteration[150/391] Loss: 0.2662 Acc:90.78%
Training: Epoch[027/190] Iteration[200/391] Loss: 0.2718 Acc:90.56%
Training: Epoch[027/190] Iteration[250/391] Loss: 0.2725 Acc:90.55%
Training: Epoch[027/190] Iteration[300/391] Loss: 0.2738 Acc:90.49%
Training: Epoch[027/190] Iteration[350/391] Loss: 0.2763 Acc:90.37%
Epoch[027/190] Train Acc: 90.27% Valid Acc:85.78% Train loss:0.2784 Valid loss:0.4539 LR:0.1
Training: Epoch[028/190] Iteration[050/391] Loss: 0.2622 Acc:90.52%
Training: Epoch[028/190] Iteration[100/391] Loss: 0.2651 Acc:90.47%
Training: Epoch[028/190] Iteration[150/391] Loss: 0.2697 Acc:90.44%
Training: Epoch[028/190] Iteration[200/391] Loss: 0.2666 Acc:90.55%
Training: Epoch[028/190] Iteration[250/391] Loss: 0.2651 Acc:90.61%
Training: Epoch[028/190] Iteration[300/391] Loss: 0.2668 Acc:90.61%
Training: Epoch[028/190] Iteration[350/391] Loss: 0.2668 Acc:90.62%
Epoch[028/190] Train Acc: 90.66% Valid Acc:84.99% Train loss:0.2666 Valid loss:0.4950 LR:0.1
Training: Epoch[029/190] Iteration[050/391] Loss: 0.2393 Acc:91.58%
Training: Epoch[029/190] Iteration[100/391] Loss: 0.2458 Acc:91.30%
Training: Epoch[029/190] Iteration[150/391] Loss: 0.2543 Acc:90.98%
Training: Epoch[029/190] Iteration[200/391] Loss: 0.2576 Acc:90.83%
Training: Epoch[029/190] Iteration[250/391] Loss: 0.2616 Acc:90.75%
Training: Epoch[029/190] Iteration[300/391] Loss: 0.2621 Acc:90.81%
Training: Epoch[029/190] Iteration[350/391] Loss: 0.2626 Acc:90.84%
Epoch[029/190] Train Acc: 90.73% Valid Acc:85.11% Train loss:0.2644 Valid loss:0.5046 LR:0.1
Training: Epoch[030/190] Iteration[050/391] Loss: 0.2356 Acc:91.88%
Training: Epoch[030/190] Iteration[100/391] Loss: 0.2400 Acc:91.80%
Training: Epoch[030/190] Iteration[150/391] Loss: 0.2476 Acc:91.47%
Training: Epoch[030/190] Iteration[200/391] Loss: 0.2509 Acc:91.23%
Training: Epoch[030/190] Iteration[250/391] Loss: 0.2551 Acc:91.11%
Training: Epoch[030/190] Iteration[300/391] Loss: 0.2567 Acc:91.04%
Training: Epoch[030/190] Iteration[350/391] Loss: 0.2575 Acc:91.02%
Epoch[030/190] Train Acc: 91.07% Valid Acc:83.31% Train loss:0.2566 Valid loss:0.5339 LR:0.1
Training: Epoch[031/190] Iteration[050/391] Loss: 0.2463 Acc:91.50%
Training: Epoch[031/190] Iteration[100/391] Loss: 0.2458 Acc:91.52%
Training: Epoch[031/190] Iteration[150/391] Loss: 0.2568 Acc:90.99%
Training: Epoch[031/190] Iteration[200/391] Loss: 0.2616 Acc:90.80%
Training: Epoch[031/190] Iteration[250/391] Loss: 0.2590 Acc:90.88%
Training: Epoch[031/190] Iteration[300/391] Loss: 0.2595 Acc:90.91%
Training: Epoch[031/190] Iteration[350/391] Loss: 0.2590 Acc:90.93%
Epoch[031/190] Train Acc: 90.81% Valid Acc:85.30% Train loss:0.2612 Valid loss:0.4726 LR:0.1
Training: Epoch[032/190] Iteration[050/391] Loss: 0.2407 Acc:91.61%
Training: Epoch[032/190] Iteration[100/391] Loss: 0.2425 Acc:91.36%
Training: Epoch[032/190] Iteration[150/391] Loss: 0.2453 Acc:91.24%
Training: Epoch[032/190] Iteration[200/391] Loss: 0.2457 Acc:91.27%
Training: Epoch[032/190] Iteration[250/391] Loss: 0.2482 Acc:91.28%
Training: Epoch[032/190] Iteration[300/391] Loss: 0.2501 Acc:91.24%
Training: Epoch[032/190] Iteration[350/391] Loss: 0.2508 Acc:91.25%
Epoch[032/190] Train Acc: 91.14% Valid Acc:87.25% Train loss:0.2541 Valid loss:0.4081 LR:0.1
Training: Epoch[033/190] Iteration[050/391] Loss: 0.2405 Acc:91.86%
Training: Epoch[033/190] Iteration[100/391] Loss: 0.2357 Acc:91.95%
Training: Epoch[033/190] Iteration[150/391] Loss: 0.2427 Acc:91.61%
Training: Epoch[033/190] Iteration[200/391] Loss: 0.2449 Acc:91.53%
Training: Epoch[033/190] Iteration[250/391] Loss: 0.2463 Acc:91.47%
Training: Epoch[033/190] Iteration[300/391] Loss: 0.2455 Acc:91.47%
Training: Epoch[033/190] Iteration[350/391] Loss: 0.2501 Acc:91.29%
Epoch[033/190] Train Acc: 91.15% Valid Acc:83.91% Train loss:0.2545 Valid loss:0.5216 LR:0.1
Training: Epoch[034/190] Iteration[050/391] Loss: 0.2470 Acc:91.08%
Training: Epoch[034/190] Iteration[100/391] Loss: 0.2447 Acc:91.39%
Training: Epoch[034/190] Iteration[150/391] Loss: 0.2426 Acc:91.41%
Training: Epoch[034/190] Iteration[200/391] Loss: 0.2408 Acc:91.40%
Training: Epoch[034/190] Iteration[250/391] Loss: 0.2400 Acc:91.43%
Training: Epoch[034/190] Iteration[300/391] Loss: 0.2467 Acc:91.24%
Training: Epoch[034/190] Iteration[350/391] Loss: 0.2475 Acc:91.23%
Epoch[034/190] Train Acc: 91.21% Valid Acc:84.24% Train loss:0.2489 Valid loss:0.5484 LR:0.1
Training: Epoch[035/190] Iteration[050/391] Loss: 0.2447 Acc:91.30%
Training: Epoch[035/190] Iteration[100/391] Loss: 0.2364 Acc:91.67%
Training: Epoch[035/190] Iteration[150/391] Loss: 0.2351 Acc:91.77%
Training: Epoch[035/190] Iteration[200/391] Loss: 0.2360 Acc:91.77%
Training: Epoch[035/190] Iteration[250/391] Loss: 0.2374 Acc:91.67%
Training: Epoch[035/190] Iteration[300/391] Loss: 0.2424 Acc:91.53%
Training: Epoch[035/190] Iteration[350/391] Loss: 0.2407 Acc:91.54%
Epoch[035/190] Train Acc: 91.48% Valid Acc:85.79% Train loss:0.2424 Valid loss:0.4564 LR:0.1
Training: Epoch[036/190] Iteration[050/391] Loss: 0.2206 Acc:92.48%
Training: Epoch[036/190] Iteration[100/391] Loss: 0.2248 Acc:92.22%
Training: Epoch[036/190] Iteration[150/391] Loss: 0.2285 Acc:92.13%
Training: Epoch[036/190] Iteration[200/391] Loss: 0.2298 Acc:92.03%
Training: Epoch[036/190] Iteration[250/391] Loss: 0.2359 Acc:91.80%
Training: Epoch[036/190] Iteration[300/391] Loss: 0.2405 Acc:91.63%
Training: Epoch[036/190] Iteration[350/391] Loss: 0.2401 Acc:91.64%
Epoch[036/190] Train Acc: 91.61% Valid Acc:85.64% Train loss:0.2402 Valid loss:0.5035 LR:0.1
Training: Epoch[037/190] Iteration[050/391] Loss: 0.2222 Acc:92.33%
Training: Epoch[037/190] Iteration[100/391] Loss: 0.2316 Acc:91.92%
Training: Epoch[037/190] Iteration[150/391] Loss: 0.2347 Acc:91.84%
Training: Epoch[037/190] Iteration[200/391] Loss: 0.2395 Acc:91.69%
Training: Epoch[037/190] Iteration[250/391] Loss: 0.2448 Acc:91.40%
Training: Epoch[037/190] Iteration[300/391] Loss: 0.2432 Acc:91.43%
Training: Epoch[037/190] Iteration[350/391] Loss: 0.2435 Acc:91.45%
Epoch[037/190] Train Acc: 91.40% Valid Acc:84.69% Train loss:0.2444 Valid loss:0.4801 LR:0.1
Training: Epoch[038/190] Iteration[050/391] Loss: 0.2258 Acc:91.97%
Training: Epoch[038/190] Iteration[100/391] Loss: 0.2255 Acc:91.94%
Training: Epoch[038/190] Iteration[150/391] Loss: 0.2330 Acc:91.66%
Training: Epoch[038/190] Iteration[200/391] Loss: 0.2315 Acc:91.77%
Training: Epoch[038/190] Iteration[250/391] Loss: 0.2303 Acc:91.84%
Training: Epoch[038/190] Iteration[300/391] Loss: 0.2334 Acc:91.78%
Training: Epoch[038/190] Iteration[350/391] Loss: 0.2357 Acc:91.70%
Epoch[038/190] Train Acc: 91.66% Valid Acc:87.18% Train loss:0.2368 Valid loss:0.4009 LR:0.1
Training: Epoch[039/190] Iteration[050/391] Loss: 0.2204 Acc:92.34%
Training: Epoch[039/190] Iteration[100/391] Loss: 0.2152 Acc:92.48%
Training: Epoch[039/190] Iteration[150/391] Loss: 0.2251 Acc:92.11%
Training: Epoch[039/190] Iteration[200/391] Loss: 0.2315 Acc:91.90%
Training: Epoch[039/190] Iteration[250/391] Loss: 0.2315 Acc:91.87%
Training: Epoch[039/190] Iteration[300/391] Loss: 0.2368 Acc:91.67%
Training: Epoch[039/190] Iteration[350/391] Loss: 0.2344 Acc:91.77%
Epoch[039/190] Train Acc: 91.83% Valid Acc:85.21% Train loss:0.2326 Valid loss:0.4941 LR:0.1
Training: Epoch[040/190] Iteration[050/391] Loss: 0.2017 Acc:92.77%
Training: Epoch[040/190] Iteration[100/391] Loss: 0.2181 Acc:92.28%
Training: Epoch[040/190] Iteration[150/391] Loss: 0.2150 Acc:92.41%
Training: Epoch[040/190] Iteration[200/391] Loss: 0.2177 Acc:92.36%
Training: Epoch[040/190] Iteration[250/391] Loss: 0.2194 Acc:92.36%
Training: Epoch[040/190] Iteration[300/391] Loss: 0.2235 Acc:92.27%
Training: Epoch[040/190] Iteration[350/391] Loss: 0.2246 Acc:92.26%
Epoch[040/190] Train Acc: 92.21% Valid Acc:88.15% Train loss:0.2264 Valid loss:0.3847 LR:0.1
Training: Epoch[041/190] Iteration[050/391] Loss: 0.1991 Acc:93.12%
Training: Epoch[041/190] Iteration[100/391] Loss: 0.2115 Acc:92.54%
Training: Epoch[041/190] Iteration[150/391] Loss: 0.2141 Acc:92.47%
Training: Epoch[041/190] Iteration[200/391] Loss: 0.2172 Acc:92.34%
Training: Epoch[041/190] Iteration[250/391] Loss: 0.2171 Acc:92.31%
Training: Epoch[041/190] Iteration[300/391] Loss: 0.2220 Acc:92.14%
Training: Epoch[041/190] Iteration[350/391] Loss: 0.2261 Acc:91.99%
Epoch[041/190] Train Acc: 91.90% Valid Acc:86.13% Train loss:0.2279 Valid loss:0.4164 LR:0.1
Training: Epoch[042/190] Iteration[050/391] Loss: 0.2273 Acc:92.09%
Training: Epoch[042/190] Iteration[100/391] Loss: 0.2134 Acc:92.62%
Training: Epoch[042/190] Iteration[150/391] Loss: 0.2169 Acc:92.43%
Training: Epoch[042/190] Iteration[200/391] Loss: 0.2172 Acc:92.33%
Training: Epoch[042/190] Iteration[250/391] Loss: 0.2218 Acc:92.26%
Training: Epoch[042/190] Iteration[300/391] Loss: 0.2239 Acc:92.18%
Training: Epoch[042/190] Iteration[350/391] Loss: 0.2255 Acc:92.19%
Epoch[042/190] Train Acc: 92.06% Valid Acc:86.38% Train loss:0.2287 Valid loss:0.4186 LR:0.1
Training: Epoch[043/190] Iteration[050/391] Loss: 0.2264 Acc:92.17%
Training: Epoch[043/190] Iteration[100/391] Loss: 0.2069 Acc:92.70%
Training: Epoch[043/190] Iteration[150/391] Loss: 0.2120 Acc:92.47%
Training: Epoch[043/190] Iteration[200/391] Loss: 0.2150 Acc:92.33%
Training: Epoch[043/190] Iteration[250/391] Loss: 0.2175 Acc:92.27%
Training: Epoch[043/190] Iteration[300/391] Loss: 0.2175 Acc:92.35%
Training: Epoch[043/190] Iteration[350/391] Loss: 0.2198 Acc:92.27%
Epoch[043/190] Train Acc: 92.24% Valid Acc:84.87% Train loss:0.2207 Valid loss:0.5048 LR:0.1
Training: Epoch[044/190] Iteration[050/391] Loss: 0.2036 Acc:92.77%
Training: Epoch[044/190] Iteration[100/391] Loss: 0.2062 Acc:92.64%
Training: Epoch[044/190] Iteration[150/391] Loss: 0.2049 Acc:92.72%
Training: Epoch[044/190] Iteration[200/391] Loss: 0.2122 Acc:92.57%
Training: Epoch[044/190] Iteration[250/391] Loss: 0.2144 Acc:92.41%
Training: Epoch[044/190] Iteration[300/391] Loss: 0.2170 Acc:92.34%
Training: Epoch[044/190] Iteration[350/391] Loss: 0.2174 Acc:92.37%
Epoch[044/190] Train Acc: 92.34% Valid Acc:87.20% Train loss:0.2187 Valid loss:0.3934 LR:0.1
Training: Epoch[045/190] Iteration[050/391] Loss: 0.1958 Acc:92.84%
Training: Epoch[045/190] Iteration[100/391] Loss: 0.2011 Acc:92.86%
Training: Epoch[045/190] Iteration[150/391] Loss: 0.1974 Acc:93.04%
Training: Epoch[045/190] Iteration[200/391] Loss: 0.2009 Acc:92.94%
Training: Epoch[045/190] Iteration[250/391] Loss: 0.2092 Acc:92.67%
Training: Epoch[045/190] Iteration[300/391] Loss: 0.2121 Acc:92.54%
Training: Epoch[045/190] Iteration[350/391] Loss: 0.2140 Acc:92.49%
Epoch[045/190] Train Acc: 92.48% Valid Acc:87.27% Train loss:0.2148 Valid loss:0.4343 LR:0.1
Training: Epoch[046/190] Iteration[050/391] Loss: 0.2103 Acc:92.44%
Training: Epoch[046/190] Iteration[100/391] Loss: 0.2088 Acc:92.51%
Training: Epoch[046/190] Iteration[150/391] Loss: 0.2055 Acc:92.76%
Training: Epoch[046/190] Iteration[200/391] Loss: 0.2102 Acc:92.60%
Training: Epoch[046/190] Iteration[250/391] Loss: 0.2130 Acc:92.50%
Training: Epoch[046/190] Iteration[300/391] Loss: 0.2158 Acc:92.45%
Training: Epoch[046/190] Iteration[350/391] Loss: 0.2162 Acc:92.48%
Epoch[046/190] Train Acc: 92.49% Valid Acc:87.41% Train loss:0.2159 Valid loss:0.4044 LR:0.1
Training: Epoch[047/190] Iteration[050/391] Loss: 0.1953 Acc:93.33%
Training: Epoch[047/190] Iteration[100/391] Loss: 0.1977 Acc:93.12%
Training: Epoch[047/190] Iteration[150/391] Loss: 0.2085 Acc:92.74%
Training: Epoch[047/190] Iteration[200/391] Loss: 0.2083 Acc:92.76%
Training: Epoch[047/190] Iteration[250/391] Loss: 0.2100 Acc:92.71%
Training: Epoch[047/190] Iteration[300/391] Loss: 0.2123 Acc:92.61%
Training: Epoch[047/190] Iteration[350/391] Loss: 0.2140 Acc:92.60%
Epoch[047/190] Train Acc: 92.59% Valid Acc:85.05% Train loss:0.2147 Valid loss:0.4895 LR:0.1
Training: Epoch[048/190] Iteration[050/391] Loss: 0.2016 Acc:92.89%
Training: Epoch[048/190] Iteration[100/391] Loss: 0.1945 Acc:93.08%
Training: Epoch[048/190] Iteration[150/391] Loss: 0.1981 Acc:93.06%
Training: Epoch[048/190] Iteration[200/391] Loss: 0.2028 Acc:93.01%
Training: Epoch[048/190] Iteration[250/391] Loss: 0.2070 Acc:92.84%
Training: Epoch[048/190] Iteration[300/391] Loss: 0.2082 Acc:92.82%
Training: Epoch[048/190] Iteration[350/391] Loss: 0.2080 Acc:92.80%
Epoch[048/190] Train Acc: 92.72% Valid Acc:84.59% Train loss:0.2093 Valid loss:0.5532 LR:0.1
Training: Epoch[049/190] Iteration[050/391] Loss: 0.1950 Acc:93.23%
Training: Epoch[049/190] Iteration[100/391] Loss: 0.1980 Acc:93.07%
Training: Epoch[049/190] Iteration[150/391] Loss: 0.2004 Acc:93.06%
Training: Epoch[049/190] Iteration[200/391] Loss: 0.2039 Acc:92.98%
Training: Epoch[049/190] Iteration[250/391] Loss: 0.2069 Acc:92.86%
Training: Epoch[049/190] Iteration[300/391] Loss: 0.2056 Acc:92.89%
Training: Epoch[049/190] Iteration[350/391] Loss: 0.2093 Acc:92.79%
Epoch[049/190] Train Acc: 92.75% Valid Acc:87.07% Train loss:0.2105 Valid loss:0.4089 LR:0.1
Training: Epoch[050/190] Iteration[050/391] Loss: 0.1851 Acc:93.30%
Training: Epoch[050/190] Iteration[100/391] Loss: 0.1886 Acc:93.22%
Training: Epoch[050/190] Iteration[150/391] Loss: 0.1886 Acc:93.23%
Training: Epoch[050/190] Iteration[200/391] Loss: 0.1952 Acc:93.14%
Training: Epoch[050/190] Iteration[250/391] Loss: 0.1979 Acc:93.05%
Training: Epoch[050/190] Iteration[300/391] Loss: 0.2017 Acc:92.92%
Training: Epoch[050/190] Iteration[350/391] Loss: 0.2033 Acc:92.84%
Epoch[050/190] Train Acc: 92.75% Valid Acc:86.19% Train loss:0.2053 Valid loss:0.4582 LR:0.1
Training: Epoch[051/190] Iteration[050/391] Loss: 0.1916 Acc:93.33%
Training: Epoch[051/190] Iteration[100/391] Loss: 0.2014 Acc:92.97%
Training: Epoch[051/190] Iteration[150/391] Loss: 0.1989 Acc:92.94%
Training: Epoch[051/190] Iteration[200/391] Loss: 0.2010 Acc:92.91%
Training: Epoch[051/190] Iteration[250/391] Loss: 0.2026 Acc:92.81%
Training: Epoch[051/190] Iteration[300/391] Loss: 0.2056 Acc:92.70%
Training: Epoch[051/190] Iteration[350/391] Loss: 0.2060 Acc:92.70%
Epoch[051/190] Train Acc: 92.64% Valid Acc:84.56% Train loss:0.2089 Valid loss:0.5015 LR:0.1
Training: Epoch[052/190] Iteration[050/391] Loss: 0.2018 Acc:93.16%
Training: Epoch[052/190] Iteration[100/391] Loss: 0.2011 Acc:93.02%
Training: Epoch[052/190] Iteration[150/391] Loss: 0.2001 Acc:93.03%
Training: Epoch[052/190] Iteration[200/391] Loss: 0.1993 Acc:93.08%
Training: Epoch[052/190] Iteration[250/391] Loss: 0.2002 Acc:93.02%
Training: Epoch[052/190] Iteration[300/391] Loss: 0.2001 Acc:93.05%
Training: Epoch[052/190] Iteration[350/391] Loss: 0.2009 Acc:93.01%
Epoch[052/190] Train Acc: 92.91% Valid Acc:86.97% Train loss:0.2031 Valid loss:0.4113 LR:0.1
Training: Epoch[053/190] Iteration[050/391] Loss: 0.1988 Acc:92.95%
Training: Epoch[053/190] Iteration[100/391] Loss: 0.1958 Acc:93.22%
Training: Epoch[053/190] Iteration[150/391] Loss: 0.1930 Acc:93.24%
Training: Epoch[053/190] Iteration[200/391] Loss: 0.1929 Acc:93.21%
Training: Epoch[053/190] Iteration[250/391] Loss: 0.1982 Acc:93.03%
Training: Epoch[053/190] Iteration[300/391] Loss: 0.2011 Acc:92.94%
Training: Epoch[053/190] Iteration[350/391] Loss: 0.2015 Acc:92.89%
Epoch[053/190] Train Acc: 92.85% Valid Acc:87.26% Train loss:0.2027 Valid loss:0.4315 LR:0.1
Training: Epoch[054/190] Iteration[050/391] Loss: 0.1789 Acc:93.61%
Training: Epoch[054/190] Iteration[100/391] Loss: 0.1814 Acc:93.52%
Training: Epoch[054/190] Iteration[150/391] Loss: 0.1876 Acc:93.32%
Training: Epoch[054/190] Iteration[200/391] Loss: 0.1909 Acc:93.16%
Training: Epoch[054/190] Iteration[250/391] Loss: 0.1949 Acc:92.98%
Training: Epoch[054/190] Iteration[300/391] Loss: 0.2008 Acc:92.82%
Training: Epoch[054/190] Iteration[350/391] Loss: 0.1993 Acc:92.85%
Epoch[054/190] Train Acc: 92.82% Valid Acc:87.62% Train loss:0.2003 Valid loss:0.4118 LR:0.1
Training: Epoch[055/190] Iteration[050/391] Loss: 0.1838 Acc:93.55%
Training: Epoch[055/190] Iteration[100/391] Loss: 0.1924 Acc:93.35%
Training: Epoch[055/190] Iteration[150/391] Loss: 0.1941 Acc:93.22%
Training: Epoch[055/190] Iteration[200/391] Loss: 0.1992 Acc:93.08%
Training: Epoch[055/190] Iteration[250/391] Loss: 0.1953 Acc:93.18%
Training: Epoch[055/190] Iteration[300/391] Loss: 0.1962 Acc:93.16%
Training: Epoch[055/190] Iteration[350/391] Loss: 0.1974 Acc:93.12%
Epoch[055/190] Train Acc: 93.07% Valid Acc:87.48% Train loss:0.1977 Valid loss:0.4163 LR:0.1
Training: Epoch[056/190] Iteration[050/391] Loss: 0.1716 Acc:94.17%
Training: Epoch[056/190] Iteration[100/391] Loss: 0.1721 Acc:94.02%
Training: Epoch[056/190] Iteration[150/391] Loss: 0.1856 Acc:93.44%
Training: Epoch[056/190] Iteration[200/391] Loss: 0.1897 Acc:93.30%
Training: Epoch[056/190] Iteration[250/391] Loss: 0.1917 Acc:93.26%
Training: Epoch[056/190] Iteration[300/391] Loss: 0.1953 Acc:93.08%
Training: Epoch[056/190] Iteration[350/391] Loss: 0.1958 Acc:93.07%
Epoch[056/190] Train Acc: 93.04% Valid Acc:87.22% Train loss:0.1974 Valid loss:0.4324 LR:0.1
Training: Epoch[057/190] Iteration[050/391] Loss: 0.1818 Acc:93.77%
Training: Epoch[057/190] Iteration[100/391] Loss: 0.1837 Acc:93.54%
Training: Epoch[057/190] Iteration[150/391] Loss: 0.1875 Acc:93.45%
Training: Epoch[057/190] Iteration[200/391] Loss: 0.1890 Acc:93.39%
Training: Epoch[057/190] Iteration[250/391] Loss: 0.1935 Acc:93.29%
Training: Epoch[057/190] Iteration[300/391] Loss: 0.1943 Acc:93.22%
Training: Epoch[057/190] Iteration[350/391] Loss: 0.1960 Acc:93.16%
Epoch[057/190] Train Acc: 93.10% Valid Acc:86.63% Train loss:0.1974 Valid loss:0.4497 LR:0.1
Training: Epoch[058/190] Iteration[050/391] Loss: 0.1884 Acc:93.36%
Training: Epoch[058/190] Iteration[100/391] Loss: 0.1782 Acc:93.78%
Training: Epoch[058/190] Iteration[150/391] Loss: 0.1819 Acc:93.76%
Training: Epoch[058/190] Iteration[200/391] Loss: 0.1878 Acc:93.54%
Training: Epoch[058/190] Iteration[250/391] Loss: 0.1896 Acc:93.41%
Training: Epoch[058/190] Iteration[300/391] Loss: 0.1918 Acc:93.33%
Training: Epoch[058/190] Iteration[350/391] Loss: 0.1943 Acc:93.21%
Epoch[058/190] Train Acc: 93.15% Valid Acc:88.42% Train loss:0.1956 Valid loss:0.3768 LR:0.1
Training: Epoch[059/190] Iteration[050/391] Loss: 0.1810 Acc:93.55%
Training: Epoch[059/190] Iteration[100/391] Loss: 0.1816 Acc:93.65%
Training: Epoch[059/190] Iteration[150/391] Loss: 0.1806 Acc:93.69%
Training: Epoch[059/190] Iteration[200/391] Loss: 0.1829 Acc:93.54%
Training: Epoch[059/190] Iteration[250/391] Loss: 0.1853 Acc:93.40%
Training: Epoch[059/190] Iteration[300/391] Loss: 0.1894 Acc:93.27%
Training: Epoch[059/190] Iteration[350/391] Loss: 0.1928 Acc:93.14%
Epoch[059/190] Train Acc: 93.14% Valid Acc:87.07% Train loss:0.1935 Valid loss:0.4303 LR:0.1
Training: Epoch[060/190] Iteration[050/391] Loss: 0.1745 Acc:94.19%
Training: Epoch[060/190] Iteration[100/391] Loss: 0.1682 Acc:94.26%
Training: Epoch[060/190] Iteration[150/391] Loss: 0.1748 Acc:93.98%
Training: Epoch[060/190] Iteration[200/391] Loss: 0.1823 Acc:93.74%
Training: Epoch[060/190] Iteration[250/391] Loss: 0.1866 Acc:93.50%
Training: Epoch[060/190] Iteration[300/391] Loss: 0.1881 Acc:93.45%
Training: Epoch[060/190] Iteration[350/391] Loss: 0.1890 Acc:93.42%
Epoch[060/190] Train Acc: 93.39% Valid Acc:86.77% Train loss:0.1898 Valid loss:0.4560 LR:0.1
Training: Epoch[061/190] Iteration[050/391] Loss: 0.1724 Acc:94.00%
Training: Epoch[061/190] Iteration[100/391] Loss: 0.1790 Acc:93.80%
Training: Epoch[061/190] Iteration[150/391] Loss: 0.1812 Acc:93.72%
Training: Epoch[061/190] Iteration[200/391] Loss: 0.1884 Acc:93.37%
Training: Epoch[061/190] Iteration[250/391] Loss: 0.1859 Acc:93.43%
Training: Epoch[061/190] Iteration[300/391] Loss: 0.1903 Acc:93.30%
Training: Epoch[061/190] Iteration[350/391] Loss: 0.1915 Acc:93.26%
Epoch[061/190] Train Acc: 93.27% Valid Acc:87.11% Train loss:0.1912 Valid loss:0.4340 LR:0.1
Training: Epoch[062/190] Iteration[050/391] Loss: 0.1748 Acc:93.52%
Training: Epoch[062/190] Iteration[100/391] Loss: 0.1717 Acc:93.84%
Training: Epoch[062/190] Iteration[150/391] Loss: 0.1757 Acc:93.80%
Training: Epoch[062/190] Iteration[200/391] Loss: 0.1770 Acc:93.76%
Training: Epoch[062/190] Iteration[250/391] Loss: 0.1827 Acc:93.57%
Training: Epoch[062/190] Iteration[300/391] Loss: 0.1863 Acc:93.45%
Training: Epoch[062/190] Iteration[350/391] Loss: 0.1916 Acc:93.24%
Epoch[062/190] Train Acc: 93.24% Valid Acc:88.56% Train loss:0.1911 Valid loss:0.3787 LR:0.1
Training: Epoch[063/190] Iteration[050/391] Loss: 0.1779 Acc:93.59%
Training: Epoch[063/190] Iteration[100/391] Loss: 0.1827 Acc:93.56%
Training: Epoch[063/190] Iteration[150/391] Loss: 0.1847 Acc:93.62%
Training: Epoch[063/190] Iteration[200/391] Loss: 0.1865 Acc:93.56%
Training: Epoch[063/190] Iteration[250/391] Loss: 0.1886 Acc:93.54%
Training: Epoch[063/190] Iteration[300/391] Loss: 0.1895 Acc:93.43%
Training: Epoch[063/190] Iteration[350/391] Loss: 0.1916 Acc:93.38%
Epoch[063/190] Train Acc: 93.39% Valid Acc:85.64% Train loss:0.1918 Valid loss:0.4866 LR:0.1
Training: Epoch[064/190] Iteration[050/391] Loss: 0.1802 Acc:93.75%
Training: Epoch[064/190] Iteration[100/391] Loss: 0.1809 Acc:93.60%
Training: Epoch[064/190] Iteration[150/391] Loss: 0.1851 Acc:93.48%
Training: Epoch[064/190] Iteration[200/391] Loss: 0.1862 Acc:93.48%
Training: Epoch[064/190] Iteration[250/391] Loss: 0.1863 Acc:93.51%
Training: Epoch[064/190] Iteration[300/391] Loss: 0.1902 Acc:93.37%
Training: Epoch[064/190] Iteration[350/391] Loss: 0.1902 Acc:93.38%
Epoch[064/190] Train Acc: 93.31% Valid Acc:87.96% Train loss:0.1913 Valid loss:0.3999 LR:0.1
Training: Epoch[065/190] Iteration[050/391] Loss: 0.1706 Acc:94.02%
Training: Epoch[065/190] Iteration[100/391] Loss: 0.1736 Acc:93.94%
Training: Epoch[065/190] Iteration[150/391] Loss: 0.1761 Acc:93.81%
Training: Epoch[065/190] Iteration[200/391] Loss: 0.1765 Acc:93.79%
Training: Epoch[065/190] Iteration[250/391] Loss: 0.1786 Acc:93.77%
Training: Epoch[065/190] Iteration[300/391] Loss: 0.1837 Acc:93.58%
Training: Epoch[065/190] Iteration[350/391] Loss: 0.1841 Acc:93.50%
Epoch[065/190] Train Acc: 93.38% Valid Acc:85.84% Train loss:0.1871 Valid loss:0.4628 LR:0.1
Training: Epoch[066/190] Iteration[050/391] Loss: 0.1936 Acc:93.36%
Training: Epoch[066/190] Iteration[100/391] Loss: 0.1871 Acc:93.40%
Training: Epoch[066/190] Iteration[150/391] Loss: 0.1834 Acc:93.47%
Training: Epoch[066/190] Iteration[200/391] Loss: 0.1837 Acc:93.47%
Training: Epoch[066/190] Iteration[250/391] Loss: 0.1831 Acc:93.53%
Training: Epoch[066/190] Iteration[300/391] Loss: 0.1844 Acc:93.50%
Training: Epoch[066/190] Iteration[350/391] Loss: 0.1880 Acc:93.36%
Epoch[066/190] Train Acc: 93.35% Valid Acc:87.78% Train loss:0.1882 Valid loss:0.4110 LR:0.1
Training: Epoch[067/190] Iteration[050/391] Loss: 0.1677 Acc:94.02%
Training: Epoch[067/190] Iteration[100/391] Loss: 0.1722 Acc:93.93%
Training: Epoch[067/190] Iteration[150/391] Loss: 0.1774 Acc:93.71%
Training: Epoch[067/190] Iteration[200/391] Loss: 0.1789 Acc:93.56%
Training: Epoch[067/190] Iteration[250/391] Loss: 0.1804 Acc:93.53%
Training: Epoch[067/190] Iteration[300/391] Loss: 0.1805 Acc:93.51%
Training: Epoch[067/190] Iteration[350/391] Loss: 0.1816 Acc:93.53%
Epoch[067/190] Train Acc: 93.45% Valid Acc:87.87% Train loss:0.1833 Valid loss:0.3967 LR:0.1
Training: Epoch[068/190] Iteration[050/391] Loss: 0.1661 Acc:94.19%
Training: Epoch[068/190] Iteration[100/391] Loss: 0.1770 Acc:93.74%
Training: Epoch[068/190] Iteration[150/391] Loss: 0.1764 Acc:93.80%
Training: Epoch[068/190] Iteration[200/391] Loss: 0.1771 Acc:93.74%
Training: Epoch[068/190] Iteration[250/391] Loss: 0.1799 Acc:93.64%
Training: Epoch[068/190] Iteration[300/391] Loss: 0.1829 Acc:93.53%
Training: Epoch[068/190] Iteration[350/391] Loss: 0.1860 Acc:93.40%
Epoch[068/190] Train Acc: 93.36% Valid Acc:88.55% Train loss:0.1875 Valid loss:0.3792 LR:0.1
Training: Epoch[069/190] Iteration[050/391] Loss: 0.1597 Acc:94.66%
Training: Epoch[069/190] Iteration[100/391] Loss: 0.1665 Acc:94.34%
Training: Epoch[069/190] Iteration[150/391] Loss: 0.1737 Acc:94.08%
Training: Epoch[069/190] Iteration[200/391] Loss: 0.1758 Acc:93.89%
Training: Epoch[069/190] Iteration[250/391] Loss: 0.1765 Acc:93.87%
Training: Epoch[069/190] Iteration[300/391] Loss: 0.1789 Acc:93.84%
Training: Epoch[069/190] Iteration[350/391] Loss: 0.1792 Acc:93.82%
Epoch[069/190] Train Acc: 93.74% Valid Acc:86.28% Train loss:0.1798 Valid loss:0.4694 LR:0.1
Training: Epoch[070/190] Iteration[050/391] Loss: 0.1778 Acc:93.86%
Training: Epoch[070/190] Iteration[100/391] Loss: 0.1749 Acc:93.93%
Training: Epoch[070/190] Iteration[150/391] Loss: 0.1743 Acc:93.98%
Training: Epoch[070/190] Iteration[200/391] Loss: 0.1804 Acc:93.80%
Training: Epoch[070/190] Iteration[250/391] Loss: 0.1813 Acc:93.73%
Training: Epoch[070/190] Iteration[300/391] Loss: 0.1844 Acc:93.57%
Training: Epoch[070/190] Iteration[350/391] Loss: 0.1849 Acc:93.55%
Epoch[070/190] Train Acc: 93.44% Valid Acc:87.90% Train loss:0.1879 Valid loss:0.3824 LR:0.1
Training: Epoch[071/190] Iteration[050/391] Loss: 0.1573 Acc:94.50%
Training: Epoch[071/190] Iteration[100/391] Loss: 0.1597 Acc:94.29%
Training: Epoch[071/190] Iteration[150/391] Loss: 0.1674 Acc:94.03%
Training: Epoch[071/190] Iteration[200/391] Loss: 0.1706 Acc:93.91%
Training: Epoch[071/190] Iteration[250/391] Loss: 0.1755 Acc:93.76%
Training: Epoch[071/190] Iteration[300/391] Loss: 0.1779 Acc:93.67%
Training: Epoch[071/190] Iteration[350/391] Loss: 0.1792 Acc:93.68%
Epoch[071/190] Train Acc: 93.63% Valid Acc:86.55% Train loss:0.1810 Valid loss:0.4412 LR:0.1
Training: Epoch[072/190] Iteration[050/391] Loss: 0.1695 Acc:94.02%
Training: Epoch[072/190] Iteration[100/391] Loss: 0.1695 Acc:94.05%
Training: Epoch[072/190] Iteration[150/391] Loss: 0.1690 Acc:94.04%
Training: Epoch[072/190] Iteration[200/391] Loss: 0.1719 Acc:94.00%
Training: Epoch[072/190] Iteration[250/391] Loss: 0.1740 Acc:93.94%
Training: Epoch[072/190] Iteration[300/391] Loss: 0.1772 Acc:93.83%
Training: Epoch[072/190] Iteration[350/391] Loss: 0.1769 Acc:93.81%
Epoch[072/190] Train Acc: 93.82% Valid Acc:87.02% Train loss:0.1789 Valid loss:0.4302 LR:0.1
Training: Epoch[073/190] Iteration[050/391] Loss: 0.1653 Acc:93.97%
Training: Epoch[073/190] Iteration[100/391] Loss: 0.1742 Acc:93.78%
Training: Epoch[073/190] Iteration[150/391] Loss: 0.1761 Acc:93.73%
Training: Epoch[073/190] Iteration[200/391] Loss: 0.1785 Acc:93.64%
Training: Epoch[073/190] Iteration[250/391] Loss: 0.1799 Acc:93.67%
Training: Epoch[073/190] Iteration[300/391] Loss: 0.1845 Acc:93.52%
Training: Epoch[073/190] Iteration[350/391] Loss: 0.1860 Acc:93.49%
Epoch[073/190] Train Acc: 93.45% Valid Acc:88.62% Train loss:0.1866 Valid loss:0.3855 LR:0.1
Training: Epoch[074/190] Iteration[050/391] Loss: 0.1562 Acc:94.61%
Training: Epoch[074/190] Iteration[100/391] Loss: 0.1629 Acc:94.34%
Training: Epoch[074/190] Iteration[150/391] Loss: 0.1676 Acc:94.16%
Training: Epoch[074/190] Iteration[200/391] Loss: 0.1723 Acc:94.00%
Training: Epoch[074/190] Iteration[250/391] Loss: 0.1731 Acc:93.99%
Training: Epoch[074/190] Iteration[300/391] Loss: 0.1776 Acc:93.81%
Training: Epoch[074/190] Iteration[350/391] Loss: 0.1772 Acc:93.82%
Epoch[074/190] Train Acc: 93.78% Valid Acc:87.84% Train loss:0.1777 Valid loss:0.4124 LR:0.1
Training: Epoch[075/190] Iteration[050/391] Loss: 0.1619 Acc:94.42%
Training: Epoch[075/190] Iteration[100/391] Loss: 0.1646 Acc:94.41%
Training: Epoch[075/190] Iteration[150/391] Loss: 0.1691 Acc:94.23%
Training: Epoch[075/190] Iteration[200/391] Loss: 0.1684 Acc:94.21%
Training: Epoch[075/190] Iteration[250/391] Loss: 0.1709 Acc:94.10%
Training: Epoch[075/190] Iteration[300/391] Loss: 0.1740 Acc:93.94%
Training: Epoch[075/190] Iteration[350/391] Loss: 0.1798 Acc:93.74%
Epoch[075/190] Train Acc: 93.73% Valid Acc:87.38% Train loss:0.1798 Valid loss:0.4064 LR:0.1
Training: Epoch[076/190] Iteration[050/391] Loss: 0.1773 Acc:93.83%
Training: Epoch[076/190] Iteration[100/391] Loss: 0.1755 Acc:93.78%
Training: Epoch[076/190] Iteration[150/391] Loss: 0.1754 Acc:93.72%
Training: Epoch[076/190] Iteration[200/391] Loss: 0.1757 Acc:93.76%
Training: Epoch[076/190] Iteration[250/391] Loss: 0.1753 Acc:93.77%
Training: Epoch[076/190] Iteration[300/391] Loss: 0.1751 Acc:93.85%
Training: Epoch[076/190] Iteration[350/391] Loss: 0.1781 Acc:93.70%
Epoch[076/190] Train Acc: 93.72% Valid Acc:86.07% Train loss:0.1790 Valid loss:0.4745 LR:0.1
Training: Epoch[077/190] Iteration[050/391] Loss: 0.1588 Acc:94.42%
Training: Epoch[077/190] Iteration[100/391] Loss: 0.1611 Acc:94.54%
Training: Epoch[077/190] Iteration[150/391] Loss: 0.1608 Acc:94.54%
Training: Epoch[077/190] Iteration[200/391] Loss: 0.1632 Acc:94.46%
Training: Epoch[077/190] Iteration[250/391] Loss: 0.1655 Acc:94.33%
Training: Epoch[077/190] Iteration[300/391] Loss: 0.1692 Acc:94.20%
Training: Epoch[077/190] Iteration[350/391] Loss: 0.1708 Acc:94.13%
Epoch[077/190] Train Acc: 94.05% Valid Acc:85.09% Train loss:0.1722 Valid loss:0.5545 LR:0.1
Training: Epoch[078/190] Iteration[050/391] Loss: 0.1630 Acc:94.25%
Training: Epoch[078/190] Iteration[100/391] Loss: 0.1721 Acc:94.12%
Training: Epoch[078/190] Iteration[150/391] Loss: 0.1724 Acc:94.09%
Training: Epoch[078/190] Iteration[200/391] Loss: 0.1739 Acc:94.03%
Training: Epoch[078/190] Iteration[250/391] Loss: 0.1726 Acc:94.05%
Training: Epoch[078/190] Iteration[300/391] Loss: 0.1738 Acc:93.96%
Training: Epoch[078/190] Iteration[350/391] Loss: 0.1763 Acc:93.87%
Epoch[078/190] Train Acc: 93.77% Valid Acc:88.57% Train loss:0.1790 Valid loss:0.3934 LR:0.1
Training: Epoch[079/190] Iteration[050/391] Loss: 0.1692 Acc:94.14%
Training: Epoch[079/190] Iteration[100/391] Loss: 0.1706 Acc:94.04%
Training: Epoch[079/190] Iteration[150/391] Loss: 0.1674 Acc:94.14%
Training: Epoch[079/190] Iteration[200/391] Loss: 0.1691 Acc:94.14%
Training: Epoch[079/190] Iteration[250/391] Loss: 0.1710 Acc:94.08%
Training: Epoch[079/190] Iteration[300/391] Loss: 0.1751 Acc:93.92%
Training: Epoch[079/190] Iteration[350/391] Loss: 0.1768 Acc:93.84%
Epoch[079/190] Train Acc: 93.76% Valid Acc:87.51% Train loss:0.1786 Valid loss:0.4368 LR:0.1
Training: Epoch[080/190] Iteration[050/391] Loss: 0.1709 Acc:94.06%
Training: Epoch[080/190] Iteration[100/391] Loss: 0.1699 Acc:94.13%
Training: Epoch[080/190] Iteration[150/391] Loss: 0.1750 Acc:93.94%
Training: Epoch[080/190] Iteration[200/391] Loss: 0.1791 Acc:93.68%
Training: Epoch[080/190] Iteration[250/391] Loss: 0.1803 Acc:93.67%
Training: Epoch[080/190] Iteration[300/391] Loss: 0.1826 Acc:93.57%
Training: Epoch[080/190] Iteration[350/391] Loss: 0.1852 Acc:93.40%
Epoch[080/190] Train Acc: 93.40% Valid Acc:89.64% Train loss:0.1846 Valid loss:0.3349 LR:0.1
Training: Epoch[081/190] Iteration[050/391] Loss: 0.1505 Acc:94.73%
Training: Epoch[081/190] Iteration[100/391] Loss: 0.1535 Acc:94.54%
Training: Epoch[081/190] Iteration[150/391] Loss: 0.1621 Acc:94.29%
Training: Epoch[081/190] Iteration[200/391] Loss: 0.1665 Acc:94.16%
Training: Epoch[081/190] Iteration[250/391] Loss: 0.1706 Acc:94.02%
Training: Epoch[081/190] Iteration[300/391] Loss: 0.1713 Acc:93.97%
Training: Epoch[081/190] Iteration[350/391] Loss: 0.1726 Acc:93.95%
Epoch[081/190] Train Acc: 93.96% Valid Acc:87.73% Train loss:0.1733 Valid loss:0.4029 LR:0.1
Training: Epoch[082/190] Iteration[050/391] Loss: 0.1578 Acc:94.56%
Training: Epoch[082/190] Iteration[100/391] Loss: 0.1650 Acc:94.14%
Training: Epoch[082/190] Iteration[150/391] Loss: 0.1698 Acc:94.04%
Training: Epoch[082/190] Iteration[200/391] Loss: 0.1709 Acc:94.02%
Training: Epoch[082/190] Iteration[250/391] Loss: 0.1734 Acc:93.87%
Training: Epoch[082/190] Iteration[300/391] Loss: 0.1734 Acc:93.93%
Training: Epoch[082/190] Iteration[350/391] Loss: 0.1754 Acc:93.83%
Epoch[082/190] Train Acc: 93.84% Valid Acc:86.91% Train loss:0.1748 Valid loss:0.4437 LR:0.1
Training: Epoch[083/190] Iteration[050/391] Loss: 0.1608 Acc:94.67%
Training: Epoch[083/190] Iteration[100/391] Loss: 0.1614 Acc:94.55%
Training: Epoch[083/190] Iteration[150/391] Loss: 0.1689 Acc:94.14%
Training: Epoch[083/190] Iteration[200/391] Loss: 0.1714 Acc:94.02%
Training: Epoch[083/190] Iteration[250/391] Loss: 0.1724 Acc:93.91%
Training: Epoch[083/190] Iteration[300/391] Loss: 0.1744 Acc:93.86%
Training: Epoch[083/190] Iteration[350/391] Loss: 0.1766 Acc:93.81%
Epoch[083/190] Train Acc: 93.82% Valid Acc:88.64% Train loss:0.1771 Valid loss:0.3750 LR:0.1
Training: Epoch[084/190] Iteration[050/391] Loss: 0.1623 Acc:94.36%
Training: Epoch[084/190] Iteration[100/391] Loss: 0.1556 Acc:94.55%
Training: Epoch[084/190] Iteration[150/391] Loss: 0.1550 Acc:94.57%
Training: Epoch[084/190] Iteration[200/391] Loss: 0.1597 Acc:94.41%
Training: Epoch[084/190] Iteration[250/391] Loss: 0.1643 Acc:94.31%
Training: Epoch[084/190] Iteration[300/391] Loss: 0.1655 Acc:94.24%
Training: Epoch[084/190] Iteration[350/391] Loss: 0.1673 Acc:94.17%
Epoch[084/190] Train Acc: 94.14% Valid Acc:89.51% Train loss:0.1674 Valid loss:0.3488 LR:0.1
Training: Epoch[085/190] Iteration[050/391] Loss: 0.1471 Acc:94.75%
Training: Epoch[085/190] Iteration[100/391] Loss: 0.1592 Acc:94.32%
Training: Epoch[085/190] Iteration[150/391] Loss: 0.1646 Acc:94.19%
Training: Epoch[085/190] Iteration[200/391] Loss: 0.1712 Acc:93.98%
Training: Epoch[085/190] Iteration[250/391] Loss: 0.1697 Acc:94.11%
Training: Epoch[085/190] Iteration[300/391] Loss: 0.1728 Acc:94.02%
Training: Epoch[085/190] Iteration[350/391] Loss: 0.1745 Acc:93.89%
Epoch[085/190] Train Acc: 93.83% Valid Acc:86.47% Train loss:0.1762 Valid loss:0.4558 LR:0.1
Training: Epoch[086/190] Iteration[050/391] Loss: 0.1450 Acc:95.06%
Training: Epoch[086/190] Iteration[100/391] Loss: 0.1512 Acc:94.85%
Training: Epoch[086/190] Iteration[150/391] Loss: 0.1603 Acc:94.49%
Training: Epoch[086/190] Iteration[200/391] Loss: 0.1611 Acc:94.42%
Training: Epoch[086/190] Iteration[250/391] Loss: 0.1655 Acc:94.26%
Training: Epoch[086/190] Iteration[300/391] Loss: 0.1656 Acc:94.22%
Training: Epoch[086/190] Iteration[350/391] Loss: 0.1661 Acc:94.25%
Epoch[086/190] Train Acc: 94.18% Valid Acc:87.83% Train loss:0.1686 Valid loss:0.4087 LR:0.1
Training: Epoch[087/190] Iteration[050/391] Loss: 0.1629 Acc:94.33%
Training: Epoch[087/190] Iteration[100/391] Loss: 0.1589 Acc:94.51%
Training: Epoch[087/190] Iteration[150/391] Loss: 0.1614 Acc:94.44%
Training: Epoch[087/190] Iteration[200/391] Loss: 0.1663 Acc:94.20%
Training: Epoch[087/190] Iteration[250/391] Loss: 0.1683 Acc:94.13%
Training: Epoch[087/190] Iteration[300/391] Loss: 0.1685 Acc:94.14%
Training: Epoch[087/190] Iteration[350/391] Loss: 0.1711 Acc:94.01%
Epoch[087/190] Train Acc: 93.96% Valid Acc:86.83% Train loss:0.1724 Valid loss:0.4558 LR:0.1
Training: Epoch[088/190] Iteration[050/391] Loss: 0.1540 Acc:94.75%
Training: Epoch[088/190] Iteration[100/391] Loss: 0.1571 Acc:94.54%
Training: Epoch[088/190] Iteration[150/391] Loss: 0.1578 Acc:94.44%
Training: Epoch[088/190] Iteration[200/391] Loss: 0.1584 Acc:94.50%
Training: Epoch[088/190] Iteration[250/391] Loss: 0.1632 Acc:94.25%
Training: Epoch[088/190] Iteration[300/391] Loss: 0.1636 Acc:94.22%
Training: Epoch[088/190] Iteration[350/391] Loss: 0.1656 Acc:94.19%
Epoch[088/190] Train Acc: 94.16% Valid Acc:88.02% Train loss:0.1661 Valid loss:0.3873 LR:0.1
Training: Epoch[089/190] Iteration[050/391] Loss: 0.1613 Acc:94.41%
Training: Epoch[089/190] Iteration[100/391] Loss: 0.1591 Acc:94.43%
Training: Epoch[089/190] Iteration[150/391] Loss: 0.1582 Acc:94.38%
Training: Epoch[089/190] Iteration[200/391] Loss: 0.1655 Acc:94.09%
Training: Epoch[089/190] Iteration[250/391] Loss: 0.1704 Acc:93.89%
Training: Epoch[089/190] Iteration[300/391] Loss: 0.1715 Acc:93.91%
Training: Epoch[089/190] Iteration[350/391] Loss: 0.1725 Acc:93.88%
Epoch[089/190] Train Acc: 93.84% Valid Acc:87.53% Train loss:0.1738 Valid loss:0.4097 LR:0.1
Training: Epoch[090/190] Iteration[050/391] Loss: 0.1489 Acc:94.77%
Training: Epoch[090/190] Iteration[100/391] Loss: 0.1490 Acc:94.77%
Training: Epoch[090/190] Iteration[150/391] Loss: 0.1523 Acc:94.69%
Training: Epoch[090/190] Iteration[200/391] Loss: 0.1570 Acc:94.48%
Training: Epoch[090/190] Iteration[250/391] Loss: 0.1626 Acc:94.24%
Training: Epoch[090/190] Iteration[300/391] Loss: 0.1640 Acc:94.20%
Training: Epoch[090/190] Iteration[350/391] Loss: 0.1641 Acc:94.21%
Epoch[090/190] Train Acc: 94.17% Valid Acc:86.61% Train loss:0.1646 Valid loss:0.4542 LR:0.1
Training: Epoch[091/190] Iteration[050/391] Loss: 0.1606 Acc:94.56%
Training: Epoch[091/190] Iteration[100/391] Loss: 0.1565 Acc:94.77%
Training: Epoch[091/190] Iteration[150/391] Loss: 0.1632 Acc:94.41%
Training: Epoch[091/190] Iteration[200/391] Loss: 0.1642 Acc:94.28%
Training: Epoch[091/190] Iteration[250/391] Loss: 0.1657 Acc:94.20%
Training: Epoch[091/190] Iteration[300/391] Loss: 0.1683 Acc:94.09%
Training: Epoch[091/190] Iteration[350/391] Loss: 0.1679 Acc:94.12%
Epoch[091/190] Train Acc: 94.02% Valid Acc:86.11% Train loss:0.1699 Valid loss:0.4825 LR:0.1
Training: Epoch[092/190] Iteration[050/391] Loss: 0.1634 Acc:94.25%
Training: Epoch[092/190] Iteration[100/391] Loss: 0.1677 Acc:94.19%
Training: Epoch[092/190] Iteration[150/391] Loss: 0.1687 Acc:94.06%
Training: Epoch[092/190] Iteration[200/391] Loss: 0.1663 Acc:94.10%
Training: Epoch[092/190] Iteration[250/391] Loss: 0.1683 Acc:94.02%
Training: Epoch[092/190] Iteration[300/391] Loss: 0.1696 Acc:94.03%
Training: Epoch[092/190] Iteration[350/391] Loss: 0.1688 Acc:94.06%
Epoch[092/190] Train Acc: 94.11% Valid Acc:85.28% Train loss:0.1679 Valid loss:0.5341 LR:0.1
Training: Epoch[093/190] Iteration[050/391] Loss: 0.1600 Acc:94.22%
Training: Epoch[093/190] Iteration[100/391] Loss: 0.1598 Acc:94.27%
Training: Epoch[093/190] Iteration[150/391] Loss: 0.1634 Acc:94.18%
Training: Epoch[093/190] Iteration[200/391] Loss: 0.1661 Acc:94.14%
Training: Epoch[093/190] Iteration[250/391] Loss: 0.1654 Acc:94.21%
Training: Epoch[093/190] Iteration[300/391] Loss: 0.1669 Acc:94.15%
Training: Epoch[093/190] Iteration[350/391] Loss: 0.1696 Acc:94.07%
Epoch[093/190] Train Acc: 94.00% Valid Acc:89.16% Train loss:0.1717 Valid loss:0.3529 LR:0.1
Training: Epoch[094/190] Iteration[050/391] Loss: 0.1234 Acc:95.72%
Training: Epoch[094/190] Iteration[100/391] Loss: 0.1106 Acc:96.25%
Training: Epoch[094/190] Iteration[150/391] Loss: 0.1039 Acc:96.49%
Training: Epoch[094/190] Iteration[200/391] Loss: 0.1003 Acc:96.64%
Training: Epoch[094/190] Iteration[250/391] Loss: 0.0956 Acc:96.79%
Training: Epoch[094/190] Iteration[300/391] Loss: 0.0923 Acc:96.91%
Training: Epoch[094/190] Iteration[350/391] Loss: 0.0892 Acc:97.04%
Epoch[094/190] Train Acc: 97.10% Valid Acc:92.38% Train loss:0.0869 Valid loss:0.2470 LR:0.01
Training: Epoch[095/190] Iteration[050/391] Loss: 0.0662 Acc:97.66%
Training: Epoch[095/190] Iteration[100/391] Loss: 0.0640 Acc:97.80%
Training: Epoch[095/190] Iteration[150/391] Loss: 0.0617 Acc:97.95%
Training: Epoch[095/190] Iteration[200/391] Loss: 0.0605 Acc:97.98%
Training: Epoch[095/190] Iteration[250/391] Loss: 0.0611 Acc:97.97%
Training: Epoch[095/190] Iteration[300/391] Loss: 0.0605 Acc:97.96%
Training: Epoch[095/190] Iteration[350/391] Loss: 0.0607 Acc:97.96%
Epoch[095/190] Train Acc: 97.98% Valid Acc:92.52% Train loss:0.0602 Valid loss:0.2459 LR:0.01
Training: Epoch[096/190] Iteration[050/391] Loss: 0.0501 Acc:98.44%
Training: Epoch[096/190] Iteration[100/391] Loss: 0.0537 Acc:98.33%
Training: Epoch[096/190] Iteration[150/391] Loss: 0.0518 Acc:98.45%
Training: Epoch[096/190] Iteration[200/391] Loss: 0.0514 Acc:98.46%
Training: Epoch[096/190] Iteration[250/391] Loss: 0.0512 Acc:98.44%
Training: Epoch[096/190] Iteration[300/391] Loss: 0.0509 Acc:98.42%
Training: Epoch[096/190] Iteration[350/391] Loss: 0.0511 Acc:98.41%
Epoch[096/190] Train Acc: 98.42% Valid Acc:92.69% Train loss:0.0508 Valid loss:0.2457 LR:0.01
Training: Epoch[097/190] Iteration[050/391] Loss: 0.0484 Acc:98.53%
Training: Epoch[097/190] Iteration[100/391] Loss: 0.0463 Acc:98.55%
Training: Epoch[097/190] Iteration[150/391] Loss: 0.0459 Acc:98.56%
Training: Epoch[097/190] Iteration[200/391] Loss: 0.0462 Acc:98.52%
Training: Epoch[097/190] Iteration[250/391] Loss: 0.0469 Acc:98.49%
Training: Epoch[097/190] Iteration[300/391] Loss: 0.0465 Acc:98.52%
Training: Epoch[097/190] Iteration[350/391] Loss: 0.0461 Acc:98.52%
Epoch[097/190] Train Acc: 98.50% Valid Acc:92.68% Train loss:0.0465 Valid loss:0.2536 LR:0.01
Training: Epoch[098/190] Iteration[050/391] Loss: 0.0464 Acc:98.61%
Training: Epoch[098/190] Iteration[100/391] Loss: 0.0433 Acc:98.69%
Training: Epoch[098/190] Iteration[150/391] Loss: 0.0414 Acc:98.69%
Training: Epoch[098/190] Iteration[200/391] Loss: 0.0407 Acc:98.70%
Training: Epoch[098/190] Iteration[250/391] Loss: 0.0416 Acc:98.68%
Training: Epoch[098/190] Iteration[300/391] Loss: 0.0414 Acc:98.70%
Training: Epoch[098/190] Iteration[350/391] Loss: 0.0410 Acc:98.71%
Epoch[098/190] Train Acc: 98.71% Valid Acc:92.67% Train loss:0.0405 Valid loss:0.2569 LR:0.01
Training: Epoch[099/190] Iteration[050/391] Loss: 0.0379 Acc:98.81%
Training: Epoch[099/190] Iteration[100/391] Loss: 0.0370 Acc:98.84%
Training: Epoch[099/190] Iteration[150/391] Loss: 0.0378 Acc:98.81%
Training: Epoch[099/190] Iteration[200/391] Loss: 0.0367 Acc:98.85%
Training: Epoch[099/190] Iteration[250/391] Loss: 0.0368 Acc:98.85%
Training: Epoch[099/190] Iteration[300/391] Loss: 0.0369 Acc:98.85%
Training: Epoch[099/190] Iteration[350/391] Loss: 0.0364 Acc:98.87%
Epoch[099/190] Train Acc: 98.86% Valid Acc:92.79% Train loss:0.0366 Valid loss:0.2545 LR:0.01
Training: Epoch[100/190] Iteration[050/391] Loss: 0.0315 Acc:99.09%
Training: Epoch[100/190] Iteration[100/391] Loss: 0.0309 Acc:99.12%
Training: Epoch[100/190] Iteration[150/391] Loss: 0.0322 Acc:99.08%
Training: Epoch[100/190] Iteration[200/391] Loss: 0.0326 Acc:99.04%
Training: Epoch[100/190] Iteration[250/391] Loss: 0.0330 Acc:99.01%
Training: Epoch[100/190] Iteration[300/391] Loss: 0.0334 Acc:98.98%
Training: Epoch[100/190] Iteration[350/391] Loss: 0.0340 Acc:98.96%
Epoch[100/190] Train Acc: 98.96% Valid Acc:92.85% Train loss:0.0339 Valid loss:0.2601 LR:0.01
Training: Epoch[101/190] Iteration[050/391] Loss: 0.0290 Acc:99.20%
Training: Epoch[101/190] Iteration[100/391] Loss: 0.0292 Acc:99.14%
Training: Epoch[101/190] Iteration[150/391] Loss: 0.0300 Acc:99.11%
Training: Epoch[101/190] Iteration[200/391] Loss: 0.0299 Acc:99.13%
Training: Epoch[101/190] Iteration[250/391] Loss: 0.0307 Acc:99.10%
Training: Epoch[101/190] Iteration[300/391] Loss: 0.0306 Acc:99.11%
Training: Epoch[101/190] Iteration[350/391] Loss: 0.0312 Acc:99.07%
Epoch[101/190] Train Acc: 99.05% Valid Acc:92.63% Train loss:0.0315 Valid loss:0.2677 LR:0.01
Training: Epoch[102/190] Iteration[050/391] Loss: 0.0269 Acc:99.12%
Training: Epoch[102/190] Iteration[100/391] Loss: 0.0292 Acc:99.09%
Training: Epoch[102/190] Iteration[150/391] Loss: 0.0292 Acc:99.10%
Training: Epoch[102/190] Iteration[200/391] Loss: 0.0296 Acc:99.08%
Training: Epoch[102/190] Iteration[250/391] Loss: 0.0298 Acc:99.08%
Training: Epoch[102/190] Iteration[300/391] Loss: 0.0300 Acc:99.05%
Training: Epoch[102/190] Iteration[350/391] Loss: 0.0297 Acc:99.06%
Epoch[102/190] Train Acc: 99.05% Valid Acc:92.87% Train loss:0.0299 Valid loss:0.2684 LR:0.01
Training: Epoch[103/190] Iteration[050/391] Loss: 0.0263 Acc:99.25%
Training: Epoch[103/190] Iteration[100/391] Loss: 0.0276 Acc:99.16%
Training: Epoch[103/190] Iteration[150/391] Loss: 0.0271 Acc:99.17%
Training: Epoch[103/190] Iteration[200/391] Loss: 0.0270 Acc:99.19%
Training: Epoch[103/190] Iteration[250/391] Loss: 0.0267 Acc:99.19%
Training: Epoch[103/190] Iteration[300/391] Loss: 0.0266 Acc:99.19%
Training: Epoch[103/190] Iteration[350/391] Loss: 0.0269 Acc:99.17%
Epoch[103/190] Train Acc: 99.17% Valid Acc:92.79% Train loss:0.0268 Valid loss:0.2696 LR:0.01
Training: Epoch[104/190] Iteration[050/391] Loss: 0.0264 Acc:99.20%
Training: Epoch[104/190] Iteration[100/391] Loss: 0.0252 Acc:99.17%
Training: Epoch[104/190] Iteration[150/391] Loss: 0.0266 Acc:99.09%
Training: Epoch[104/190] Iteration[200/391] Loss: 0.0256 Acc:99.14%
Training: Epoch[104/190] Iteration[250/391] Loss: 0.0253 Acc:99.18%
Training: Epoch[104/190] Iteration[300/391] Loss: 0.0251 Acc:99.18%
Training: Epoch[104/190] Iteration[350/391] Loss: 0.0246 Acc:99.21%
Epoch[104/190] Train Acc: 99.19% Valid Acc:92.79% Train loss:0.0255 Valid loss:0.2793 LR:0.01
Training: Epoch[105/190] Iteration[050/391] Loss: 0.0244 Acc:99.28%
Training: Epoch[105/190] Iteration[100/391] Loss: 0.0261 Acc:99.12%
Training: Epoch[105/190] Iteration[150/391] Loss: 0.0248 Acc:99.20%
Training: Epoch[105/190] Iteration[200/391] Loss: 0.0248 Acc:99.22%
Training: Epoch[105/190] Iteration[250/391] Loss: 0.0259 Acc:99.16%
Training: Epoch[105/190] Iteration[300/391] Loss: 0.0255 Acc:99.16%
Training: Epoch[105/190] Iteration[350/391] Loss: 0.0256 Acc:99.17%
Epoch[105/190] Train Acc: 99.17% Valid Acc:92.79% Train loss:0.0256 Valid loss:0.2744 LR:0.01
Training: Epoch[106/190] Iteration[050/391] Loss: 0.0235 Acc:99.31%
Training: Epoch[106/190] Iteration[100/391] Loss: 0.0224 Acc:99.32%
Training: Epoch[106/190] Iteration[150/391] Loss: 0.0224 Acc:99.33%
Training: Epoch[106/190] Iteration[200/391] Loss: 0.0218 Acc:99.35%
Training: Epoch[106/190] Iteration[250/391] Loss: 0.0222 Acc:99.33%
Training: Epoch[106/190] Iteration[300/391] Loss: 0.0223 Acc:99.34%
Training: Epoch[106/190] Iteration[350/391] Loss: 0.0222 Acc:99.34%
Epoch[106/190] Train Acc: 99.34% Valid Acc:92.90% Train loss:0.0224 Valid loss:0.2783 LR:0.01
Training: Epoch[107/190] Iteration[050/391] Loss: 0.0198 Acc:99.34%
Training: Epoch[107/190] Iteration[100/391] Loss: 0.0199 Acc:99.35%
Training: Epoch[107/190] Iteration[150/391] Loss: 0.0211 Acc:99.30%
Training: Epoch[107/190] Iteration[200/391] Loss: 0.0207 Acc:99.36%
Training: Epoch[107/190] Iteration[250/391] Loss: 0.0208 Acc:99.38%
Training: Epoch[107/190] Iteration[300/391] Loss: 0.0214 Acc:99.34%
Training: Epoch[107/190] Iteration[350/391] Loss: 0.0213 Acc:99.37%
Epoch[107/190] Train Acc: 99.37% Valid Acc:92.95% Train loss:0.0212 Valid loss:0.2827 LR:0.01
Training: Epoch[108/190] Iteration[050/391] Loss: 0.0211 Acc:99.34%
Training: Epoch[108/190] Iteration[100/391] Loss: 0.0210 Acc:99.35%
Training: Epoch[108/190] Iteration[150/391] Loss: 0.0205 Acc:99.39%
Training: Epoch[108/190] Iteration[200/391] Loss: 0.0200 Acc:99.39%
Training: Epoch[108/190] Iteration[250/391] Loss: 0.0206 Acc:99.38%
Training: Epoch[108/190] Iteration[300/391] Loss: 0.0207 Acc:99.36%
Training: Epoch[108/190] Iteration[350/391] Loss: 0.0212 Acc:99.35%
Epoch[108/190] Train Acc: 99.36% Valid Acc:93.11% Train loss:0.0210 Valid loss:0.2807 LR:0.01
Training: Epoch[109/190] Iteration[050/391] Loss: 0.0194 Acc:99.42%
Training: Epoch[109/190] Iteration[100/391] Loss: 0.0197 Acc:99.40%
Training: Epoch[109/190] Iteration[150/391] Loss: 0.0191 Acc:99.42%
Training: Epoch[109/190] Iteration[200/391] Loss: 0.0199 Acc:99.37%
Training: Epoch[109/190] Iteration[250/391] Loss: 0.0200 Acc:99.37%
Training: Epoch[109/190] Iteration[300/391] Loss: 0.0200 Acc:99.37%
Training: Epoch[109/190] Iteration[350/391] Loss: 0.0200 Acc:99.38%
Epoch[109/190] Train Acc: 99.38% Valid Acc:92.92% Train loss:0.0197 Valid loss:0.2844 LR:0.01
Training: Epoch[110/190] Iteration[050/391] Loss: 0.0159 Acc:99.55%
Training: Epoch[110/190] Iteration[100/391] Loss: 0.0161 Acc:99.55%
Training: Epoch[110/190] Iteration[150/391] Loss: 0.0158 Acc:99.58%
Training: Epoch[110/190] Iteration[200/391] Loss: 0.0166 Acc:99.56%
Training: Epoch[110/190] Iteration[250/391] Loss: 0.0170 Acc:99.52%
Training: Epoch[110/190] Iteration[300/391] Loss: 0.0177 Acc:99.50%
Training: Epoch[110/190] Iteration[350/391] Loss: 0.0182 Acc:99.47%
Epoch[110/190] Train Acc: 99.45% Valid Acc:93.01% Train loss:0.0187 Valid loss:0.2874 LR:0.01
Training: Epoch[111/190] Iteration[050/391] Loss: 0.0163 Acc:99.61%
Training: Epoch[111/190] Iteration[100/391] Loss: 0.0170 Acc:99.53%
Training: Epoch[111/190] Iteration[150/391] Loss: 0.0168 Acc:99.53%
Training: Epoch[111/190] Iteration[200/391] Loss: 0.0176 Acc:99.50%
Training: Epoch[111/190] Iteration[250/391] Loss: 0.0170 Acc:99.53%
Training: Epoch[111/190] Iteration[300/391] Loss: 0.0170 Acc:99.52%
Training: Epoch[111/190] Iteration[350/391] Loss: 0.0176 Acc:99.50%
Epoch[111/190] Train Acc: 99.50% Valid Acc:93.11% Train loss:0.0176 Valid loss:0.2842 LR:0.01
Training: Epoch[112/190] Iteration[050/391] Loss: 0.0212 Acc:99.39%
Training: Epoch[112/190] Iteration[100/391] Loss: 0.0193 Acc:99.44%
Training: Epoch[112/190] Iteration[150/391] Loss: 0.0197 Acc:99.44%
Training: Epoch[112/190] Iteration[200/391] Loss: 0.0196 Acc:99.42%
Training: Epoch[112/190] Iteration[250/391] Loss: 0.0194 Acc:99.43%
Training: Epoch[112/190] Iteration[300/391] Loss: 0.0193 Acc:99.42%
Training: Epoch[112/190] Iteration[350/391] Loss: 0.0189 Acc:99.44%
Epoch[112/190] Train Acc: 99.44% Valid Acc:93.16% Train loss:0.0186 Valid loss:0.2896 LR:0.01
Training: Epoch[113/190] Iteration[050/391] Loss: 0.0178 Acc:99.38%
Training: Epoch[113/190] Iteration[100/391] Loss: 0.0167 Acc:99.49%
Training: Epoch[113/190] Iteration[150/391] Loss: 0.0168 Acc:99.48%
Training: Epoch[113/190] Iteration[200/391] Loss: 0.0165 Acc:99.51%
Training: Epoch[113/190] Iteration[250/391] Loss: 0.0166 Acc:99.51%
Training: Epoch[113/190] Iteration[300/391] Loss: 0.0165 Acc:99.52%
Training: Epoch[113/190] Iteration[350/391] Loss: 0.0169 Acc:99.50%
Epoch[113/190] Train Acc: 99.50% Valid Acc:93.24% Train loss:0.0170 Valid loss:0.2873 LR:0.01
Training: Epoch[114/190] Iteration[050/391] Loss: 0.0152 Acc:99.52%
Training: Epoch[114/190] Iteration[100/391] Loss: 0.0164 Acc:99.52%
Training: Epoch[114/190] Iteration[150/391] Loss: 0.0160 Acc:99.55%
Training: Epoch[114/190] Iteration[200/391] Loss: 0.0164 Acc:99.52%
Training: Epoch[114/190] Iteration[250/391] Loss: 0.0161 Acc:99.53%
Training: Epoch[114/190] Iteration[300/391] Loss: 0.0159 Acc:99.53%
Training: Epoch[114/190] Iteration[350/391] Loss: 0.0156 Acc:99.54%
Epoch[114/190] Train Acc: 99.53% Valid Acc:93.06% Train loss:0.0160 Valid loss:0.2933 LR:0.01
Training: Epoch[115/190] Iteration[050/391] Loss: 0.0139 Acc:99.62%
Training: Epoch[115/190] Iteration[100/391] Loss: 0.0141 Acc:99.65%
Training: Epoch[115/190] Iteration[150/391] Loss: 0.0143 Acc:99.62%
Training: Epoch[115/190] Iteration[200/391] Loss: 0.0145 Acc:99.61%
Training: Epoch[115/190] Iteration[250/391] Loss: 0.0146 Acc:99.58%
Training: Epoch[115/190] Iteration[300/391] Loss: 0.0145 Acc:99.58%
Training: Epoch[115/190] Iteration[350/391] Loss: 0.0155 Acc:99.55%
Epoch[115/190] Train Acc: 99.56% Valid Acc:93.00% Train loss:0.0155 Valid loss:0.2993 LR:0.01
Training: Epoch[116/190] Iteration[050/391] Loss: 0.0147 Acc:99.55%
Training: Epoch[116/190] Iteration[100/391] Loss: 0.0150 Acc:99.54%
Training: Epoch[116/190] Iteration[150/391] Loss: 0.0148 Acc:99.55%
Training: Epoch[116/190] Iteration[200/391] Loss: 0.0148 Acc:99.54%
Training: Epoch[116/190] Iteration[250/391] Loss: 0.0146 Acc:99.55%
Training: Epoch[116/190] Iteration[300/391] Loss: 0.0144 Acc:99.56%
Training: Epoch[116/190] Iteration[350/391] Loss: 0.0145 Acc:99.56%
Epoch[116/190] Train Acc: 99.56% Valid Acc:93.15% Train loss:0.0144 Valid loss:0.2924 LR:0.01
Training: Epoch[117/190] Iteration[050/391] Loss: 0.0132 Acc:99.66%
Training: Epoch[117/190] Iteration[100/391] Loss: 0.0144 Acc:99.57%
Training: Epoch[117/190] Iteration[150/391] Loss: 0.0144 Acc:99.56%
Training: Epoch[117/190] Iteration[200/391] Loss: 0.0142 Acc:99.59%
Training: Epoch[117/190] Iteration[250/391] Loss: 0.0138 Acc:99.57%
Training: Epoch[117/190] Iteration[300/391] Loss: 0.0136 Acc:99.59%
Training: Epoch[117/190] Iteration[350/391] Loss: 0.0141 Acc:99.57%
Epoch[117/190] Train Acc: 99.56% Valid Acc:93.42% Train loss:0.0144 Valid loss:0.2886 LR:0.01
Training: Epoch[118/190] Iteration[050/391] Loss: 0.0138 Acc:99.56%
Training: Epoch[118/190] Iteration[100/391] Loss: 0.0126 Acc:99.62%
Training: Epoch[118/190] Iteration[150/391] Loss: 0.0123 Acc:99.65%
Training: Epoch[118/190] Iteration[200/391] Loss: 0.0132 Acc:99.63%
Training: Epoch[118/190] Iteration[250/391] Loss: 0.0133 Acc:99.65%
Training: Epoch[118/190] Iteration[300/391] Loss: 0.0132 Acc:99.66%
Training: Epoch[118/190] Iteration[350/391] Loss: 0.0137 Acc:99.64%
Epoch[118/190] Train Acc: 99.64% Valid Acc:93.18% Train loss:0.0138 Valid loss:0.3023 LR:0.01
Training: Epoch[119/190] Iteration[050/391] Loss: 0.0133 Acc:99.64%
Training: Epoch[119/190] Iteration[100/391] Loss: 0.0127 Acc:99.65%
Training: Epoch[119/190] Iteration[150/391] Loss: 0.0130 Acc:99.65%
Training: Epoch[119/190] Iteration[200/391] Loss: 0.0127 Acc:99.66%
Training: Epoch[119/190] Iteration[250/391] Loss: 0.0130 Acc:99.63%
Training: Epoch[119/190] Iteration[300/391] Loss: 0.0129 Acc:99.63%
Training: Epoch[119/190] Iteration[350/391] Loss: 0.0127 Acc:99.65%
Epoch[119/190] Train Acc: 99.63% Valid Acc:93.11% Train loss:0.0127 Valid loss:0.3016 LR:0.01
Training: Epoch[120/190] Iteration[050/391] Loss: 0.0151 Acc:99.52%
Training: Epoch[120/190] Iteration[100/391] Loss: 0.0142 Acc:99.55%
Training: Epoch[120/190] Iteration[150/391] Loss: 0.0141 Acc:99.55%
Training: Epoch[120/190] Iteration[200/391] Loss: 0.0138 Acc:99.58%
Training: Epoch[120/190] Iteration[250/391] Loss: 0.0139 Acc:99.58%
Training: Epoch[120/190] Iteration[300/391] Loss: 0.0137 Acc:99.60%
Training: Epoch[120/190] Iteration[350/391] Loss: 0.0134 Acc:99.60%
Epoch[120/190] Train Acc: 99.60% Valid Acc:93.18% Train loss:0.0134 Valid loss:0.3079 LR:0.01
Training: Epoch[121/190] Iteration[050/391] Loss: 0.0157 Acc:99.41%
Training: Epoch[121/190] Iteration[100/391] Loss: 0.0144 Acc:99.48%
Training: Epoch[121/190] Iteration[150/391] Loss: 0.0134 Acc:99.56%
Training: Epoch[121/190] Iteration[200/391] Loss: 0.0129 Acc:99.59%
Training: Epoch[121/190] Iteration[250/391] Loss: 0.0130 Acc:99.61%
Training: Epoch[121/190] Iteration[300/391] Loss: 0.0132 Acc:99.61%
Training: Epoch[121/190] Iteration[350/391] Loss: 0.0133 Acc:99.60%
Epoch[121/190] Train Acc: 99.62% Valid Acc:93.12% Train loss:0.0132 Valid loss:0.3072 LR:0.01
Training: Epoch[122/190] Iteration[050/391] Loss: 0.0150 Acc:99.50%
Training: Epoch[122/190] Iteration[100/391] Loss: 0.0138 Acc:99.56%
Training: Epoch[122/190] Iteration[150/391] Loss: 0.0132 Acc:99.58%
Training: Epoch[122/190] Iteration[200/391] Loss: 0.0136 Acc:99.58%
Training: Epoch[122/190] Iteration[250/391] Loss: 0.0133 Acc:99.59%
Training: Epoch[122/190] Iteration[300/391] Loss: 0.0136 Acc:99.58%
Training: Epoch[122/190] Iteration[350/391] Loss: 0.0135 Acc:99.58%
Epoch[122/190] Train Acc: 99.59% Valid Acc:93.03% Train loss:0.0133 Valid loss:0.3120 LR:0.01
Training: Epoch[123/190] Iteration[050/391] Loss: 0.0112 Acc:99.64%
Training: Epoch[123/190] Iteration[100/391] Loss: 0.0117 Acc:99.65%
Training: Epoch[123/190] Iteration[150/391] Loss: 0.0121 Acc:99.66%
Training: Epoch[123/190] Iteration[200/391] Loss: 0.0120 Acc:99.66%
Training: Epoch[123/190] Iteration[250/391] Loss: 0.0121 Acc:99.67%
Training: Epoch[123/190] Iteration[300/391] Loss: 0.0119 Acc:99.66%
Training: Epoch[123/190] Iteration[350/391] Loss: 0.0119 Acc:99.66%
Epoch[123/190] Train Acc: 99.66% Valid Acc:93.23% Train loss:0.0119 Valid loss:0.3036 LR:0.01
Training: Epoch[124/190] Iteration[050/391] Loss: 0.0095 Acc:99.75%
Training: Epoch[124/190] Iteration[100/391] Loss: 0.0098 Acc:99.73%
Training: Epoch[124/190] Iteration[150/391] Loss: 0.0095 Acc:99.75%
Training: Epoch[124/190] Iteration[200/391] Loss: 0.0102 Acc:99.72%
Training: Epoch[124/190] Iteration[250/391] Loss: 0.0107 Acc:99.69%
Training: Epoch[124/190] Iteration[300/391] Loss: 0.0110 Acc:99.67%
Training: Epoch[124/190] Iteration[350/391] Loss: 0.0108 Acc:99.69%
Epoch[124/190] Train Acc: 99.68% Valid Acc:93.04% Train loss:0.0110 Valid loss:0.3160 LR:0.01
Training: Epoch[125/190] Iteration[050/391] Loss: 0.0089 Acc:99.80%
Training: Epoch[125/190] Iteration[100/391] Loss: 0.0096 Acc:99.74%
Training: Epoch[125/190] Iteration[150/391] Loss: 0.0108 Acc:99.69%
Training: Epoch[125/190] Iteration[200/391] Loss: 0.0111 Acc:99.70%
Training: Epoch[125/190] Iteration[250/391] Loss: 0.0111 Acc:99.69%
Training: Epoch[125/190] Iteration[300/391] Loss: 0.0113 Acc:99.68%
Training: Epoch[125/190] Iteration[350/391] Loss: 0.0114 Acc:99.68%
Epoch[125/190] Train Acc: 99.68% Valid Acc:93.05% Train loss:0.0114 Valid loss:0.3106 LR:0.01
Training: Epoch[126/190] Iteration[050/391] Loss: 0.0141 Acc:99.50%
Training: Epoch[126/190] Iteration[100/391] Loss: 0.0132 Acc:99.56%
Training: Epoch[126/190] Iteration[150/391] Loss: 0.0123 Acc:99.62%
Training: Epoch[126/190] Iteration[200/391] Loss: 0.0121 Acc:99.64%
Training: Epoch[126/190] Iteration[250/391] Loss: 0.0126 Acc:99.63%
Training: Epoch[126/190] Iteration[300/391] Loss: 0.0123 Acc:99.64%
Training: Epoch[126/190] Iteration[350/391] Loss: 0.0124 Acc:99.64%
Epoch[126/190] Train Acc: 99.64% Valid Acc:93.08% Train loss:0.0123 Valid loss:0.3161 LR:0.01
Training: Epoch[127/190] Iteration[050/391] Loss: 0.0102 Acc:99.73%
Training: Epoch[127/190] Iteration[100/391] Loss: 0.0103 Acc:99.70%
Training: Epoch[127/190] Iteration[150/391] Loss: 0.0112 Acc:99.69%
Training: Epoch[127/190] Iteration[200/391] Loss: 0.0111 Acc:99.70%
Training: Epoch[127/190] Iteration[250/391] Loss: 0.0111 Acc:99.68%
Training: Epoch[127/190] Iteration[300/391] Loss: 0.0110 Acc:99.68%
Training: Epoch[127/190] Iteration[350/391] Loss: 0.0110 Acc:99.69%
Epoch[127/190] Train Acc: 99.70% Valid Acc:92.99% Train loss:0.0109 Valid loss:0.3161 LR:0.01
Training: Epoch[128/190] Iteration[050/391] Loss: 0.0113 Acc:99.66%
Training: Epoch[128/190] Iteration[100/391] Loss: 0.0100 Acc:99.73%
Training: Epoch[128/190] Iteration[150/391] Loss: 0.0095 Acc:99.74%
Training: Epoch[128/190] Iteration[200/391] Loss: 0.0091 Acc:99.77%
Training: Epoch[128/190] Iteration[250/391] Loss: 0.0094 Acc:99.76%
Training: Epoch[128/190] Iteration[300/391] Loss: 0.0097 Acc:99.75%
Training: Epoch[128/190] Iteration[350/391] Loss: 0.0096 Acc:99.76%
Epoch[128/190] Train Acc: 99.75% Valid Acc:93.07% Train loss:0.0096 Valid loss:0.3208 LR:0.01
Training: Epoch[129/190] Iteration[050/391] Loss: 0.0116 Acc:99.69%
Training: Epoch[129/190] Iteration[100/391] Loss: 0.0112 Acc:99.70%
Training: Epoch[129/190] Iteration[150/391] Loss: 0.0109 Acc:99.70%
Training: Epoch[129/190] Iteration[200/391] Loss: 0.0107 Acc:99.70%
Training: Epoch[129/190] Iteration[250/391] Loss: 0.0103 Acc:99.73%
Training: Epoch[129/190] Iteration[300/391] Loss: 0.0102 Acc:99.73%
Training: Epoch[129/190] Iteration[350/391] Loss: 0.0100 Acc:99.74%
Epoch[129/190] Train Acc: 99.73% Valid Acc:93.06% Train loss:0.0100 Valid loss:0.3237 LR:0.01
Training: Epoch[130/190] Iteration[050/391] Loss: 0.0110 Acc:99.72%
Training: Epoch[130/190] Iteration[100/391] Loss: 0.0106 Acc:99.71%
Training: Epoch[130/190] Iteration[150/391] Loss: 0.0105 Acc:99.70%
Training: Epoch[130/190] Iteration[200/391] Loss: 0.0102 Acc:99.70%
Training: Epoch[130/190] Iteration[250/391] Loss: 0.0105 Acc:99.68%
Training: Epoch[130/190] Iteration[300/391] Loss: 0.0104 Acc:99.69%
Training: Epoch[130/190] Iteration[350/391] Loss: 0.0103 Acc:99.69%
Epoch[130/190] Train Acc: 99.69% Valid Acc:93.07% Train loss:0.0105 Valid loss:0.3234 LR:0.01
Training: Epoch[131/190] Iteration[050/391] Loss: 0.0109 Acc:99.66%
Training: Epoch[131/190] Iteration[100/391] Loss: 0.0113 Acc:99.68%
Training: Epoch[131/190] Iteration[150/391] Loss: 0.0110 Acc:99.70%
Training: Epoch[131/190] Iteration[200/391] Loss: 0.0105 Acc:99.69%
Training: Epoch[131/190] Iteration[250/391] Loss: 0.0105 Acc:99.70%
Training: Epoch[131/190] Iteration[300/391] Loss: 0.0109 Acc:99.69%
Training: Epoch[131/190] Iteration[350/391] Loss: 0.0105 Acc:99.70%
Epoch[131/190] Train Acc: 99.70% Valid Acc:93.23% Train loss:0.0104 Valid loss:0.3319 LR:0.01
Training: Epoch[132/190] Iteration[050/391] Loss: 0.0103 Acc:99.73%
Training: Epoch[132/190] Iteration[100/391] Loss: 0.0098 Acc:99.73%
Training: Epoch[132/190] Iteration[150/391] Loss: 0.0094 Acc:99.76%
Training: Epoch[132/190] Iteration[200/391] Loss: 0.0090 Acc:99.77%
Training: Epoch[132/190] Iteration[250/391] Loss: 0.0091 Acc:99.76%
Training: Epoch[132/190] Iteration[300/391] Loss: 0.0089 Acc:99.78%
Training: Epoch[132/190] Iteration[350/391] Loss: 0.0089 Acc:99.77%
Epoch[132/190] Train Acc: 99.76% Valid Acc:93.24% Train loss:0.0088 Valid loss:0.3289 LR:0.01
Training: Epoch[133/190] Iteration[050/391] Loss: 0.0077 Acc:99.78%
Training: Epoch[133/190] Iteration[100/391] Loss: 0.0083 Acc:99.77%
Training: Epoch[133/190] Iteration[150/391] Loss: 0.0087 Acc:99.76%
Training: Epoch[133/190] Iteration[200/391] Loss: 0.0086 Acc:99.78%
Training: Epoch[133/190] Iteration[250/391] Loss: 0.0089 Acc:99.76%
Training: Epoch[133/190] Iteration[300/391] Loss: 0.0088 Acc:99.76%
Training: Epoch[133/190] Iteration[350/391] Loss: 0.0089 Acc:99.75%
Epoch[133/190] Train Acc: 99.75% Valid Acc:93.15% Train loss:0.0090 Valid loss:0.3344 LR:0.01
Training: Epoch[134/190] Iteration[050/391] Loss: 0.0075 Acc:99.81%
Training: Epoch[134/190] Iteration[100/391] Loss: 0.0084 Acc:99.77%
Training: Epoch[134/190] Iteration[150/391] Loss: 0.0086 Acc:99.76%
Training: Epoch[134/190] Iteration[200/391] Loss: 0.0089 Acc:99.75%
Training: Epoch[134/190] Iteration[250/391] Loss: 0.0090 Acc:99.72%
Training: Epoch[134/190] Iteration[300/391] Loss: 0.0090 Acc:99.72%
Training: Epoch[134/190] Iteration[350/391] Loss: 0.0090 Acc:99.73%
Epoch[134/190] Train Acc: 99.72% Valid Acc:92.96% Train loss:0.0092 Valid loss:0.3288 LR:0.01
Training: Epoch[135/190] Iteration[050/391] Loss: 0.0088 Acc:99.72%
Training: Epoch[135/190] Iteration[100/391] Loss: 0.0088 Acc:99.70%
Training: Epoch[135/190] Iteration[150/391] Loss: 0.0090 Acc:99.71%
Training: Epoch[135/190] Iteration[200/391] Loss: 0.0090 Acc:99.73%
Training: Epoch[135/190] Iteration[250/391] Loss: 0.0092 Acc:99.72%
Training: Epoch[135/190] Iteration[300/391] Loss: 0.0094 Acc:99.71%
Training: Epoch[135/190] Iteration[350/391] Loss: 0.0098 Acc:99.70%
Epoch[135/190] Train Acc: 99.71% Valid Acc:93.20% Train loss:0.0097 Valid loss:0.3288 LR:0.01
Training: Epoch[136/190] Iteration[050/391] Loss: 0.0109 Acc:99.69%
Training: Epoch[136/190] Iteration[100/391] Loss: 0.0100 Acc:99.73%
Training: Epoch[136/190] Iteration[150/391] Loss: 0.0093 Acc:99.71%
Training: Epoch[136/190] Iteration[200/391] Loss: 0.0090 Acc:99.73%
Training: Epoch[136/190] Iteration[250/391] Loss: 0.0088 Acc:99.75%
Training: Epoch[136/190] Iteration[300/391] Loss: 0.0085 Acc:99.76%
Training: Epoch[136/190] Iteration[350/391] Loss: 0.0082 Acc:99.78%
Epoch[136/190] Train Acc: 99.79% Valid Acc:92.95% Train loss:0.0081 Valid loss:0.3323 LR:0.01
Training: Epoch[137/190] Iteration[050/391] Loss: 0.0075 Acc:99.83%
Training: Epoch[137/190] Iteration[100/391] Loss: 0.0075 Acc:99.84%
Training: Epoch[137/190] Iteration[150/391] Loss: 0.0073 Acc:99.84%
Training: Epoch[137/190] Iteration[200/391] Loss: 0.0077 Acc:99.83%
Training: Epoch[137/190] Iteration[250/391] Loss: 0.0077 Acc:99.83%
Training: Epoch[137/190] Iteration[300/391] Loss: 0.0082 Acc:99.79%
Training: Epoch[137/190] Iteration[350/391] Loss: 0.0080 Acc:99.80%
Epoch[137/190] Train Acc: 99.79% Valid Acc:92.94% Train loss:0.0084 Valid loss:0.3321 LR:0.01
Training: Epoch[138/190] Iteration[050/391] Loss: 0.0099 Acc:99.73%
Training: Epoch[138/190] Iteration[100/391] Loss: 0.0089 Acc:99.77%
Training: Epoch[138/190] Iteration[150/391] Loss: 0.0084 Acc:99.77%
Training: Epoch[138/190] Iteration[200/391] Loss: 0.0081 Acc:99.77%
Training: Epoch[138/190] Iteration[250/391] Loss: 0.0083 Acc:99.76%
Training: Epoch[138/190] Iteration[300/391] Loss: 0.0079 Acc:99.78%
Training: Epoch[138/190] Iteration[350/391] Loss: 0.0082 Acc:99.77%
Epoch[138/190] Train Acc: 99.78% Valid Acc:92.95% Train loss:0.0082 Valid loss:0.3266 LR:0.001
Training: Epoch[139/190] Iteration[050/391] Loss: 0.0057 Acc:99.88%
Training: Epoch[139/190] Iteration[100/391] Loss: 0.0070 Acc:99.83%
Training: Epoch[139/190] Iteration[150/391] Loss: 0.0062 Acc:99.86%
Training: Epoch[139/190] Iteration[200/391] Loss: 0.0065 Acc:99.84%
Training: Epoch[139/190] Iteration[250/391] Loss: 0.0067 Acc:99.82%
Training: Epoch[139/190] Iteration[300/391] Loss: 0.0069 Acc:99.82%
Training: Epoch[139/190] Iteration[350/391] Loss: 0.0071 Acc:99.81%
Epoch[139/190] Train Acc: 99.81% Valid Acc:92.95% Train loss:0.0072 Valid loss:0.3285 LR:0.001
Training: Epoch[140/190] Iteration[050/391] Loss: 0.0064 Acc:99.86%
Training: Epoch[140/190] Iteration[100/391] Loss: 0.0070 Acc:99.84%
Training: Epoch[140/190] Iteration[150/391] Loss: 0.0071 Acc:99.84%
Training: Epoch[140/190] Iteration[200/391] Loss: 0.0071 Acc:99.84%
Training: Epoch[140/190] Iteration[250/391] Loss: 0.0071 Acc:99.83%
Training: Epoch[140/190] Iteration[300/391] Loss: 0.0069 Acc:99.84%
Training: Epoch[140/190] Iteration[350/391] Loss: 0.0069 Acc:99.84%
Epoch[140/190] Train Acc: 99.84% Valid Acc:93.03% Train loss:0.0068 Valid loss:0.3250 LR:0.001
Training: Epoch[141/190] Iteration[050/391] Loss: 0.0066 Acc:99.81%
Training: Epoch[141/190] Iteration[100/391] Loss: 0.0064 Acc:99.83%
Training: Epoch[141/190] Iteration[150/391] Loss: 0.0064 Acc:99.84%
Training: Epoch[141/190] Iteration[200/391] Loss: 0.0065 Acc:99.84%
Training: Epoch[141/190] Iteration[250/391] Loss: 0.0063 Acc:99.84%
Training: Epoch[141/190] Iteration[300/391] Loss: 0.0064 Acc:99.84%
Training: Epoch[141/190] Iteration[350/391] Loss: 0.0066 Acc:99.83%
Epoch[141/190] Train Acc: 99.85% Valid Acc:93.05% Train loss:0.0064 Valid loss:0.3293 LR:0.001
Training: Epoch[142/190] Iteration[050/391] Loss: 0.0081 Acc:99.78%
Training: Epoch[142/190] Iteration[100/391] Loss: 0.0077 Acc:99.81%
Training: Epoch[142/190] Iteration[150/391] Loss: 0.0073 Acc:99.82%
Training: Epoch[142/190] Iteration[200/391] Loss: 0.0068 Acc:99.85%
Training: Epoch[142/190] Iteration[250/391] Loss: 0.0067 Acc:99.85%
Training: Epoch[142/190] Iteration[300/391] Loss: 0.0066 Acc:99.85%
Training: Epoch[142/190] Iteration[350/391] Loss: 0.0067 Acc:99.85%
Epoch[142/190] Train Acc: 99.84% Valid Acc:93.11% Train loss:0.0066 Valid loss:0.3256 LR:0.001
Training: Epoch[143/190] Iteration[050/391] Loss: 0.0074 Acc:99.81%
Training: Epoch[143/190] Iteration[100/391] Loss: 0.0073 Acc:99.82%
Training: Epoch[143/190] Iteration[150/391] Loss: 0.0066 Acc:99.85%
Training: Epoch[143/190] Iteration[200/391] Loss: 0.0064 Acc:99.85%
Training: Epoch[143/190] Iteration[250/391] Loss: 0.0065 Acc:99.85%
Training: Epoch[143/190] Iteration[300/391] Loss: 0.0064 Acc:99.86%
Training: Epoch[143/190] Iteration[350/391] Loss: 0.0062 Acc:99.87%
Epoch[143/190] Train Acc: 99.86% Valid Acc:93.05% Train loss:0.0063 Valid loss:0.3242 LR:0.001
Training: Epoch[144/190] Iteration[050/391] Loss: 0.0052 Acc:99.92%
Training: Epoch[144/190] Iteration[100/391] Loss: 0.0058 Acc:99.86%
Training: Epoch[144/190] Iteration[150/391] Loss: 0.0059 Acc:99.84%
Training: Epoch[144/190] Iteration[200/391] Loss: 0.0063 Acc:99.84%
Training: Epoch[144/190] Iteration[250/391] Loss: 0.0062 Acc:99.84%
Training: Epoch[144/190] Iteration[300/391] Loss: 0.0062 Acc:99.85%
Training: Epoch[144/190] Iteration[350/391] Loss: 0.0062 Acc:99.84%
Epoch[144/190] Train Acc: 99.84% Valid Acc:93.11% Train loss:0.0062 Valid loss:0.3204 LR:0.001
Training: Epoch[145/190] Iteration[050/391] Loss: 0.0064 Acc:99.91%
Training: Epoch[145/190] Iteration[100/391] Loss: 0.0059 Acc:99.90%
Training: Epoch[145/190] Iteration[150/391] Loss: 0.0059 Acc:99.89%
Training: Epoch[145/190] Iteration[200/391] Loss: 0.0064 Acc:99.85%
Training: Epoch[145/190] Iteration[250/391] Loss: 0.0061 Acc:99.87%
Training: Epoch[145/190] Iteration[300/391] Loss: 0.0059 Acc:99.88%
Training: Epoch[145/190] Iteration[350/391] Loss: 0.0058 Acc:99.89%
Epoch[145/190] Train Acc: 99.89% Valid Acc:93.13% Train loss:0.0057 Valid loss:0.3251 LR:0.001
Training: Epoch[146/190] Iteration[050/391] Loss: 0.0059 Acc:99.88%
Training: Epoch[146/190] Iteration[100/391] Loss: 0.0054 Acc:99.88%
Training: Epoch[146/190] Iteration[150/391] Loss: 0.0056 Acc:99.89%
Training: Epoch[146/190] Iteration[200/391] Loss: 0.0060 Acc:99.86%
Training: Epoch[146/190] Iteration[250/391] Loss: 0.0059 Acc:99.88%
Training: Epoch[146/190] Iteration[300/391] Loss: 0.0063 Acc:99.85%
Training: Epoch[146/190] Iteration[350/391] Loss: 0.0064 Acc:99.85%
Epoch[146/190] Train Acc: 99.85% Valid Acc:93.18% Train loss:0.0063 Valid loss:0.3222 LR:0.001
Training: Epoch[147/190] Iteration[050/391] Loss: 0.0053 Acc:99.89%
Training: Epoch[147/190] Iteration[100/391] Loss: 0.0056 Acc:99.88%
Training: Epoch[147/190] Iteration[150/391] Loss: 0.0056 Acc:99.88%
Training: Epoch[147/190] Iteration[200/391] Loss: 0.0057 Acc:99.87%
Training: Epoch[147/190] Iteration[250/391] Loss: 0.0058 Acc:99.87%
Training: Epoch[147/190] Iteration[300/391] Loss: 0.0059 Acc:99.87%
Training: Epoch[147/190] Iteration[350/391] Loss: 0.0059 Acc:99.87%
Epoch[147/190] Train Acc: 99.87% Valid Acc:93.19% Train loss:0.0060 Valid loss:0.3224 LR:0.001
Training: Epoch[148/190] Iteration[050/391] Loss: 0.0050 Acc:99.89%
Training: Epoch[148/190] Iteration[100/391] Loss: 0.0052 Acc:99.90%
Training: Epoch[148/190] Iteration[150/391] Loss: 0.0048 Acc:99.93%
Training: Epoch[148/190] Iteration[200/391] Loss: 0.0052 Acc:99.90%
Training: Epoch[148/190] Iteration[250/391] Loss: 0.0053 Acc:99.89%
Training: Epoch[148/190] Iteration[300/391] Loss: 0.0053 Acc:99.90%
Training: Epoch[148/190] Iteration[350/391] Loss: 0.0052 Acc:99.90%
Epoch[148/190] Train Acc: 99.89% Valid Acc:93.17% Train loss:0.0055 Valid loss:0.3209 LR:0.001
Training: Epoch[149/190] Iteration[050/391] Loss: 0.0058 Acc:99.86%
Training: Epoch[149/190] Iteration[100/391] Loss: 0.0058 Acc:99.85%
Training: Epoch[149/190] Iteration[150/391] Loss: 0.0060 Acc:99.86%
Training: Epoch[149/190] Iteration[200/391] Loss: 0.0060 Acc:99.86%
Training: Epoch[149/190] Iteration[250/391] Loss: 0.0064 Acc:99.84%
Training: Epoch[149/190] Iteration[300/391] Loss: 0.0064 Acc:99.83%
Training: Epoch[149/190] Iteration[350/391] Loss: 0.0063 Acc:99.84%
Epoch[149/190] Train Acc: 99.84% Valid Acc:93.11% Train loss:0.0064 Valid loss:0.3269 LR:0.001
Training: Epoch[150/190] Iteration[050/391] Loss: 0.0052 Acc:99.91%
Training: Epoch[150/190] Iteration[100/391] Loss: 0.0056 Acc:99.87%
Training: Epoch[150/190] Iteration[150/391] Loss: 0.0055 Acc:99.88%
Training: Epoch[150/190] Iteration[200/391] Loss: 0.0055 Acc:99.89%
Training: Epoch[150/190] Iteration[250/391] Loss: 0.0059 Acc:99.86%
Training: Epoch[150/190] Iteration[300/391] Loss: 0.0057 Acc:99.86%
Training: Epoch[150/190] Iteration[350/391] Loss: 0.0056 Acc:99.87%
Epoch[150/190] Train Acc: 99.88% Valid Acc:93.21% Train loss:0.0056 Valid loss:0.3196 LR:0.001
Training: Epoch[151/190] Iteration[050/391] Loss: 0.0052 Acc:99.89%
Training: Epoch[151/190] Iteration[100/391] Loss: 0.0052 Acc:99.88%
Training: Epoch[151/190] Iteration[150/391] Loss: 0.0055 Acc:99.89%
Training: Epoch[151/190] Iteration[200/391] Loss: 0.0053 Acc:99.89%
Training: Epoch[151/190] Iteration[250/391] Loss: 0.0053 Acc:99.89%
Training: Epoch[151/190] Iteration[300/391] Loss: 0.0055 Acc:99.88%
Training: Epoch[151/190] Iteration[350/391] Loss: 0.0054 Acc:99.89%
Epoch[151/190] Train Acc: 99.89% Valid Acc:93.26% Train loss:0.0054 Valid loss:0.3213 LR:0.001
Training: Epoch[152/190] Iteration[050/391] Loss: 0.0053 Acc:99.92%
Training: Epoch[152/190] Iteration[100/391] Loss: 0.0056 Acc:99.88%
Training: Epoch[152/190] Iteration[150/391] Loss: 0.0057 Acc:99.89%
Training: Epoch[152/190] Iteration[200/391] Loss: 0.0054 Acc:99.89%
Training: Epoch[152/190] Iteration[250/391] Loss: 0.0056 Acc:99.88%
Training: Epoch[152/190] Iteration[300/391] Loss: 0.0057 Acc:99.88%
Training: Epoch[152/190] Iteration[350/391] Loss: 0.0054 Acc:99.90%
Epoch[152/190] Train Acc: 99.89% Valid Acc:93.23% Train loss:0.0054 Valid loss:0.3222 LR:0.001
Training: Epoch[153/190] Iteration[050/391] Loss: 0.0068 Acc:99.77%
Training: Epoch[153/190] Iteration[100/391] Loss: 0.0063 Acc:99.82%
Training: Epoch[153/190] Iteration[150/391] Loss: 0.0059 Acc:99.85%
Training: Epoch[153/190] Iteration[200/391] Loss: 0.0056 Acc:99.88%
Training: Epoch[153/190] Iteration[250/391] Loss: 0.0052 Acc:99.89%
Training: Epoch[153/190] Iteration[300/391] Loss: 0.0051 Acc:99.89%
Training: Epoch[153/190] Iteration[350/391] Loss: 0.0051 Acc:99.89%
Epoch[153/190] Train Acc: 99.89% Valid Acc:93.27% Train loss:0.0051 Valid loss:0.3221 LR:0.001
Training: Epoch[154/190] Iteration[050/391] Loss: 0.0042 Acc:99.97%
Training: Epoch[154/190] Iteration[100/391] Loss: 0.0049 Acc:99.92%
Training: Epoch[154/190] Iteration[150/391] Loss: 0.0055 Acc:99.89%
Training: Epoch[154/190] Iteration[200/391] Loss: 0.0057 Acc:99.88%
Training: Epoch[154/190] Iteration[250/391] Loss: 0.0055 Acc:99.88%
Training: Epoch[154/190] Iteration[300/391] Loss: 0.0055 Acc:99.88%
Training: Epoch[154/190] Iteration[350/391] Loss: 0.0053 Acc:99.89%
Epoch[154/190] Train Acc: 99.88% Valid Acc:93.25% Train loss:0.0056 Valid loss:0.3233 LR:0.001
Training: Epoch[155/190] Iteration[050/391] Loss: 0.0054 Acc:99.88%
Training: Epoch[155/190] Iteration[100/391] Loss: 0.0063 Acc:99.84%
Training: Epoch[155/190] Iteration[150/391] Loss: 0.0058 Acc:99.86%
Training: Epoch[155/190] Iteration[200/391] Loss: 0.0057 Acc:99.87%
Training: Epoch[155/190] Iteration[250/391] Loss: 0.0057 Acc:99.86%
Training: Epoch[155/190] Iteration[300/391] Loss: 0.0056 Acc:99.86%
Training: Epoch[155/190] Iteration[350/391] Loss: 0.0058 Acc:99.85%
Epoch[155/190] Train Acc: 99.85% Valid Acc:93.25% Train loss:0.0056 Valid loss:0.3207 LR:0.001
Training: Epoch[156/190] Iteration[050/391] Loss: 0.0067 Acc:99.84%
Training: Epoch[156/190] Iteration[100/391] Loss: 0.0065 Acc:99.84%
Training: Epoch[156/190] Iteration[150/391] Loss: 0.0060 Acc:99.84%
Training: Epoch[156/190] Iteration[200/391] Loss: 0.0058 Acc:99.86%
Training: Epoch[156/190] Iteration[250/391] Loss: 0.0058 Acc:99.85%
Training: Epoch[156/190] Iteration[300/391] Loss: 0.0057 Acc:99.85%
Training: Epoch[156/190] Iteration[350/391] Loss: 0.0056 Acc:99.85%
Epoch[156/190] Train Acc: 99.86% Valid Acc:93.22% Train loss:0.0054 Valid loss:0.3224 LR:0.001
Training: Epoch[157/190] Iteration[050/391] Loss: 0.0071 Acc:99.75%
Training: Epoch[157/190] Iteration[100/391] Loss: 0.0062 Acc:99.85%
Training: Epoch[157/190] Iteration[150/391] Loss: 0.0059 Acc:99.86%
Training: Epoch[157/190] Iteration[200/391] Loss: 0.0059 Acc:99.85%
Training: Epoch[157/190] Iteration[250/391] Loss: 0.0054 Acc:99.88%
Training: Epoch[157/190] Iteration[300/391] Loss: 0.0051 Acc:99.88%
Training: Epoch[157/190] Iteration[350/391] Loss: 0.0051 Acc:99.88%
Epoch[157/190] Train Acc: 99.88% Valid Acc:93.24% Train loss:0.0053 Valid loss:0.3214 LR:0.001
Training: Epoch[158/190] Iteration[050/391] Loss: 0.0043 Acc:99.95%
Training: Epoch[158/190] Iteration[100/391] Loss: 0.0048 Acc:99.92%
Training: Epoch[158/190] Iteration[150/391] Loss: 0.0051 Acc:99.91%
Training: Epoch[158/190] Iteration[200/391] Loss: 0.0056 Acc:99.89%
Training: Epoch[158/190] Iteration[250/391] Loss: 0.0059 Acc:99.87%
Training: Epoch[158/190] Iteration[300/391] Loss: 0.0058 Acc:99.88%
Training: Epoch[158/190] Iteration[350/391] Loss: 0.0058 Acc:99.86%
Epoch[158/190] Train Acc: 99.87% Valid Acc:93.28% Train loss:0.0057 Valid loss:0.3204 LR:0.001
Training: Epoch[159/190] Iteration[050/391] Loss: 0.0054 Acc:99.89%
Training: Epoch[159/190] Iteration[100/391] Loss: 0.0054 Acc:99.88%
Training: Epoch[159/190] Iteration[150/391] Loss: 0.0057 Acc:99.88%
Training: Epoch[159/190] Iteration[200/391] Loss: 0.0057 Acc:99.88%
Training: Epoch[159/190] Iteration[250/391] Loss: 0.0055 Acc:99.88%
Training: Epoch[159/190] Iteration[300/391] Loss: 0.0056 Acc:99.88%
Training: Epoch[159/190] Iteration[350/391] Loss: 0.0055 Acc:99.89%
Epoch[159/190] Train Acc: 99.88% Valid Acc:93.23% Train loss:0.0057 Valid loss:0.3215 LR:0.001
Training: Epoch[160/190] Iteration[050/391] Loss: 0.0041 Acc:99.91%
Training: Epoch[160/190] Iteration[100/391] Loss: 0.0043 Acc:99.91%
Training: Epoch[160/190] Iteration[150/391] Loss: 0.0047 Acc:99.90%
Training: Epoch[160/190] Iteration[200/391] Loss: 0.0047 Acc:99.91%
Training: Epoch[160/190] Iteration[250/391] Loss: 0.0047 Acc:99.91%
Training: Epoch[160/190] Iteration[300/391] Loss: 0.0049 Acc:99.89%
Training: Epoch[160/190] Iteration[350/391] Loss: 0.0051 Acc:99.88%
Epoch[160/190] Train Acc: 99.88% Valid Acc:93.14% Train loss:0.0051 Valid loss:0.3184 LR:0.001
Training: Epoch[161/190] Iteration[050/391] Loss: 0.0049 Acc:99.89%
Training: Epoch[161/190] Iteration[100/391] Loss: 0.0052 Acc:99.88%
Training: Epoch[161/190] Iteration[150/391] Loss: 0.0052 Acc:99.90%
Training: Epoch[161/190] Iteration[200/391] Loss: 0.0051 Acc:99.89%
Training: Epoch[161/190] Iteration[250/391] Loss: 0.0051 Acc:99.89%
Training: Epoch[161/190] Iteration[300/391] Loss: 0.0054 Acc:99.88%
Training: Epoch[161/190] Iteration[350/391] Loss: 0.0053 Acc:99.88%
Epoch[161/190] Train Acc: 99.88% Valid Acc:93.20% Train loss:0.0054 Valid loss:0.3235 LR:0.001
Training: Epoch[162/190] Iteration[050/391] Loss: 0.0039 Acc:99.97%
Training: Epoch[162/190] Iteration[100/391] Loss: 0.0042 Acc:99.94%
Training: Epoch[162/190] Iteration[150/391] Loss: 0.0042 Acc:99.95%
Training: Epoch[162/190] Iteration[200/391] Loss: 0.0042 Acc:99.95%
Training: Epoch[162/190] Iteration[250/391] Loss: 0.0042 Acc:99.95%
Training: Epoch[162/190] Iteration[300/391] Loss: 0.0046 Acc:99.93%
Training: Epoch[162/190] Iteration[350/391] Loss: 0.0046 Acc:99.93%
Epoch[162/190] Train Acc: 99.92% Valid Acc:93.26% Train loss:0.0049 Valid loss:0.3214 LR:0.001
Training: Epoch[163/190] Iteration[050/391] Loss: 0.0058 Acc:99.88%
Training: Epoch[163/190] Iteration[100/391] Loss: 0.0057 Acc:99.89%
Training: Epoch[163/190] Iteration[150/391] Loss: 0.0055 Acc:99.88%
Training: Epoch[163/190] Iteration[200/391] Loss: 0.0053 Acc:99.89%
Training: Epoch[163/190] Iteration[250/391] Loss: 0.0053 Acc:99.90%
Training: Epoch[163/190] Iteration[300/391] Loss: 0.0053 Acc:99.89%
Training: Epoch[163/190] Iteration[350/391] Loss: 0.0053 Acc:99.88%
Epoch[163/190] Train Acc: 99.88% Valid Acc:93.19% Train loss:0.0053 Valid loss:0.3248 LR:0.001
Training: Epoch[164/190] Iteration[050/391] Loss: 0.0057 Acc:99.86%
Training: Epoch[164/190] Iteration[100/391] Loss: 0.0055 Acc:99.86%
Training: Epoch[164/190] Iteration[150/391] Loss: 0.0053 Acc:99.86%
Training: Epoch[164/190] Iteration[200/391] Loss: 0.0058 Acc:99.84%
Training: Epoch[164/190] Iteration[250/391] Loss: 0.0057 Acc:99.84%
Training: Epoch[164/190] Iteration[300/391] Loss: 0.0053 Acc:99.86%
Training: Epoch[164/190] Iteration[350/391] Loss: 0.0052 Acc:99.87%
Epoch[164/190] Train Acc: 99.87% Valid Acc:93.21% Train loss:0.0053 Valid loss:0.3208 LR:0.001
Training: Epoch[165/190] Iteration[050/391] Loss: 0.0050 Acc:99.91%
Training: Epoch[165/190] Iteration[100/391] Loss: 0.0047 Acc:99.91%
Training: Epoch[165/190] Iteration[150/391] Loss: 0.0047 Acc:99.92%
Training: Epoch[165/190] Iteration[200/391] Loss: 0.0050 Acc:99.91%
Training: Epoch[165/190] Iteration[250/391] Loss: 0.0048 Acc:99.92%
Training: Epoch[165/190] Iteration[300/391] Loss: 0.0049 Acc:99.92%
Training: Epoch[165/190] Iteration[350/391] Loss: 0.0050 Acc:99.92%
Epoch[165/190] Train Acc: 99.90% Valid Acc:93.25% Train loss:0.0051 Valid loss:0.3196 LR:0.001
Training: Epoch[166/190] Iteration[050/391] Loss: 0.0053 Acc:99.91%
Training: Epoch[166/190] Iteration[100/391] Loss: 0.0053 Acc:99.88%
Training: Epoch[166/190] Iteration[150/391] Loss: 0.0051 Acc:99.90%
Training: Epoch[166/190] Iteration[200/391] Loss: 0.0050 Acc:99.90%
Training: Epoch[166/190] Iteration[250/391] Loss: 0.0051 Acc:99.90%
Training: Epoch[166/190] Iteration[300/391] Loss: 0.0051 Acc:99.90%
Training: Epoch[166/190] Iteration[350/391] Loss: 0.0051 Acc:99.91%
Epoch[166/190] Train Acc: 99.91% Valid Acc:93.21% Train loss:0.0050 Valid loss:0.3249 LR:0.001
Training: Epoch[167/190] Iteration[050/391] Loss: 0.0060 Acc:99.83%
Training: Epoch[167/190] Iteration[100/391] Loss: 0.0054 Acc:99.85%
Training: Epoch[167/190] Iteration[150/391] Loss: 0.0051 Acc:99.87%
Training: Epoch[167/190] Iteration[200/391] Loss: 0.0053 Acc:99.88%
Training: Epoch[167/190] Iteration[250/391] Loss: 0.0053 Acc:99.87%
Training: Epoch[167/190] Iteration[300/391] Loss: 0.0052 Acc:99.88%
Training: Epoch[167/190] Iteration[350/391] Loss: 0.0051 Acc:99.88%
Epoch[167/190] Train Acc: 99.89% Valid Acc:93.35% Train loss:0.0050 Valid loss:0.3267 LR:0.001
Training: Epoch[168/190] Iteration[050/391] Loss: 0.0044 Acc:99.95%
Training: Epoch[168/190] Iteration[100/391] Loss: 0.0045 Acc:99.92%
Training: Epoch[168/190] Iteration[150/391] Loss: 0.0047 Acc:99.92%
Training: Epoch[168/190] Iteration[200/391] Loss: 0.0049 Acc:99.91%
Training: Epoch[168/190] Iteration[250/391] Loss: 0.0047 Acc:99.92%
Training: Epoch[168/190] Iteration[300/391] Loss: 0.0048 Acc:99.91%
Training: Epoch[168/190] Iteration[350/391] Loss: 0.0046 Acc:99.91%
Epoch[168/190] Train Acc: 99.91% Valid Acc:93.27% Train loss:0.0046 Valid loss:0.3268 LR:0.001
Training: Epoch[169/190] Iteration[050/391] Loss: 0.0059 Acc:99.83%
Training: Epoch[169/190] Iteration[100/391] Loss: 0.0066 Acc:99.82%
Training: Epoch[169/190] Iteration[150/391] Loss: 0.0074 Acc:99.81%
Training: Epoch[169/190] Iteration[200/391] Loss: 0.0068 Acc:99.82%
Training: Epoch[169/190] Iteration[250/391] Loss: 0.0064 Acc:99.83%
Training: Epoch[169/190] Iteration[300/391] Loss: 0.0061 Acc:99.84%
Training: Epoch[169/190] Iteration[350/391] Loss: 0.0058 Acc:99.85%
Epoch[169/190] Train Acc: 99.86% Valid Acc:93.27% Train loss:0.0057 Valid loss:0.3248 LR:0.001
Training: Epoch[170/190] Iteration[050/391] Loss: 0.0072 Acc:99.80%
Training: Epoch[170/190] Iteration[100/391] Loss: 0.0061 Acc:99.84%
Training: Epoch[170/190] Iteration[150/391] Loss: 0.0056 Acc:99.85%
Training: Epoch[170/190] Iteration[200/391] Loss: 0.0054 Acc:99.86%
Training: Epoch[170/190] Iteration[250/391] Loss: 0.0051 Acc:99.87%
Training: Epoch[170/190] Iteration[300/391] Loss: 0.0049 Acc:99.88%
Training: Epoch[170/190] Iteration[350/391] Loss: 0.0048 Acc:99.89%
Epoch[170/190] Train Acc: 99.89% Valid Acc:93.25% Train loss:0.0049 Valid loss:0.3203 LR:0.001
Training: Epoch[171/190] Iteration[050/391] Loss: 0.0044 Acc:99.91%
Training: Epoch[171/190] Iteration[100/391] Loss: 0.0045 Acc:99.92%
Training: Epoch[171/190] Iteration[150/391] Loss: 0.0045 Acc:99.91%
Training: Epoch[171/190] Iteration[200/391] Loss: 0.0048 Acc:99.89%
Training: Epoch[171/190] Iteration[250/391] Loss: 0.0047 Acc:99.90%
Training: Epoch[171/190] Iteration[300/391] Loss: 0.0048 Acc:99.90%
Training: Epoch[171/190] Iteration[350/391] Loss: 0.0048 Acc:99.90%
Epoch[171/190] Train Acc: 99.90% Valid Acc:93.22% Train loss:0.0047 Valid loss:0.3225 LR:0.001
Training: Epoch[172/190] Iteration[050/391] Loss: 0.0056 Acc:99.86%
Training: Epoch[172/190] Iteration[100/391] Loss: 0.0049 Acc:99.90%
Training: Epoch[172/190] Iteration[150/391] Loss: 0.0048 Acc:99.91%
Training: Epoch[172/190] Iteration[200/391] Loss: 0.0050 Acc:99.89%
Training: Epoch[172/190] Iteration[250/391] Loss: 0.0050 Acc:99.88%
Training: Epoch[172/190] Iteration[300/391] Loss: 0.0052 Acc:99.89%
Training: Epoch[172/190] Iteration[350/391] Loss: 0.0051 Acc:99.90%
Epoch[172/190] Train Acc: 99.89% Valid Acc:93.31% Train loss:0.0052 Valid loss:0.3210 LR:0.001
Training: Epoch[173/190] Iteration[050/391] Loss: 0.0038 Acc:99.94%
Training: Epoch[173/190] Iteration[100/391] Loss: 0.0047 Acc:99.89%
Training: Epoch[173/190] Iteration[150/391] Loss: 0.0047 Acc:99.90%
Training: Epoch[173/190] Iteration[200/391] Loss: 0.0049 Acc:99.90%
Training: Epoch[173/190] Iteration[250/391] Loss: 0.0047 Acc:99.91%
Training: Epoch[173/190] Iteration[300/391] Loss: 0.0047 Acc:99.90%
Training: Epoch[173/190] Iteration[350/391] Loss: 0.0047 Acc:99.90%
Epoch[173/190] Train Acc: 99.90% Valid Acc:93.27% Train loss:0.0047 Valid loss:0.3226 LR:0.001
Training: Epoch[174/190] Iteration[050/391] Loss: 0.0045 Acc:99.94%
Training: Epoch[174/190] Iteration[100/391] Loss: 0.0041 Acc:99.95%
Training: Epoch[174/190] Iteration[150/391] Loss: 0.0046 Acc:99.92%
Training: Epoch[174/190] Iteration[200/391] Loss: 0.0047 Acc:99.91%
Training: Epoch[174/190] Iteration[250/391] Loss: 0.0046 Acc:99.92%
Training: Epoch[174/190] Iteration[300/391] Loss: 0.0045 Acc:99.92%
Training: Epoch[174/190] Iteration[350/391] Loss: 0.0047 Acc:99.91%
Epoch[174/190] Train Acc: 99.90% Valid Acc:93.27% Train loss:0.0047 Valid loss:0.3227 LR:0.001
Training: Epoch[175/190] Iteration[050/391] Loss: 0.0050 Acc:99.92%
Training: Epoch[175/190] Iteration[100/391] Loss: 0.0051 Acc:99.91%
Training: Epoch[175/190] Iteration[150/391] Loss: 0.0049 Acc:99.92%
Training: Epoch[175/190] Iteration[200/391] Loss: 0.0048 Acc:99.91%
Training: Epoch[175/190] Iteration[250/391] Loss: 0.0048 Acc:99.91%
Training: Epoch[175/190] Iteration[300/391] Loss: 0.0047 Acc:99.91%
Training: Epoch[175/190] Iteration[350/391] Loss: 0.0048 Acc:99.90%
Epoch[175/190] Train Acc: 99.91% Valid Acc:93.13% Train loss:0.0048 Valid loss:0.3205 LR:0.001
Training: Epoch[176/190] Iteration[050/391] Loss: 0.0044 Acc:99.92%
Training: Epoch[176/190] Iteration[100/391] Loss: 0.0046 Acc:99.91%
Training: Epoch[176/190] Iteration[150/391] Loss: 0.0048 Acc:99.91%
Training: Epoch[176/190] Iteration[200/391] Loss: 0.0049 Acc:99.89%
Training: Epoch[176/190] Iteration[250/391] Loss: 0.0048 Acc:99.89%
Training: Epoch[176/190] Iteration[300/391] Loss: 0.0047 Acc:99.90%
Training: Epoch[176/190] Iteration[350/391] Loss: 0.0048 Acc:99.90%
Epoch[176/190] Train Acc: 99.90% Valid Acc:93.26% Train loss:0.0047 Valid loss:0.3230 LR:0.001
Training: Epoch[177/190] Iteration[050/391] Loss: 0.0044 Acc:99.92%
Training: Epoch[177/190] Iteration[100/391] Loss: 0.0053 Acc:99.91%
Training: Epoch[177/190] Iteration[150/391] Loss: 0.0053 Acc:99.89%
Training: Epoch[177/190] Iteration[200/391] Loss: 0.0052 Acc:99.88%
Training: Epoch[177/190] Iteration[250/391] Loss: 0.0053 Acc:99.88%
Training: Epoch[177/190] Iteration[300/391] Loss: 0.0051 Acc:99.89%
Training: Epoch[177/190] Iteration[350/391] Loss: 0.0050 Acc:99.90%
Epoch[177/190] Train Acc: 99.90% Valid Acc:93.27% Train loss:0.0050 Valid loss:0.3218 LR:0.001
Training: Epoch[178/190] Iteration[050/391] Loss: 0.0062 Acc:99.84%
Training: Epoch[178/190] Iteration[100/391] Loss: 0.0051 Acc:99.88%
Training: Epoch[178/190] Iteration[150/391] Loss: 0.0049 Acc:99.89%
Training: Epoch[178/190] Iteration[200/391] Loss: 0.0047 Acc:99.89%
Training: Epoch[178/190] Iteration[250/391] Loss: 0.0047 Acc:99.90%
Training: Epoch[178/190] Iteration[300/391] Loss: 0.0046 Acc:99.90%
Training: Epoch[178/190] Iteration[350/391] Loss: 0.0046 Acc:99.91%
Epoch[178/190] Train Acc: 99.91% Valid Acc:93.24% Train loss:0.0046 Valid loss:0.3233 LR:0.001
Training: Epoch[179/190] Iteration[050/391] Loss: 0.0045 Acc:99.94%
Training: Epoch[179/190] Iteration[100/391] Loss: 0.0048 Acc:99.92%
Training: Epoch[179/190] Iteration[150/391] Loss: 0.0046 Acc:99.92%
Training: Epoch[179/190] Iteration[200/391] Loss: 0.0048 Acc:99.90%
Training: Epoch[179/190] Iteration[250/391] Loss: 0.0046 Acc:99.91%
Training: Epoch[179/190] Iteration[300/391] Loss: 0.0047 Acc:99.91%
Training: Epoch[179/190] Iteration[350/391] Loss: 0.0047 Acc:99.91%
Epoch[179/190] Train Acc: 99.91% Valid Acc:93.22% Train loss:0.0047 Valid loss:0.3149 LR:0.001
Training: Epoch[180/190] Iteration[050/391] Loss: 0.0046 Acc:99.91%
Training: Epoch[180/190] Iteration[100/391] Loss: 0.0045 Acc:99.90%
Training: Epoch[180/190] Iteration[150/391] Loss: 0.0053 Acc:99.88%
Training: Epoch[180/190] Iteration[200/391] Loss: 0.0052 Acc:99.89%
Training: Epoch[180/190] Iteration[250/391] Loss: 0.0053 Acc:99.89%
Training: Epoch[180/190] Iteration[300/391] Loss: 0.0051 Acc:99.89%
Training: Epoch[180/190] Iteration[350/391] Loss: 0.0051 Acc:99.88%
Epoch[180/190] Train Acc: 99.88% Valid Acc:93.25% Train loss:0.0052 Valid loss:0.3204 LR:0.001
Training: Epoch[181/190] Iteration[050/391] Loss: 0.0051 Acc:99.91%
Training: Epoch[181/190] Iteration[100/391] Loss: 0.0055 Acc:99.88%
Training: Epoch[181/190] Iteration[150/391] Loss: 0.0053 Acc:99.89%
Training: Epoch[181/190] Iteration[200/391] Loss: 0.0051 Acc:99.89%
Training: Epoch[181/190] Iteration[250/391] Loss: 0.0048 Acc:99.91%
Training: Epoch[181/190] Iteration[300/391] Loss: 0.0049 Acc:99.90%
Training: Epoch[181/190] Iteration[350/391] Loss: 0.0049 Acc:99.91%
Epoch[181/190] Train Acc: 99.91% Valid Acc:93.24% Train loss:0.0048 Valid loss:0.3223 LR:0.001
Training: Epoch[182/190] Iteration[050/391] Loss: 0.0074 Acc:99.81%
Training: Epoch[182/190] Iteration[100/391] Loss: 0.0064 Acc:99.86%
Training: Epoch[182/190] Iteration[150/391] Loss: 0.0059 Acc:99.88%
Training: Epoch[182/190] Iteration[200/391] Loss: 0.0057 Acc:99.88%
Training: Epoch[182/190] Iteration[250/391] Loss: 0.0054 Acc:99.89%
Training: Epoch[182/190] Iteration[300/391] Loss: 0.0054 Acc:99.89%
Training: Epoch[182/190] Iteration[350/391] Loss: 0.0052 Acc:99.90%
Epoch[182/190] Train Acc: 99.90% Valid Acc:93.30% Train loss:0.0050 Valid loss:0.3218 LR:0.001
Training: Epoch[183/190] Iteration[050/391] Loss: 0.0050 Acc:99.94%
Training: Epoch[183/190] Iteration[100/391] Loss: 0.0044 Acc:99.95%
Training: Epoch[183/190] Iteration[150/391] Loss: 0.0047 Acc:99.90%
Training: Epoch[183/190] Iteration[200/391] Loss: 0.0049 Acc:99.90%
Training: Epoch[183/190] Iteration[250/391] Loss: 0.0048 Acc:99.91%
Training: Epoch[183/190] Iteration[300/391] Loss: 0.0047 Acc:99.91%
Training: Epoch[183/190] Iteration[350/391] Loss: 0.0047 Acc:99.91%
Epoch[183/190] Train Acc: 99.90% Valid Acc:93.21% Train loss:0.0048 Valid loss:0.3211 LR:0.001
Training: Epoch[184/190] Iteration[050/391] Loss: 0.0050 Acc:99.88%
Training: Epoch[184/190] Iteration[100/391] Loss: 0.0046 Acc:99.90%
Training: Epoch[184/190] Iteration[150/391] Loss: 0.0044 Acc:99.91%
Training: Epoch[184/190] Iteration[200/391] Loss: 0.0047 Acc:99.90%
Training: Epoch[184/190] Iteration[250/391] Loss: 0.0048 Acc:99.90%
Training: Epoch[184/190] Iteration[300/391] Loss: 0.0047 Acc:99.91%
Training: Epoch[184/190] Iteration[350/391] Loss: 0.0047 Acc:99.90%
Epoch[184/190] Train Acc: 99.90% Valid Acc:93.30% Train loss:0.0048 Valid loss:0.3211 LR:0.001
Training: Epoch[185/190] Iteration[050/391] Loss: 0.0039 Acc:99.89%
Training: Epoch[185/190] Iteration[100/391] Loss: 0.0043 Acc:99.91%
Training: Epoch[185/190] Iteration[150/391] Loss: 0.0040 Acc:99.93%
Training: Epoch[185/190] Iteration[200/391] Loss: 0.0041 Acc:99.92%
Training: Epoch[185/190] Iteration[250/391] Loss: 0.0041 Acc:99.92%
Training: Epoch[185/190] Iteration[300/391] Loss: 0.0042 Acc:99.92%
Training: Epoch[185/190] Iteration[350/391] Loss: 0.0042 Acc:99.93%
Epoch[185/190] Train Acc: 99.93% Valid Acc:93.19% Train loss:0.0042 Valid loss:0.3246 LR:0.001
Training: Epoch[186/190] Iteration[050/391] Loss: 0.0047 Acc:99.91%
Training: Epoch[186/190] Iteration[100/391] Loss: 0.0044 Acc:99.92%
Training: Epoch[186/190] Iteration[150/391] Loss: 0.0045 Acc:99.93%
Training: Epoch[186/190] Iteration[200/391] Loss: 0.0045 Acc:99.93%
Training: Epoch[186/190] Iteration[250/391] Loss: 0.0045 Acc:99.93%
Training: Epoch[186/190] Iteration[300/391] Loss: 0.0045 Acc:99.93%
Training: Epoch[186/190] Iteration[350/391] Loss: 0.0046 Acc:99.93%
Epoch[186/190] Train Acc: 99.93% Valid Acc:93.26% Train loss:0.0045 Valid loss:0.3234 LR:0.001
Training: Epoch[187/190] Iteration[050/391] Loss: 0.0053 Acc:99.88%
Training: Epoch[187/190] Iteration[100/391] Loss: 0.0056 Acc:99.85%
Training: Epoch[187/190] Iteration[150/391] Loss: 0.0052 Acc:99.86%
Training: Epoch[187/190] Iteration[200/391] Loss: 0.0052 Acc:99.87%
Training: Epoch[187/190] Iteration[250/391] Loss: 0.0050 Acc:99.88%
Training: Epoch[187/190] Iteration[300/391] Loss: 0.0047 Acc:99.89%
Training: Epoch[187/190] Iteration[350/391] Loss: 0.0047 Acc:99.89%
Epoch[187/190] Train Acc: 99.89% Valid Acc:93.21% Train loss:0.0048 Valid loss:0.3223 LR:0.001
Training: Epoch[188/190] Iteration[050/391] Loss: 0.0046 Acc:99.92%
Training: Epoch[188/190] Iteration[100/391] Loss: 0.0049 Acc:99.89%
Training: Epoch[188/190] Iteration[150/391] Loss: 0.0048 Acc:99.88%
Training: Epoch[188/190] Iteration[200/391] Loss: 0.0049 Acc:99.88%
Training: Epoch[188/190] Iteration[250/391] Loss: 0.0049 Acc:99.87%
Training: Epoch[188/190] Iteration[300/391] Loss: 0.0050 Acc:99.87%
Training: Epoch[188/190] Iteration[350/391] Loss: 0.0049 Acc:99.88%
Epoch[188/190] Train Acc: 99.88% Valid Acc:93.23% Train loss:0.0048 Valid loss:0.3260 LR:0.001
Training: Epoch[189/190] Iteration[050/391] Loss: 0.0046 Acc:99.89%
Training: Epoch[189/190] Iteration[100/391] Loss: 0.0044 Acc:99.91%
Training: Epoch[189/190] Iteration[150/391] Loss: 0.0041 Acc:99.92%
Training: Epoch[189/190] Iteration[200/391] Loss: 0.0044 Acc:99.90%
Training: Epoch[189/190] Iteration[250/391] Loss: 0.0044 Acc:99.91%
Training: Epoch[189/190] Iteration[300/391] Loss: 0.0043 Acc:99.91%
Training: Epoch[189/190] Iteration[350/391] Loss: 0.0044 Acc:99.91%
Epoch[189/190] Train Acc: 99.91% Valid Acc:93.13% Train loss:0.0044 Valid loss:0.3268 LR:0.001
Training: Epoch[190/190] Iteration[050/391] Loss: 0.0031 Acc:99.98%
Training: Epoch[190/190] Iteration[100/391] Loss: 0.0040 Acc:99.93%
Training: Epoch[190/190] Iteration[150/391] Loss: 0.0039 Acc:99.94%
Training: Epoch[190/190] Iteration[200/391] Loss: 0.0042 Acc:99.92%
Training: Epoch[190/190] Iteration[250/391] Loss: 0.0040 Acc:99.92%
Training: Epoch[190/190] Iteration[300/391] Loss: 0.0041 Acc:99.92%
Training: Epoch[190/190] Iteration[350/391] Loss: 0.0044 Acc:99.91%
Epoch[190/190] Train Acc: 99.90% Valid Acc:93.26% Train loss:0.0045 Valid loss:0.3231 LR:0.001
class:plane     , total num:5000.0, correct num:4996.0  Recall: 99.92% Precision: 99.92%
class:car       , total num:5000.0, correct num:4996.0  Recall: 99.92% Precision: 99.96%
class:bird      , total num:5000.0, correct num:4993.0  Recall: 99.86% Precision: 99.92%
class:cat       , total num:5000.0, correct num:4989.0  Recall: 99.78% Precision: 99.84%
class:deer      , total num:5000.0, correct num:4999.0  Recall: 99.98% Precision: 99.88%
class:dog       , total num:5000.0, correct num:4994.0  Recall: 99.88% Precision: 99.82%
class:frog      , total num:5000.0, correct num:4998.0  Recall: 99.96% Precision: 99.88%
class:horse     , total num:5000.0, correct num:4991.0  Recall: 99.82% Precision: 99.96%
class:ship      , total num:5000.0, correct num:4998.0  Recall: 99.96% Precision: 99.92%
class:truck     , total num:5000.0, correct num:4996.0  Recall: 99.92% Precision: 99.90%
class:plane     , total num:1000.0, correct num:934.0  Recall: 93.39% Precision: 93.30%
class:car       , total num:1000.0, correct num:966.0  Recall: 96.59% Precision: 97.08%
class:bird      , total num:1000.0, correct num:910.0  Recall: 90.99% Precision: 91.36%
class:cat       , total num:1000.0, correct num:851.0  Recall: 85.09% Precision: 86.04%
class:deer      , total num:1000.0, correct num:936.0  Recall: 93.59% Precision: 92.48%
class:dog       , total num:1000.0, correct num:907.0  Recall: 90.69% Precision: 89.88%
class:frog      , total num:1000.0, correct num:952.0  Recall: 95.19% Precision: 94.72%
class:horse     , total num:1000.0, correct num:954.0  Recall: 95.39% Precision: 96.65%
class:ship      , total num:1000.0, correct num:954.0  Recall: 95.39% Precision: 95.87%
class:truck     , total num:1000.0, correct num:962.0  Recall: 96.19% Precision: 95.14%
 done ~~~~ 03-24_00-04, best acc: 0.9342 in :116
